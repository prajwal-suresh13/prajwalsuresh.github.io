<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Prajwal Suresh</title>
<link>https://prajwalsuresh.com/blog.html</link>
<atom:link href="https://prajwalsuresh.com/blog.xml" rel="self" type="application/rss+xml"/>
<description>Prajwal&#39;s personal website</description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Wed, 23 Nov 2022 18:30:00 GMT</lastBuildDate>
<item>
  <title>Stable diffusion using 🤗 Hugging Face - Variations</title>
  <dc:creator>Prajwal Suresh</dc:creator>
  <link>https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Introduction to variations provided in 🤗 <a href="https://huggingface.co/">Hugging face</a> - <a href="https://github.com/huggingface/diffusers">Diffusers library</a> such as Negative prompt, Image to Image and training new concept using Textual Inversion</p>
</blockquote>
<div data-align="center">
<p><a href="https://colab.research.google.com/github/prajwal-suresh13/image_colorization/blob/master/image_colorization_cyclegan_training.ipynb"> <img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/https:/colab.research.google.com/assets/colab-badge.svg" width="150"> </a></p>
</div>
<p><br></p>
<p>This is the second post on Stable Diffusion. Check out the previous post through link below if you haven’t read it yet<br> 1. <strong>Part 1</strong> - <a href="https://aayushmnit.com/posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html">Stable diffusion using 🤗 Hugging Face - Introduction</a>.</p>
<p>In previous post, we learnt about Stable Diffusion and its key components VAE, UNet, Scheduler &amp; CLIPTextModel. We also built a pipeline named <code>SDPipeline</code> using these key components and generated images for multiple prompts. In this post, we will further modify our pipeline to work for negative prompts and image to image. At the end, we will learn about Textual Inversion which is useful to train a new concept</p>
<p>At first, let’s download and import the required libraries, authenticate and login to HuggingFace</p>
<div class="cell" data-outputid="228d7dd1-4ac9-48fa-cf2c-b066c593be9f">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq accelerate transformers diffusers ftfy</span></code></pre></div>
</details>
</div>
<div class="cell" data-outputid="b0f51085-cf9c-49df-c52a-52147ec263a6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> notebook_login</span>
<span id="cb2-2">notebook_login()</span></code></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> autocast</span>
<span id="cb3-3"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> transforms <span class="im" style="color: #00769E;">as</span> tfms</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb3-6"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> logging</span>
<span id="cb3-7">logging.set_verbosity_error() <span class="co" style="color: #5E5E5E;"># Suppress unnecessary warning from CLIPTextModel</span></span>
<span id="cb3-8"></span>
<span id="cb3-9"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb3-12"><span class="im" style="color: #00769E;">from</span> matplotlib <span class="im" style="color: #00769E;">import</span> pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb3-13"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb3-14"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb3-15"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb3-16"><span class="im" style="color: #00769E;">from</span> fastdownload <span class="im" style="color: #00769E;">import</span> FastDownload</span>
<span id="cb3-17"></span>
<span id="cb3-18"><span class="im" style="color: #00769E;">from</span> IPython.display <span class="im" style="color: #00769E;">import</span> HTML</span>
<span id="cb3-19"><span class="im" style="color: #00769E;">from</span> base64 <span class="im" style="color: #00769E;">import</span> b64encode</span></code></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> image_grid(imgs, rows, cols):</span>
<span id="cb5-2">    w,h <span class="op" style="color: #5E5E5E;">=</span> imgs[<span class="dv" style="color: #AD0000;">0</span>].size</span>
<span id="cb5-3">    grid <span class="op" style="color: #5E5E5E;">=</span> Image.new(<span class="st" style="color: #20794D;">"RGB"</span>, size<span class="op" style="color: #5E5E5E;">=</span>(cols<span class="op" style="color: #5E5E5E;">*</span>w, rows<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;">for</span> i, img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(imgs):grid.paste(img, box<span class="op" style="color: #5E5E5E;">=</span>(i<span class="op" style="color: #5E5E5E;">%</span>cols<span class="op" style="color: #5E5E5E;">*</span>w, i<span class="op" style="color: #5E5E5E;">//</span>cols<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;">return</span> grid</span></code></pre></div>
</div>
<p>Below is the pipeline which we built in the previous post using the core components.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb6-2"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> SDPipeline():</span>
<span id="cb7-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model_path):</span>
<span id="cb7-3">        <span class="va" style="color: #111111;">self</span>.model_path<span class="op" style="color: #5E5E5E;">=</span>model_path</span>
<span id="cb7-4">        <span class="va" style="color: #111111;">self</span>.vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(model_path,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb7-5">        <span class="va" style="color: #111111;">self</span>.unet<span class="op" style="color: #5E5E5E;">=</span>UNet2DConditionModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb7-6">        <span class="va" style="color: #111111;">self</span>.text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"text_encoder"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb7-7">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"tokenizer"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16)</span>
<span id="cb7-8">        <span class="va" style="color: #111111;">self</span>.scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb7-9"></span>
<span id="cb7-10">    <span class="kw" style="color: #003B4F;">def</span> encode_text(<span class="va" style="color: #111111;">self</span>, prompts, maxlen<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-11">        <span class="cf" style="color: #003B4F;">if</span> maxlen <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: maxlen <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer.model_max_length</span>
<span id="cb7-12">        inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer(prompts, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>maxlen, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb7-13">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.text_encoder(inp.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>].half()</span>
<span id="cb7-14"></span>
<span id="cb7-15">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>,prompts, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-16">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb7-17"></span>
<span id="cb7-18">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb7-19">        uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb7-20">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb7-21">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb7-22"></span>
<span id="cb7-23">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb7-24"></span>
<span id="cb7-25">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb7-26">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb7-27"></span>
<span id="cb7-28">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(<span class="va" style="color: #111111;">self</span>.scheduler.timesteps)):</span>
<span id="cb7-29">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb7-30">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb7-31">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb7-32"></span>
<span id="cb7-33">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb7-34"></span>
<span id="cb7-35">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb7-36">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb7-37">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb7-38">        </span>
<span id="cb7-39">        </span>
<span id="cb7-40">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.latents_to_pil(latents)</span>
<span id="cb7-41"></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">def</span> create_image(t):</span>
<span id="cb8-2">    image <span class="op" style="color: #5E5E5E;">=</span> (t<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>).detach().cpu().permute(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">0</span>).numpy()</span>
<span id="cb8-3">    <span class="cf" style="color: #003B4F;">return</span> Image.fromarray((image<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">255</span>).<span class="bu" style="color: null;">round</span>().astype(<span class="st" style="color: #20794D;">"uint8"</span>))</span>
<span id="cb8-4"></span>
<span id="cb8-5"><span class="kw" style="color: #003B4F;">def</span> pil_to_latents(<span class="va" style="color: #111111;">self</span>,input_img):</span>
<span id="cb8-6">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb8-7">        latent <span class="op" style="color: #5E5E5E;">=</span> tfms.ToTensor()(input_img).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).to(torch_device)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb8-8">        latent <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.vae.encode(latent.half())</span>
<span id="cb8-9">    <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latent.latent_dist.sample()</span>
<span id="cb8-10"></span>
<span id="cb8-11"><span class="kw" style="color: #003B4F;">def</span> latents_to_pil(<span class="va" style="color: #111111;">self</span>, latents):</span>
<span id="cb8-12">    latents <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.18215</span>) <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb8-14">        images <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.vae.decode(latents).sample</span>
<span id="cb8-15">    pil_images <span class="op" style="color: #5E5E5E;">=</span>[create_image(image) <span class="cf" style="color: #003B4F;">for</span> image <span class="kw" style="color: #003B4F;">in</span> images]</span>
<span id="cb8-16">    <span class="cf" style="color: #003B4F;">return</span> pil_images</span>
<span id="cb8-17"></span>
<span id="cb8-18">SDPipeline.pil_to_latents <span class="op" style="color: #5E5E5E;">=</span> pil_to_latents</span>
<span id="cb8-19">SDPipeline.latents_to_pil  <span class="op" style="color: #5E5E5E;">=</span> latents_to_pil</span></code></pre></div>
</div>
<section id="negative-prompts" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Negative Prompts</h1>
<section id="what-is-negative-prompt" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="what-is-negative-prompt"><span class="header-section-number">1.1</span> What is Negative Prompt?</h2>
<p>Negative prompt is an additional capability which removes anything that the user doesn’t want to see in generated images. Suppose you don’t want a particular color or particular object to be part of the generated images, you can mention it in <code>negative_prompt</code> argument in the <code>StableDiffusionPipeline</code>.</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/https:/miro.medium.com/max/828/0*wnoQYUZJiCwhtUTg.webp" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 1 : Example for Negative Prompt <a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265">(Image Credit)</a>
</h4>
</figcaption>
</figure>
</section>
<section id="adding-negative-prompt-to-our-sdpipeline" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="adding-negative-prompt-to-our-sdpipeline"><span class="header-section-number">1.2</span> Adding negative prompt to our SDPipeline</h2>
<p>Let’s find how the negative prompt works and how can we integrate it into our pipeline.</p>
<p>In the previous post, we used an empty string, generated unconditional embeddings from it and used it along with text embeddings to implement Classifier-Free guidance as below</p>
<pre><code>uncond = self.encode_text([""]*batch_size, cond.shape[1])</code></pre>
<p>The way negative prompt works is that the empty string is replaced by the string containing the objects, styles or colors provided by the user which they don’t want to be present in the generated images. So to implement negative prompt, we can modify our code to use empty string if negative prompts is not provided else to use negative prompt provided by user</p>
<pre><code>if not negative_prompts:
    uncond = self.encode_text([""]*batch_size, cond.shape[1])
else:
    uncond = self.encode_text(neg_prompts, cond.shape[1])</code></pre>
<p>Let’s make those changes in the diffusion loop of our pipeline and check if our negative prompt implementation is working</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> diff_pipeline_negprompts(<span class="va" style="color: #111111;">self</span>,prompts,negative_prompts<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb11-2">        <span class="co" style="color: #5E5E5E;">#calculate number of prompts</span></span>
<span id="cb11-3">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb11-4">        </span>
<span id="cb11-5">        <span class="co" style="color: #5E5E5E;">#generate text_embeddings</span></span>
<span id="cb11-6">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb11-7">        </span>
<span id="cb11-8">        <span class="co" style="color: #5E5E5E;">#generate embedding for negative prompts if available else generate unconditional embeddings</span></span>
<span id="cb11-9">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> negative_prompts: uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb11-10">        <span class="cf" style="color: #003B4F;">else</span> : uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(negative_prompts, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb11-11">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb11-12"></span>
<span id="cb11-13">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb11-14">        </span>
<span id="cb11-15">        <span class="co" style="color: #5E5E5E;">#generate random noise and add noise</span></span>
<span id="cb11-16">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb11-17">        </span>
<span id="cb11-18">        <span class="co" style="color: #5E5E5E;">#Setting number of steps in scheduler</span></span>
<span id="cb11-19">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb11-20">        </span>
<span id="cb11-21">        <span class="co" style="color: #5E5E5E;">#Add noise to latents</span></span>
<span id="cb11-22">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb11-23"></span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(<span class="va" style="color: #111111;">self</span>.scheduler.timesteps)):</span>
<span id="cb11-25">            <span class="co" style="color: #5E5E5E;"># scale the latents for particular timestep</span></span>
<span id="cb11-26">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb11-27">            </span>
<span id="cb11-28">            <span class="co" style="color: #5E5E5E;">#predict noise using UNet</span></span>
<span id="cb11-29">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb11-30">                </span>
<span id="cb11-31">            <span class="co" style="color: #5E5E5E;">#Perform Classifier-Free Guidance    </span></span>
<span id="cb11-32">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb11-33">            </span>
<span id="cb11-34">            <span class="co" style="color: #5E5E5E;">#Removing predicted noise from the latents</span></span>
<span id="cb11-35">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb11-36">            </span>
<span id="cb11-37">            <span class="co" style="color: #5E5E5E;">#run callbacks if any are added</span></span>
<span id="cb11-38">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb11-39">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb11-40">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb11-41">        </span>
<span id="cb11-42">        <span class="co" style="color: #5E5E5E;">#after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated</span></span>
<span id="cb11-43">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.latents_to_pil(latents)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">SDPipeline.<span class="fu" style="color: #4758AB;">__call__</span> <span class="op" style="color: #5E5E5E;">=</span> diff_pipeline_negprompts</span></code></pre></div>
</div>
<p>We will generate two images with same prompt and for one of the images we will use a negative prompt</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">pipe <span class="op" style="color: #5E5E5E;">=</span> SDPipeline(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="e1d66cf2-929e-4d79-9650-a1a665e4ccc1">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb14-2">img1 <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"a photo of a forest with trees and bushes"</span>])[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb14-3">img2 <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"a photo of a forest with trees and bushes"</span>], negative_prompts<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"green"</span>])[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"850628b0986e4f99a75d266493e6adec","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ae7e125a4e90406d94cc79935936bd30","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="40611403-ec13-4d4b-ef90-8c1eb3742eae">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">imgs<span class="op" style="color: #5E5E5E;">=</span>[img1, img2]</span>
<span id="cb15-2">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 2: Negative Prompt Results for prompt “a photo of a forest with trees and bushes” and negative prompt=“green”“
</h4>
</figcaption>
<p>As you can see in the images above, the image on the left generated image of forest. When given negative prompt as color green, the model generated same image without green color in it (on the right)</p>
</section>
</section>
<section id="image-to-image" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Image To Image</h1>
<section id="adding-image-to-image-capability-to-sdpipeline" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="adding-image-to-image-capability-to-sdpipeline"><span class="header-section-number">2.1</span> Adding Image to Image capability to SDPipeline</h2>
<p>In the previous post, we learnt that we can also pass an image to our model and it generates new images using the provided image as the initial image along with the prompt. There are two scenarios 1) with image (Image to Image) 2) without image (Prompt to Image)</p>
<p>For scenario 1 which is Image to Image pipeline, we expect 2 parameters: 1. init_image - This is used as the initial image 2. strength - The value of this parameter is between 0 and 1</p>
<p>First, the provided init_image is passed to the VAE’s encoder to generate latents. We then need to add noise to the latents. Since we are using initial image we can reduce the number of inference steps. The <code>strength</code> parameter is used for this and higher its value higher the <code>inference_steps</code>. If we want our generated image to be similar to the init_image, the number of steps should be less so the <code>strength</code> parameter value should also be less. The amount of noise to be added is calculated by multiplying the <code>strength</code> and <code>inference_steps</code></p>
<p>For example, suppose we want our generated image to be similar to the initial image so we assign the <code>strength</code> parameter to 0.4 and <code>inference_steps</code> is equal to 50 . Then the noise added is for the step 30 (<code>50 -( 0.4 x 50) == 30</code>). The <code>inference_steps</code> is reduced to 20 steps (<code>50x0.4</code>)</p>
<p>For scenario 2 which is prompt to image, we need to create a random noise. The amount of noise for the first step is multiplied with <code>scheduler.init_noise_sigma</code> . The <code>inference_steps</code> remains same.</p>
<p>Let’s create a function which can handle both the scenarios. The function should return latents and timesteps. <br> 1. In case of Prompt to Image, random noise is created and multiplied with init_noise is returned. The timesteps is returned without modifying it 2. In case of Image to Image, latents are created from init_image and amount of noise to be added is calculated using strength and returned. The timesteps is also reduced based on the strength and returned</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> create_latents(<span class="va" style="color: #111111;">self</span>, batch_size, init_image, strength, inference_steps):</span>
<span id="cb16-2">    </span>
<span id="cb16-3">    <span class="cf" style="color: #003B4F;">if</span> init_image <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb16-4">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb16-5">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb16-6">        <span class="cf" style="color: #003B4F;">return</span> latents, <span class="va" style="color: #111111;">self</span>.scheduler.timesteps.to(torch_device)</span>
<span id="cb16-7">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb16-8">        img_latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.pil_to_latents(init_image)</span>
<span id="cb16-9">        noise <span class="op" style="color: #5E5E5E;">=</span> torch.randn(img_latents.shape, generator<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, device<span class="op" style="color: #5E5E5E;">=</span>torch_device, dtype<span class="op" style="color: #5E5E5E;">=</span>img_latents.dtype)</span>
<span id="cb16-10"></span>
<span id="cb16-11">        init_timestep <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(inference_steps <span class="op" style="color: #5E5E5E;">*</span> strength)</span>
<span id="cb16-12">        timesteps <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.timesteps[<span class="op" style="color: #5E5E5E;">-</span>init_timestep]</span>
<span id="cb16-13">        timesteps <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([timesteps], device<span class="op" style="color: #5E5E5E;">=</span>torch_device)</span>
<span id="cb16-14"></span>
<span id="cb16-15">        init_latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.add_noise(img_latents, noise, timesteps)</span>
<span id="cb16-16"></span>
<span id="cb16-17">        t_start <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">max</span>(inference_steps <span class="op" style="color: #5E5E5E;">-</span> init_timestep, <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb16-18">        timesteps <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.timesteps[t_start:].to(torch_device)</span>
<span id="cb16-19">        </span>
<span id="cb16-20">        <span class="cf" style="color: #003B4F;">return</span> init_latents, timesteps</span>
<span id="cb16-21"></span>
<span id="cb16-22">SDPipeline.create_latents <span class="op" style="color: #5E5E5E;">=</span> create_latents</span></code></pre></div>
</div>
<p>Let’s replace the diffusion loop code to get latents and timesteps from the above function</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;">def</span> diff_pipeline_img2img(<span class="va" style="color: #111111;">self</span>,prompts,init_image<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, strength<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.8</span>, negative_prompts<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb17-2">        <span class="co" style="color: #5E5E5E;">#calculate number of prompts</span></span>
<span id="cb17-3">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb17-4">        </span>
<span id="cb17-5">        <span class="co" style="color: #5E5E5E;">#generate text_embeddings</span></span>
<span id="cb17-6">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb17-7">        </span>
<span id="cb17-8">        <span class="co" style="color: #5E5E5E;">#generate embedding for negative prompts if available else generate unconditional embeddings</span></span>
<span id="cb17-9">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> negative_prompts: uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb17-10">        <span class="cf" style="color: #003B4F;">else</span> : uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(negative_prompts, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb17-11">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb17-12">        </span>
<span id="cb17-13">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb17-14"></span>
<span id="cb17-15">        <span class="co" style="color: #5E5E5E;">#Setting number of steps in scheduler</span></span>
<span id="cb17-16">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb17-17">        </span>
<span id="cb17-18">        <span class="co" style="color: #5E5E5E;">#generate random noise and add noise</span></span>
<span id="cb17-19">        latents, timesteps <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.create_latents(batch_size, init_image, strength, inference_steps)</span>
<span id="cb17-20">        </span>
<span id="cb17-21">        </span>
<span id="cb17-22">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(timesteps)):</span>
<span id="cb17-23">            <span class="co" style="color: #5E5E5E;"># scale the latents for particular timestep</span></span>
<span id="cb17-24">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb17-25">            </span>
<span id="cb17-26">            <span class="co" style="color: #5E5E5E;">#predict noise using UNet</span></span>
<span id="cb17-27">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-28">                </span>
<span id="cb17-29">            <span class="co" style="color: #5E5E5E;">#Perform Classifier-Free Guidance    </span></span>
<span id="cb17-30">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb17-31">            </span>
<span id="cb17-32">            <span class="co" style="color: #5E5E5E;">#Removing predicted noise from the latents</span></span>
<span id="cb17-33">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb17-34">            </span>
<span id="cb17-35">            <span class="co" style="color: #5E5E5E;">#run callbacks if any are added</span></span>
<span id="cb17-36">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb17-37">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb17-38">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb17-39">        </span>
<span id="cb17-40">        <span class="co" style="color: #5E5E5E;">#after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated</span></span>
<span id="cb17-41">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.latents_to_pil(latents)</span>
<span id="cb17-42"></span>
<span id="cb17-43">SDPipeline.<span class="fu" style="color: #4758AB;">__call__</span> <span class="op" style="color: #5E5E5E;">=</span> diff_pipeline_img2img</span></code></pre></div>
</div>
<p>Now let’s download an image pass it to our Image To Image pipeline</p>
<div class="cell" data-outputid="d83aa588-e356-427f-8573-92ee3e0d5359">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;">from</span> fastdownload <span class="im" style="color: #00769E;">import</span> FastDownload</span>
<span id="cb18-2">p <span class="op" style="color: #5E5E5E;">=</span> FastDownload().download(<span class="st" style="color: #20794D;">'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png'</span>)</span>
<span id="cb18-3">image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(p).convert(<span class="st" style="color: #20794D;">'RGB'</span>).resize((<span class="dv" style="color: #AD0000;">512</span>,<span class="dv" style="color: #AD0000;">512</span>))</span>
<span id="cb18-4">image</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="49152" class="" max="46150" style="width:300px; height:20px; vertical-align: middle;"></progress>
      106.50% [49152/46150 00:00&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-17-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 3: Initial Image for Image To Image Pipeline
</h4>
</figcaption>
<p>We can also add our Callback function to visualize how the image is generated</p>
<div class="cell" data-outputid="21fbd64c-dac4-4f79-8e3a-c4712953e976">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb19-2"><span class="kw" style="color: #003B4F;">class</span> Latents_Callback:</span>
<span id="cb19-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, prompts):</span>
<span id="cb19-4">        <span class="va" style="color: #111111;">self</span>.imgs <span class="op" style="color: #5E5E5E;">=</span> [[] <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompts))]</span>
<span id="cb19-5">    </span>
<span id="cb19-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, i, t, latents):</span>
<span id="cb19-7">        latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb19-8">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(latents.shape[<span class="dv" style="color: #AD0000;">0</span>]):</span>
<span id="cb19-9">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample[i]</span>
<span id="cb19-10">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> create_image(callback_imgs)</span>
<span id="cb19-11">            <span class="va" style="color: #111111;">self</span>.imgs[i].append(callback_imgs)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"828e334014b94b5796ba29eab096aeed","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a388bc78fb2f4f8fbc05b9da18113511","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>We are using a random sketch of wolf howling at moon as initial image. So we can use higher value of strength to generate good image</p>
<div class="cell" data-outputid="d61c79a4-26ec-4918-a2f0-71bc9be0758e">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">prompt <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"Wolf howling at the moon, photorealistic 4K"</span>]</span>
<span id="cb20-2">pipei2i <span class="op" style="color: #5E5E5E;">=</span> SDPipeline(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c564857265014adda83d9f96c2558e28","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"80022c09b8b04b0098eca913ba42e1a3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c1b49922bb0640bdb9ef7820cf97b24a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"048a260c7cd146d9ad12d8d55bdaa74b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"55cddc447aea4423b1ab8bb0cc539ec6","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f3771ee38586467b871057bed4988e8c","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2419039a38ee46f89baf2149d01050bb","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8923024c900a4576a9b61662a9162ce0","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="72ba68de-251f-4f32-8c87-544c0d648036">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">lc <span class="op" style="color: #5E5E5E;">=</span> Latents_Callback(prompt)</span>
<span id="cb21-2">img <span class="op" style="color: #5E5E5E;">=</span> pipei2i(prompt,init_image<span class="op" style="color: #5E5E5E;">=</span>image, callbacks<span class="op" style="color: #5E5E5E;">=</span>[lc], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>,inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb21-3">img[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bfec3d4e3c644f75b928070eff93e9a6","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 4: Generated Image for Image To Image pipeline
</h4>
</figcaption>
<div class="cell" data-outputid="a319560a-040e-4f6c-a05d-190c41e07c66" data-scrolled="true">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">rows, cols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompt), <span class="bu" style="color: null;">len</span>(lc.imgs[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb22-2">imgs <span class="op" style="color: #5E5E5E;">=</span>[img <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompt)) <span class="cf" style="color: #003B4F;">for</span> img <span class="kw" style="color: #003B4F;">in</span> lc.imgs[i]]</span>
<span id="cb22-3">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span>rows, cols<span class="op" style="color: #5E5E5E;">=</span>cols)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="using-both-image-to-image-and-negative-prompt" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Using both Image to Image and Negative Prompt</h1>
<p>Let’s use both Image to Image and Negative prompt in the next example. We will download a picture of sunflower vase and pass it ot Image to Image pipeline to convert it to a Rose vase. We will remove yellow color from the generated image using negative prompt</p>
<div class="cell" data-outputid="28405071-a649-4ea9-c523-f44d3cc979fd">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;">from</span> fastdownload <span class="im" style="color: #00769E;">import</span> FastDownload</span>
<span id="cb23-2">p <span class="op" style="color: #5E5E5E;">=</span> FastDownload().download(<span class="st" style="color: #20794D;">'https://img.freepik.com/premium-photo/oil-painting-yellow-flowers-sunflowers-vase-yellow-oil-paints_175677-2889.jpg?w=2000'</span>)</span>
<span id="cb23-3">image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(p).convert(<span class="st" style="color: #20794D;">'RGB'</span>).resize((<span class="dv" style="color: #AD0000;">512</span>,<span class="dv" style="color: #AD0000;">512</span>))</span>
<span id="cb23-4">image</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="794624" class="" max="793835" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.10% [794624/793835 00:00&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-22-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored" data-anchor-id="using-both-image-to-image-and-negative-prompt">
Fig 5: Initial Image of Sunflower vase
</h4>
</figcaption>
<div class="cell" data-outputid="5e8847b6-0e3f-4457-86c4-1b4fa345ed8b">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">prompt <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"A photo of a rose flower vase, photorealistic 4K"</span>]</span>
<span id="cb24-2">negative_prompts <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'yellow, lowres, worst quality, low quality'</span>]</span>
<span id="cb24-3">img <span class="op" style="color: #5E5E5E;">=</span> pipei2i(prompt, negative_prompts<span class="op" style="color: #5E5E5E;">=</span>negative_prompts,init_image<span class="op" style="color: #5E5E5E;">=</span>image,inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, strength<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.6</span>)</span>
<span id="cb24-4">img[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"456a6798e75a4ee9bff3317252c34d94","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="57">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-23-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 6: Generated Image of Rose vase for initial image of sunflower vase and Negative Prompt as yellow
</h4>
</figcaption>
<p>Until now, we saw how we can use StableDiffusion to generate realistic images based on prompts. But what if we want to generate images for objects or person which the model has never seen. What if we want to generate our own images based on prompt. This can be done using process such as Textual Inversion and Dreambooth. Let’s learn about Textual inversion in the next section</p>
</section>
<section id="textual-inversion" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Textual Inversion</h1>
<p>Textual Inversion is a process to teach a new word to the text model and get its embeddings close to the visual representation of the image. This is achieved by adding a new token to the vocabulary, freezing the weights of all the models except the text encoder and train with the images.</p>
<p>This is a schematic representation of the process by the authors of the <a href="https://textual-inversion.github.io/">Textual Inversion paper</a></p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/https:/textual-inversion.github.io/static/images/training/training.JPG" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored" data-anchor-id="textual-inversion">
Fig 7 : Textual Inversion diagram <a href="https://textual-inversion.github.io/">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>We will be implementing Dreambooth in the next post which is similar to Textual Inversion. So we will use an already trained token using Textual Inversion and use it in inference. You can train your own tokens with photos you provide using <a href="https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion">this training script</a> or <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb">Google Colab notebook</a>.</p>
<p>We’ll try an example using embeddings trained for <a href="https://huggingface.co/sd-concepts-library/indian-watercolor-portraits">this style</a>.</p>
<div class="cell" data-outputid="0c159f62-553f-439a-b0f0-763c8deeb315">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline</span>
<span id="cb25-2">pipe <span class="op" style="color: #5E5E5E;">=</span> StableDiffusionPipeline.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, revision<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"fp16"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16) </span>
<span id="cb25-3">pipe <span class="op" style="color: #5E5E5E;">=</span> pipe.to(torch_device)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b91e012d7375432e948c2e5bb0c68e26","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"23d0f35739e849238fee7bf256cbe5eb","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b0837bbbbedb417b82cdb9b81c3bb0f8","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"169b4f80589147e992aa2af6e3574f4a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"46f9ab0a4339452cb859ccfad9962222","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b4a498941beb4840bf0afba501219fd7","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d1023ef18fc94d3ca4e0d7174e0c21ac","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"89f1d234c6f64b428a20b38846e79ae1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0e962814fa014936bfd21d9941225333","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"902fd9798f334a169be1b9f374b55d9f","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c55e1ae5dd414e2aaa83782300f37d7a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"05b76a258cf942c089ed87e92ce024a4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"74f45681ef904ae7a3224d8906b1f56c","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6ed67f8e8af048cdba31489df9c95913","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f42f7baa5bf9462a8127443702de1e96","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8b06067bf19443f5b67d206459bb56df","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3800a7c286e348c7801a60da6bd4ede8","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Let’s download the embedding for the watercolor portrait style</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">embeds_url <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin"</span></span>
<span id="cb26-2">embeds_path <span class="op" style="color: #5E5E5E;">=</span> FastDownload().download(embeds_url)</span>
<span id="cb26-3">embeds_dict <span class="op" style="color: #5E5E5E;">=</span> torch.load(<span class="bu" style="color: null;">str</span>(embeds_path), map_location<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"cpu"</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="0ae8b824-d629-4599-d107-eb0ac1e367b0">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">tokenizer <span class="op" style="color: #5E5E5E;">=</span> pipe.tokenizer</span>
<span id="cb27-2">text_encoder <span class="op" style="color: #5E5E5E;">=</span> pipe.text_encoder</span>
<span id="cb27-3">new_token, embeds <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(embeds_dict.items()))</span>
<span id="cb27-4">embeds <span class="op" style="color: #5E5E5E;">=</span> embeds.to(text_encoder.dtype)</span>
<span id="cb27-5">new_token, embeds.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>('&lt;watercolor-portrait&gt;', torch.Size([768]))</code></pre>
</div>
</div>
<p>We can add the new token which in our case is <code>&lt;watercolor-portrait&gt;</code> to the tokenizer. We also resize the token embedding of text_encoder and add the embeddings of the <code>&lt;watercolor-portrait&gt;</code> to the text_encoder</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="cf" style="color: #003B4F;">assert</span> tokenizer.add_tokens(new_token) <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="st" style="color: #20794D;">"The token already exists!"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">text_encoder.resize_token_embeddings(<span class="bu" style="color: null;">len</span>(tokenizer))</span>
<span id="cb30-2">new_token_id <span class="op" style="color: #5E5E5E;">=</span> tokenizer.convert_tokens_to_ids(new_token)</span>
<span id="cb30-3">text_encoder.get_input_embeddings().weight.data[new_token_id] <span class="op" style="color: #5E5E5E;">=</span> embeds</span></code></pre></div>
</div>
<p>We can then use the new token in the prompt to generate images</p>
<div class="cell" data-outputid="bc7f398b-5345-4eb5-e076-56e5361287e7">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb31-2">image <span class="op" style="color: #5E5E5E;">=</span> pipe(<span class="st" style="color: #20794D;">"Einstein reading newspaper in the style of &lt;watercolor-portrait&gt;"</span>).images[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb31-3">image</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ce1ea175c082483a859225925f2286a1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-29-output-2.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 8: Using new token generated using Textual Inversion in prompt
</h4>
</figcaption>
</section>
<section id="conclusion" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Conclusion</h1>
<p>In this post, we have covered a brief introduction on variation provided by the diffusers library such as negative prompt and Image To Image. We then implemented both in our <code>SDPipeline</code>. We also saw how we can train a new concept using Textual Inversion.</p>
<p>I hope you enjoyed reading it. If there is any feedback, feel free to reach out on <a href="https://www.linkedin.com/in/prajwal-s-1416061b3/">Linkedin</a> or on <a href="prajwalsuresh13@gmail.com">mail</a></p>
</section>
<section id="references" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> References</h1>
<ul>
<li><a href="https://www.fast.ai/posts/part2-2022-preview.html">FastAI course</a></li>
<li><a href="https://aayushmnit.com/blog.html">Aayush Agrawal’s blog</a></li>
</ul>


</section>

 ]]></description>
  <category>Stable Diffusion</category>
  <guid>https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html</guid>
  <pubDate>Wed, 23 Nov 2022 18:30:00 GMT</pubDate>
  <media:content url="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionVariations/01.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Stable diffusion using 🤗 Hugging Face - Introduction</title>
  <dc:creator>Prajwal Suresh</dc:creator>
  <link>https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Introduction to using 🤗 <a href="https://huggingface.co/">Hugging face</a> - <a href="https://github.com/huggingface/diffusers">Diffusers library</a> for Stable Diffusion, looking into the core components and implementing StableDiffusionPipeline using it</p>
</blockquote>
<div data-align="center">
<p><a href="https://colab.research.google.com/github/prajwal-suresh13/image_colorization/blob/master/image_colorization_cyclegan_training.ipynb"> <img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/colab.research.google.com/assets/colab-badge.svg" width="150"> </a></p>
</div>
<p><br></p>
<p>In this post I will give an introduction on using 🤗 Diffusers library to generate images using texts as input. This blog is based on the knowledge acquired while doing the <a href="https://www.fast.ai/posts/part2-2022.html"><em>‘From Deep learning foundations to Stable Diffusion’</em></a> course by <a href="https://www.fast.ai/posts/part2-2022.html">FastAI</a> . The first few lessons of the FastAI course are publicly available <a href="https://www.fast.ai/posts/part2-2022-preview.html">here</a>. Until the rest of the lessons are available, you can also checkout the first part of the <a href="https://course.fast.ai/">course</a> which is one of the best Deep Learning courses</p>
<p>At first, let’s install the diffusers library</p>
<div class="cell" data-outputid="fb0ef25e-0a88-488c-ff87-366216258407">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq accelerate transformers diffusers ftfy</span></code></pre></div>
</div>
<section id="what-is-stable-diffusion" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is Stable Diffusion</h1>
<p>Stable Diffusion is a popular text-to-image model. It is a deep learning model which takes a sentence as input and generates an image which represents the text. You can find amazing examples of images generated by the model with just text as input on the <a href="https://lexica.art/">Lexica</a> website. Stable Diffusion model can also take an image and text as input, modify the provided image and generate new images based on text.</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/pbs.twimg.com/media/FePCGVNXkAIAGG2.jpg" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored" data-anchor-id="what-is-stable-diffusion">
Fig 1: Stable Diffusion <a href="https://jalammar.github.io/illustrated-stable-diffusion/">(Image Credit)</a>
</h4>
</figcaption>
</figure>
</section>
<section id="getting-started-with-huggingface-diffusers-library" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Getting started with HuggingFace diffusers library</h1>
<section id="creating-huggingface-account-and-accepting-the-model-license" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="creating-huggingface-account-and-accepting-the-model-license"><span class="header-section-number">2.1</span> Creating HuggingFace account and accepting the model license</h2>
<p>At first, you will have to create a HuggingFace account by going to this <a href="https://huggingface.co/join">link</a>. HuggingFace provides many pretrained models for Stable Diffusion. In order to use the model, you have to accept the model license. The pretrained model we are using is <code>CompVis/stable-diffusion-v1-4</code> .Click on this <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4?text=A+pikachu+fine+dining+with+a+view+to+the+Eiffel+Tower">link</a> to accept the model license</p>
</section>
<section id="token-generation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="token-generation"><span class="header-section-number">2.2</span> Token generation</h2>
<p>User Access Tokens are the preferred way by HuggingFace to authenticate an application or notebook to Huggingface services. You can learn more about it in their <a href="https://huggingface.co/docs/hub/security-tokens#:~:text=There%20are%20plenty%20of%20ways,git%20or%20with%20basic%20authentication.">documentation</a></p>
<p>To create an access token, click on top right icon, go to Settings. Select Access Tokens and New Token. Give the token a name and the role. For following along with this post read role is sufficient. Click on Generate a Token. You can find the newly created token under User Access Token. Copy the generated token. On running <code>notebook_login()</code> command, you will be prompted to enter the token which will provide access to HuggingFace services through Jupyter Notebook</p>
<div class="cell" data-outputid="f8a63e6d-fd3d-4b50-9127-e7816d0714bf">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> notebook_login</span>
<span id="cb2-2">notebook_login()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Token is valid.
Your token has been saved in your configured git credential helpers (store).
Your token has been saved to /root/.huggingface/token
Login successful</code></pre>
</div>
</div>
</section>
<section id="stablediffusionpipeline" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="stablediffusionpipeline"><span class="header-section-number">2.3</span> StableDiffusionPipeline</h2>
<p>Next we will import the required libraries. We will set a variable <code>torch_device</code> to “cuda” if GPU is available else to “cpu”. The <code>image_grid</code> function is useful to visualize the results obtained from the StableDiffusionPipeline.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb4-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> autocast</span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> transforms <span class="im" style="color: #00769E;">as</span> tfms</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb4-6"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> logging</span>
<span id="cb4-7">logging.set_verbosity_error() <span class="co" style="color: #5E5E5E;"># Suppress unnecessary warning from CLIPTextModel</span></span>
<span id="cb4-8"></span>
<span id="cb4-9"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb4-12"><span class="im" style="color: #00769E;">from</span> matplotlib <span class="im" style="color: #00769E;">import</span> pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb4-13"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb4-14"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb4-15"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span></code></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;">#checks if gpu is available if not cpu is used</span></span>
<span id="cb5-2">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> image_grid(imgs, rows, cols):</span>
<span id="cb6-2">    w,h <span class="op" style="color: #5E5E5E;">=</span> imgs[<span class="dv" style="color: #AD0000;">0</span>].size</span>
<span id="cb6-3">    grid <span class="op" style="color: #5E5E5E;">=</span> Image.new(<span class="st" style="color: #20794D;">"RGB"</span>, size<span class="op" style="color: #5E5E5E;">=</span>(cols<span class="op" style="color: #5E5E5E;">*</span>w, rows<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;">for</span> i, img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(imgs):grid.paste(img, box<span class="op" style="color: #5E5E5E;">=</span>(i<span class="op" style="color: #5E5E5E;">%</span>cols<span class="op" style="color: #5E5E5E;">*</span>w, i<span class="op" style="color: #5E5E5E;">//</span>cols<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;">return</span> grid</span></code></pre></div>
</div>
<p>Next step is to import the StableDiffusionPipeline from diffusers library &amp; download the model weights. The model weights are stored in Huggingface repo. We have to provide the repo path to <code>from_pretrained</code>. In our case the repo path is <code>CompVis/stable-diffusion-v1-4</code> model. We then move it to GPU if available</p>
<div class="cell" data-outputid="952f0b7f-9ab9-42c9-f825-4e70065e41de">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline</span>
<span id="cb7-2"></span>
<span id="cb7-3">pipe <span class="op" style="color: #5E5E5E;">=</span> StableDiffusionPipeline.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>).to(torch_device)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f96a3018da73424da8539112c9d13d93","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d96d08d4d7404acba88ee7397c000a0b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ea42ccbfe7ce4acb99df595c19f9894f","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"63845d86d488489e98a1b8c60ecb9c3d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"516755158b1c423e8232916ee5051f2b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"95620e1bf8f945c0a2532a8fe4f99a40","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5e72883081004f5a9feab6b218cccb52","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"92121f8aa17446f19a463dfa6360a2bc","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3944d5dab6d448b892e8a21a3cea2499","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"16d34efa17674a08bf18f9e3e5165535","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0942e4625479496380238bc28689adf0","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fdf7483ba3b24bd1826f6f054e7e0673","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"628ebf54409f4ebdb52eac905632c248","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ac7cc317a55e4b25adb44db65d9677bd","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"06f9bc93acb244f49d82757cf87a6147","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4a47bbbd34094f09a4ef42f8cba93b28","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e8ec4855395f4f57a08fee013952b357","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Now let’s pass a textual prompt to the pretrained model to generate image</p>
<div class="cell" data-outputid="1a8e3a8f-4859-4826-90f3-750efdad4aca">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb8-2">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"a photo of a puppy wearing suit"</span></span>
<span id="cb8-3">pipe(prompt).images[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5e054372f8474c6ba8748606934405f9","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 2: An example of image generated by the StableDiffusion pipeline
</h4>
</figcaption>
<p>
</p>
<p>You can also pass multiple textual prompts and the pipeline will generate images for each prompt</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">prompts <span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'a photograph of a dog wearing hat and glasses'</span>,</span>
<span id="cb9-2">          <span class="st" style="color: #20794D;">'a photograph of an astronaut riding horse '</span>,</span>
<span id="cb9-3">          <span class="st" style="color: #20794D;">'an oil painting of dog'</span>]</span></code></pre></div>
</div>
<div class="cell" data-outputid="46eeab4e-66ad-4ecf-a552-bf14632c3a19">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb10-2">images <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts).images</span>
<span id="cb10-3"><span class="bu" style="color: null;">len</span>(images)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"04f155a5f8bd4e04940523e29db4bba4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>3</code></pre>
</div>
</div>
<div class="cell" data-outputid="b7a342cc-679b-41b4-ac40-8f56f1898dbe">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">image_grid(images, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 3: Images generated by pipeline for multiple prompts(A photograph of a dog wearing hat and glasses, A photograph of an astronaut riding horse, An oil painting of dog”)
</h4>
</figcaption>
</section>
</section>
<section id="callbacks" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Callbacks</h1>
<p>You might have observed that before producing the output, there is a progress bar with certain number of steps being completed after which image is generated. Let’s visualize what happens during each step using <code>latents_callback</code> function</p>
<div class="cell" data-outputid="36628492-16b3-48da-b739-a28102ceaa1a">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">vae <span class="op" style="color: #5E5E5E;">=</span> pipe.vae</span>
<span id="cb13-2">images <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb13-3"></span>
<span id="cb13-4"><span class="kw" style="color: #003B4F;">def</span> latents_callback(i, t, latents):</span>
<span id="cb13-5">    latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb13-6">    image <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb13-7">    image <span class="op" style="color: #5E5E5E;">=</span> (image <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb13-8">    image <span class="op" style="color: #5E5E5E;">=</span> image.cpu().permute(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>).numpy()</span>
<span id="cb13-9">    images.extend(pipe.numpy_to_pil(image))</span>
<span id="cb13-10"></span>
<span id="cb13-11"></span>
<span id="cb13-12">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb13-13"><span class="cf" style="color: #003B4F;">for</span> prompt <span class="kw" style="color: #003B4F;">in</span> prompts:</span>
<span id="cb13-14">    final_images <span class="op" style="color: #5E5E5E;">=</span> pipe(prompt, callback<span class="op" style="color: #5E5E5E;">=</span>latents_callback, callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>).images[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb13-15">    images.append(final_images)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"35c068ae5e11412a8fc66d7f1d637810","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8e80a4d3b8b947fa9b24988a3215e2db","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b77a65acce4340cea3703c948ab7f89f","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="7c4be038-55a2-499f-efd1-75559a52b693">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">image_grid(images, rows<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(prompts),cols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored" data-anchor-id="callbacks">
Fig 4: Visualizing Diffusion steps
</h4>
</figcaption>
<p>You can see in the above image that at first step for all prompts, there is only random noise and as the steps are increased we start to get images representing the prompts. So how is the model able to generate images from random noise and text? How is StableDiffusion model working?. Let’s try to answer our questions by looking under the hood of StableDiffusionPipeline</p>
</section>
<section id="deep-dive" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Deep Dive</h1>
<p>Stable Diffusion is based on a progressive denoising algorithm that is able to create images from pure random noise. These models start with pure random noise and with each step it improves the quality of generated images. Models in this family are known as diffusion models. In case of our above examples in Fig 4, the model took 51 steps and as you can see it improves the quality of generated images with each step. We can define the number of steps the model can take with <code>num_inference_steps</code> parameter.</p>
<p>Stable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>. The difference between the Standard diffusion and latent diffusion models is that when generating high-resolution images in latent diffusion the model works on the to latent(compressed) representation of the images whereas the Standard diffusion model works on actual pixel space.</p>
<p>For example, consider an image of size 512x512 needs to be generated. The Standard diffusion model works on this pixel space which is of 512x512 and starts with noise of dimensions 512x512. This is expensive as it takes more memory. But in case of Latent Diffusion, the model works on noise which in our above examples is 64x64 and then scaled to 512x512 size images. This scaling is done by models known as Autoencoder which will be explained in next section.</p>
<p>The three main components in latent diffusion are <br></p>
<ol type="1">
<li>An autoencoder, in our case, a Variational Auto Encoder (VAE)<br></li>
<li>Text-encoder, in our case, a CLIP Text Enocder<br></li>
<li>U-Net<br></li>
</ol>
<section id="autoencoder" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="autoencoder"><span class="header-section-number">4.1</span> Autoencoder</h2>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/d3i71xaburhd42.cloudfront.net/b1786e74e233ac21f503f59d03f6af19a3699024/2-Figure1-1.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 5:Autoencoder <a href="https://www.semanticscholar.org/paper/A-Better-Autoencoder-for-Image%3A-Convolutional-Zhang/b1786e74e233ac21f503f59d03f6af19a3699024">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>Autoencoders are models which take images as input and produce same images as output. So you might think how is this model even useful if it is producing same output as input. It is useful because autoencoder is made up of two parts and after training autoencoder these two parts can be used independently. The two parts are<br> 1. Encoder - Takes image as input and converts it to low dimensional latent representation ie compressed data<br> 2. Decoder - Takes compressed data and converts it back into an image<br></p>
<p>These models can be used for various applications. For example, it can be used as a compression application, where the encoder squishes the image into lower dimensions which uses less memory for storing and when the image is required we can recreate by passing the squished data to decoder. It is to be noted that the compression and decompression by the encoder-decoder is not lossless.</p>
<p>The Latent diffusion models as mentioned in the previous section works on the latent space. This latent space is produced by Encoder of Variational Autoencoder(VAE) and once we have our desired latent outputs produced by the model through diffusion process, we can convert them back to high-resolution images using VAE’s decoder</p>
<p>In case of Image to Image StableDiffusion Pipeline, the input image is passed through the VAE’s encoder and the latent inputs obtained combined with random noise are used to perform diffusion. The latent outputs produced by the diffusion process is then converted back to high resolution image using VAE’s decoder</p>
<p>In case of Text to Image StableDiffusion pipeline, only random noise is used to perform diffusion and the latent outputs produced is converted back high-resolution image using VAE’s decoder</p>
<p>Now let’s look into the VAE code. HuggingFace Diffusers library already provides us with a pretrained Autoencoder to use which can be imported as below</p>
<div class="cell" data-outputid="de34fcde-22ab-43c6-c4bd-780f84ff32f7">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL</span>
<span id="cb15-2">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span>)</span>
<span id="cb15-3">vae.to(torch_device)</span></code></pre></div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> pil_to_latents(input_img):</span>
<span id="cb16-2">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb16-3">        latent <span class="op" style="color: #5E5E5E;">=</span> vae.encode(tfms.ToTensor()(input_img).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).to(torch_device)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb16-4">    <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latent.latent_dist.sample()</span>
<span id="cb16-5"></span>
<span id="cb16-6"><span class="kw" style="color: #003B4F;">def</span> create_image(t):</span>
<span id="cb16-7">    image <span class="op" style="color: #5E5E5E;">=</span> (t<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>).detach().cpu().permute(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">0</span>).numpy()</span>
<span id="cb16-8">    <span class="cf" style="color: #003B4F;">return</span> Image.fromarray((image<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">255</span>).<span class="bu" style="color: null;">round</span>().astype(<span class="st" style="color: #20794D;">"uint8"</span>))</span>
<span id="cb16-9"></span>
<span id="cb16-10"><span class="kw" style="color: #003B4F;">def</span> latents_to_pil(latents):</span>
<span id="cb16-11">    latents <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.18215</span>) <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb16-12">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb16-13">        images <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample</span>
<span id="cb16-14">    pil_images <span class="op" style="color: #5E5E5E;">=</span>[create_image(image) <span class="cf" style="color: #003B4F;">for</span> image <span class="kw" style="color: #003B4F;">in</span> images]</span>
<span id="cb16-15">    <span class="cf" style="color: #003B4F;">return</span> pil_images</span></code></pre></div>
</details>
</div>
<p>Let’s download an image from internet, compress the image using Encoder and view the latent representation. We will use <code>pil_to_latents</code> helper function to do it</p>
<div class="cell" data-outputid="4ad6affd-867f-4aac-e839-17031a61d003">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="op" style="color: #5E5E5E;">!</span>curl <span class="op" style="color: #5E5E5E;">--</span>output macaw.jpg <span class="st" style="color: #20794D;">'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'</span></span>
<span id="cb17-2">input_image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(<span class="st" style="color: #20794D;">'macaw.jpg'</span>).resize((<span class="dv" style="color: #AD0000;">512</span>, <span class="dv" style="color: #AD0000;">512</span>))</span>
<span id="cb17-3">input_image</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 62145  100 62145    0     0   240k      0 --:--:-- --:--:-- --:--:--  240k</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="64254c93-176a-40f8-cff4-889c28be06a8">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">encoded <span class="op" style="color: #5E5E5E;">=</span> pil_to_latents(input_image)</span>
<span id="cb19-2">fig, axs <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">4</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb19-3"><span class="cf" style="color: #003B4F;">for</span> c <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb19-4">    axs[c].imshow(encoded[<span class="dv" style="color: #AD0000;">0</span>][c].cpu(), cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Greys'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 6: Latent Representation of the Input image
</h4>
</figcaption>
<div class="cell" data-outputid="3d0180b5-d954-4cb7-d0b3-e660f0305ee7" data-scrolled="true">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">encoded.shape,tfms.ToTensor()(input_image).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>(torch.Size([1, 4, 64, 64]), torch.Size([1, 3, 512, 512]))</code></pre>
</div>
</div>
<p>As you can see the VAE has compressed 3x512x512 image into 4x64x64 image. That is a compression ratio of 48x. These latent representation has captured information about the original image and on passing them to the VAE’s decoder provide the original image. For this we use the <code>latents_to_pil</code> function.</p>
<div class="cell" data-outputid="0d76a0f5-2a02-4e06-dda8-701e636e5b00">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">decoded <span class="op" style="color: #5E5E5E;">=</span> latents_to_pil(encoded)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb22-2">decoded</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So we understood how Stable diffusion can create images from random noise using diffusion process. But how is it able to generate images representing the text? Let’s find out in the next section.</p>
</section>
<section id="text-encoder" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="text-encoder"><span class="header-section-number">4.2</span> Text Encoder</h2>
<p>Deep Learning models only understand numbers and not text. So inorder to convert the text data into numbers which our models can work with Tokenizers and Text Encoders are used</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/developers.google.com/static/machine-learning/guides/text-classification/images/EmbeddingLayer.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 7 : Tokenizer and Text Encoder <a href="https://developers.google.com/machine-learning/guides/text-classification/step-3">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>This process of converting text to numbers is done in two steps:<br> 1. Tokenizer - The prompt provided is broken down into words and then it uses a lookup table to convert them into a number. As you can see in the Fig 7 , the input sentences are converted to numbers using the lookup table<br> 2. Text Encoder - This converts numbers into a high dimensional vectors which captures the meaning of the words. These are called embeddings. In Fig 7, the Text Encoder is represented as Embedding layer.<br></p>
<section id="what-is-embedding" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="what-is-embedding"><span class="header-section-number">4.2.1</span> What is embedding?</h3>
<p>Embedding is a high dimensional vector that allows words with similar meaning to have the same representation. It can approximate the meaning of the word. An embedding with 50 values holds the capability of representing 50 unique features. This is known as embedding size. In the Fig 7, the embedding size used to represent each word is 4</p>
</section>
<section id="how-clip-model-works" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="how-clip-model-works"><span class="header-section-number">4.2.2</span> How CLIP model works</h3>
<p>The embedding from the text encoder in the Fig 7 represents the meaning of each word in the given prompt. But we also need the same embeddings to represent an image which is described in the prompt. We need a model that connects text and images. CLIP model acts as bridge that connects texts and images</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/cdn.dida.do/blog/20210621_fg_clip/contrastive_pretraining.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 8 : CLIP model <h7><a href="https://arxiv.org/abs/2103.00020">(Image Credit)</a></h7>
</h4>
</figcaption>
</figure>
<p>The CLIP model consists of two sub-models called encoders:<br> 1. Text Encoder - Converts text to embeddings<br> 2. Image encoder - Converts Image to embedding<br></p>
<p>The embeddings produced by both the encoders needs to be similar if both text and image represent same thing and should be different otherwise</p>
<p>For example, lets consider the embedding size as 4. Suppose the text “pepper the aussie pup” when passed to Text encoder produces embeddings (0, 0.5, 0.8, 0.1) then the embeddings generated when image of the Pepper the Aussie Pup is passed to the Image encoder should be similar to that produced by the text encoder ie (0.05, 0.52, 0.78, 0.13). You can learn more about the CLIP model <a href="https://openai.com/blog/clip/">here</a></p>
<p>We use the pretrained Text Encoder of the CLIP model to generate embeddings for our prompts. Let’s look into the CLIPTextEncoder code</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'A picture of a puppy'</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTokenizer, CLIPTextModel</span>
<span id="cb24-2"></span>
<span id="cb24-3">tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-large-patch14"</span>)</span>
<span id="cb24-4">text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-large-patch14"</span>)</span>
<span id="cb24-5">text_encoder <span class="op" style="color: #5E5E5E;">=</span> text_encoder.to(torch_device)</span></code></pre></div>
</div>
<p>Tokenizer expects 77 length vector. So the padding token 49407 is repeated till the length of 77 is reached. The embedding size of the CLIPTextModel is 768.</p>
<div class="cell" data-outputid="1e8a5042-9f27-464b-b124-d497d95ceb51">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">text_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer(prompt, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>tokenizer.model_max_length, trucation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb25-2">text_input[<span class="st" style="color: #20794D;">'input_ids'</span>][<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([49406,   320,  1674,   539,   320,  6829, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407])</code></pre>
</div>
</div>
<p>We can find to which word each number is mapped using tokenizer’s decoder.</p>
<div class="cell" data-outputid="c7891799-2e58-4971-cc10-3ba27512f4df">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> text_input[<span class="st" style="color: #20794D;">'input_ids'</span>][<span class="dv" style="color: #AD0000;">0</span>][:<span class="dv" style="color: #AD0000;">8</span>]:<span class="bu" style="color: null;">print</span>(t, tokenizer.decoder.get(<span class="bu" style="color: null;">int</span>(t)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(49406) &lt;|startoftext|&gt;
tensor(320) a&lt;/w&gt;
tensor(1674) picture&lt;/w&gt;
tensor(539) of&lt;/w&gt;
tensor(320) a&lt;/w&gt;
tensor(6829) puppy&lt;/w&gt;
tensor(49407) &lt;|endoftext|&gt;
tensor(49407) &lt;|endoftext|&gt;</code></pre>
</div>
</div>
<p>The numbers from Tokenizer is passed to Text encoder to generate embeddings</p>
<div class="cell" data-outputid="e67cc76f-46e3-4a55-9ed7-b6c81dadd3ca">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">output_embeddings <span class="op" style="color: #5E5E5E;">=</span> text_encoder(text_input.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb29-2">output_embeddings</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],
         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],
         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],
         ...,
         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],
         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],
         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],
       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="classifier--free-guidance" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="classifier--free-guidance"><span class="header-section-number">4.2.3</span> Classifier- Free Guidance</h3>
<p>Classifier-Free Guidance is a method to increase the adherence of the output to the text provided as input. Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. The default value of guidance is 7.5</p>
<p>In order to implement this, along with the embeddings from the prompt we also pass embeddings for an empty string.This is known as Unconditional embedding. This helps the model to go in which ever direction it wants as long as it results in a reasonably-looking image. The prediction are then calculated as follows</p>
<pre><code>prediction = unconditional_output + guidance(text_output - unconditional_output)
</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">max_length <span class="op" style="color: #5E5E5E;">=</span> text_input.input_ids.shape[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb32-2">uncond_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer([<span class="st" style="color: #20794D;">""</span>] <span class="op" style="color: #5E5E5E;">*</span> <span class="bu" style="color: null;">len</span>(prompts), padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>max_length, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb32-3">uncond_embeddings <span class="op" style="color: #5E5E5E;">=</span> text_encoder(uncond_input.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb32-4">emb <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond_embeddings, output_embeddings])</span></code></pre></div>
</div>
</section>
</section>
<section id="unet" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="unet"><span class="header-section-number">4.3</span> UNet</h2>
<p>The UNet is the model which produces latent outputs by removing noise.</p>
<p>UNet model takes three inputs: <br> 1. Noisy latent or Noise - If initial image is provided, noisy latents are produced by adding VAE encoder’s latents and noise otherwise it takes pure noise as input and produces new image based on the textual description<br> 2. Timestep - Timestep is the number of times the diffusion process is repeated. It takes as input the current timestep<br> 3. Embeddings - Embeddings generated by CLIP for the input prompts and unconditional embeddings<br></p>
<p>The UNet model predicts the noise which is then subtracted from the Noisy latents. This process is repeated for the timesteps mentioned and at the end, the latent outputs produced are passed to VAE’s decoder to generate High-resolution image</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/miro.medium.com/max/720/0*4CMZbQvwXaSjQRRX.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 9 : UNet <a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8#:~:text=Stable%20diffusion%20only%20uses%20a,are%20similar%20in%20latent%20space.">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>Let’s look into the code for UNet. Along with UNet we also import a scheduler. Scheduler determines the amount of noise to be added to latent at a given step in diffusion process</p>
<div class="cell" data-outputid="58c66dde-e462-4864-f190-295d27059629">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> UNet2DConditionModel, LMSDiscreteScheduler</span>
<span id="cb33-2"></span>
<span id="cb33-3">scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb33-4">scheduler.set_timesteps(<span class="dv" style="color: #AD0000;">51</span>)</span>
<span id="cb33-5"></span>
<span id="cb33-6">unet <span class="op" style="color: #5E5E5E;">=</span> UNet2DConditionModel.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>)</span>
<span id="cb33-7">unet.to(torch_device)</span></code></pre></div>
</div>
<p>You can see the amount of noise added in each step below. This is called sigmas. The noise added is highest at first step and is gradually decreased in the following steps until it becomes zero at the last step</p>
<div class="cell" data-outputid="5d458232-4a69-4fb3-95cc-658d38e1f380">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="bu" style="color: null;">print</span>(scheduler.sigmas)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([14.6146, 12.9679, 11.5454, 10.3129,  9.2414,  8.3072,  7.4900,  6.7731,
         6.1422,  5.5854,  5.0924,  4.6547,  4.2650,  3.9169,  3.6051,  3.3251,
         3.0728,  2.8449,  2.6383,  2.4507,  2.2797,  2.1235,  1.9804,  1.8488,
         1.7276,  1.6156,  1.5118,  1.4154,  1.3255,  1.2415,  1.1629,  1.0890,
         1.0193,  0.9535,  0.8912,  0.8319,  0.7754,  0.7213,  0.6695,  0.6195,
         0.5712,  0.5242,  0.4784,  0.4333,  0.3885,  0.3437,  0.2981,  0.2505,
         0.1989,  0.1379,  0.0292,  0.0000])</code></pre>
</div>
</div>
<p>Let’s add noise to the latents obtained from VAE’s encoder earlier and see how U-Net removes noise</p>
<div class="cell" data-outputid="ab6a7c91-f609-4598-b6a4-6005e568f2bb">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">noise <span class="op" style="color: #5E5E5E;">=</span> torch.randn_like(encoded)</span>
<span id="cb36-2">encoded_and_noised <span class="op" style="color: #5E5E5E;">=</span> scheduler.add_noise(encoded, noise, timesteps<span class="op" style="color: #5E5E5E;">=</span>torch.tensor([scheduler.timesteps[<span class="dv" style="color: #AD0000;">40</span>]]))</span>
<span id="cb36-3">latents_to_pil(encoded_and_noised)[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="f27d1314-aa60-4c0d-e070-73c990b9587d">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="co" style="color: #5E5E5E;">#Unconditional prompt</span></span>
<span id="cb37-2">prompt<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">""</span>]</span>
<span id="cb37-3"></span>
<span id="cb37-4"><span class="co" style="color: #5E5E5E;">## tokenizing and getting embeddings from clip model</span></span>
<span id="cb37-5">text_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer(prompt, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>tokenizer.model_max_length, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb37-6"></span>
<span id="cb37-7"><span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb37-8">    text_embeddings<span class="op" style="color: #5E5E5E;">=</span>text_encoder(text_input.input_ids.to(<span class="st" style="color: #20794D;">"cuda"</span>))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb37-9"></span>
<span id="cb37-10"><span class="co" style="color: #5E5E5E;">#Using U-Net to predict noise</span></span>
<span id="cb37-11">latent_model_input <span class="op" style="color: #5E5E5E;">=</span> torch.cat([encoded_and_noised.to(<span class="st" style="color: #20794D;">"cuda"</span>).<span class="bu" style="color: null;">float</span>()])</span>
<span id="cb37-12"><span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb37-13">    noise_pred <span class="op" style="color: #5E5E5E;">=</span> unet(latent_model_input, <span class="dv" style="color: #AD0000;">40</span>, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>text_embeddings)[<span class="st" style="color: #20794D;">"sample"</span>]</span>
<span id="cb37-14"></span>
<span id="cb37-15"><span class="co" style="color: #5E5E5E;">#Visualize after subtracting noise</span></span>
<span id="cb37-16">latents_to_pil(encoded_and_noised <span class="op" style="color: #5E5E5E;">-</span>noise_pred)[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>You can see that the output image is clearer than the provided noisy image.</p>
</section>
</section>
<section id="implementing-stablediffusionpipeline" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Implementing StableDiffusionPipeline</h1>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/miro.medium.com/max/720/0*z5eQUBRBVtgD3Vgv.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 10 : Diffusion process <a href="https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>
Now that we have understood the core components of the StableDiffusion, let’s summarize the diffusion process<br> 1. The prompts provided by the end user is converted to embeddings by the CLIPTextModel<br> 2. If input image is provided, <br>  - It is converted to latents by the VAE’s encoder and combined with noise from the Scheduler. <br> Else,<br>  - Random noise is generated and combined with noise from the Scheduler <br> 3. The embeddings (textual and unconditional) and the latents are provided as input to the UNet along with the current timestep <br> 4. UNet predicts noise and this noise is removed from the latents. This process is repeated N times ie num_inference_steps <br> 5. The latents produced by the UNet are passed to VAE’s decoder to get high resolution image <br>
</p>
<p>Finally, let’s try to implement <code>StableDiffusionPipeline</code> using these core components. Let’s call our pipeline as <code>SDPipeline</code></p>
<section id="intialize-models" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="intialize-models"><span class="header-section-number">5.1</span> Intialize models</h2>
<p>First lets import the model required from diffusers and transformers library. We then create a class <code>SDPipeline</code> and the <code>__init__</code> method takes model_path as input which in our case is <code>CompVis/stable-diffusion-v1-4</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb38-2"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">class</span> SDPipeline():</span>
<span id="cb39-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model_path):</span>
<span id="cb39-3">        <span class="va" style="color: #111111;">self</span>.model_path<span class="op" style="color: #5E5E5E;">=</span>model_path</span>
<span id="cb39-4">        <span class="va" style="color: #111111;">self</span>.vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(model_path,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb39-5">        <span class="va" style="color: #111111;">self</span>.unet<span class="op" style="color: #5E5E5E;">=</span>UNet2DConditionModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb39-6">        <span class="va" style="color: #111111;">self</span>.text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"text_encoder"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb39-7">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"tokenizer"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16)</span>
<span id="cb39-8">        <span class="va" style="color: #111111;">self</span>.scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span></code></pre></div>
</div>
</section>
<section id="tokenize-texts" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="tokenize-texts"><span class="header-section-number">5.2</span> Tokenize Texts</h2>
<p>As explained under Classifier-Free Guidance, we not only pass the embeddings of the prompt but also embeddings for empty string. So let’s create a function which tokenizes and provides embeddings. We can make use of the dynamic nature of the Python language to add this function dynamically to class <code>SDPipeline</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="kw" style="color: #003B4F;">def</span> encode_text(<span class="va" style="color: #111111;">self</span>, prompts, maxlen<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb40-2">    <span class="cf" style="color: #003B4F;">if</span> maxlen <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: maxlen <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer.model_max_length</span>
<span id="cb40-3">    inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer(prompts, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>maxlen, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb40-4">    <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.text_encoder(inp.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>].half()</span>
<span id="cb40-5"></span>
<span id="cb40-6">SDPipeline.encode_text <span class="op" style="color: #5E5E5E;">=</span> encode_text</span></code></pre></div>
</div>
</section>
<section id="diffusion-loop" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="diffusion-loop"><span class="header-section-number">5.3</span> Diffusion Loop</h2>
<p>The diffusion loop includes all the steps which we summarized at the starting of this section. We can also add this to class <code>SDPipeline</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="kw" style="color: #003B4F;">def</span> diff_pipeline(<span class="va" style="color: #111111;">self</span>,prompts, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb41-2">        <span class="co" style="color: #5E5E5E;">#calculate number of prompts</span></span>
<span id="cb41-3">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb41-4">        </span>
<span id="cb41-5">        <span class="co" style="color: #5E5E5E;">#generate text_embeddings</span></span>
<span id="cb41-6">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb41-7">        </span>
<span id="cb41-8">        <span class="co" style="color: #5E5E5E;">#generate unconditional embeddings and concat them</span></span>
<span id="cb41-9">        uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb41-10">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb41-11">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb41-12">        </span>
<span id="cb41-13">        <span class="co" style="color: #5E5E5E;">#generate random noise and add noise</span></span>
<span id="cb41-14">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb41-15">        </span>
<span id="cb41-16">        <span class="co" style="color: #5E5E5E;">#Setting number of steps in scheduler</span></span>
<span id="cb41-17">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb41-18">        </span>
<span id="cb41-19">        <span class="co" style="color: #5E5E5E;">#Add noise to latents</span></span>
<span id="cb41-20">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb41-21"></span>
<span id="cb41-22">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(<span class="va" style="color: #111111;">self</span>.scheduler.timesteps)):</span>
<span id="cb41-23">            <span class="co" style="color: #5E5E5E;"># scale the latents for particular timestep</span></span>
<span id="cb41-24">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb41-25">            </span>
<span id="cb41-26">            <span class="co" style="color: #5E5E5E;">#predict noise using UNet</span></span>
<span id="cb41-27">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb41-28">                </span>
<span id="cb41-29">            <span class="co" style="color: #5E5E5E;">#Perform Classifier-Free Guidance    </span></span>
<span id="cb41-30">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb41-31">            </span>
<span id="cb41-32">            <span class="co" style="color: #5E5E5E;">#Removing predicted noise from the latents</span></span>
<span id="cb41-33">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb41-34">            </span>
<span id="cb41-35">            <span class="co" style="color: #5E5E5E;">#run callbacks if any are added</span></span>
<span id="cb41-36">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb41-37">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb41-38">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb41-39">        </span>
<span id="cb41-40">        <span class="co" style="color: #5E5E5E;">#after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated</span></span>
<span id="cb41-41">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb41-42">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.vae.decode(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.18125</span> <span class="op" style="color: #5E5E5E;">*</span> latents).sample</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">SDPipeline.<span class="fu" style="color: #4758AB;">__call__</span> <span class="op" style="color: #5E5E5E;">=</span> diff_pipeline</span></code></pre></div>
</div>
</section>
<section id="adding-callback" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="adding-callback"><span class="header-section-number">5.4</span> Adding Callback</h2>
<p>Let’s create a callback as well, which will help to visualize the steps like we did at the starting . We will create a class which stores intermediate steps images. Since they are latents we use VAE to decode and display for us</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb43-2"><span class="kw" style="color: #003B4F;">class</span> Latents_Callback:</span>
<span id="cb43-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, prompts):</span>
<span id="cb43-4">        <span class="va" style="color: #111111;">self</span>.imgs <span class="op" style="color: #5E5E5E;">=</span> [[] <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompts))]</span>
<span id="cb43-5">    </span>
<span id="cb43-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, i, t, latents):</span>
<span id="cb43-7">        latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb43-8">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(latents.shape[<span class="dv" style="color: #AD0000;">0</span>]):</span>
<span id="cb43-9">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample[i]</span>
<span id="cb43-10">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> create_image(callback_imgs)</span>
<span id="cb43-11">            <span class="va" style="color: #111111;">self</span>.imgs[i].append(callback_imgs)</span></code></pre></div>
</div>
</section>
<section id="running-our-sdpipeline" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="running-our-sdpipeline"><span class="header-section-number">5.5</span> Running our SDPipeline</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">prompts <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb44-2">    <span class="st" style="color: #20794D;">'a photograph of an astronaut riding a horse'</span>,</span>
<span id="cb44-3">    <span class="st" style="color: #20794D;">'an oil painting of an astronaut riding a horse in the style of grant wood'</span>,</span>
<span id="cb44-4">    <span class="st" style="color: #20794D;">'a painting of dog wearing suit'</span></span>
<span id="cb44-5">]</span></code></pre></div>
</div>
<p>Initializing callbacks and SDPipeline objects</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">lc <span class="op" style="color: #5E5E5E;">=</span> Latents_Callback(prompts)</span></code></pre></div>
</div>
<div class="cell" data-outputid="c834a081-77be-4cea-f069-c0dec736166a">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">pipe <span class="op" style="color: #5E5E5E;">=</span> SDPipeline(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a1c1f87c8a6942e18f6d0fe62400a1c2","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bf768efb17549d0bec616fbaf9d15c9","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ef25f6769af44ee8a9de78523d1ebe9d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"81ab569df6bf44139250ea42db8b5b75","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ef4d487ab4d64cb7b5a3f24aa90a1809","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9938302db04e4511b25604a679e1c387","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Let’s pass the prompts and the callback object to the SDPipeline object <code>pipe</code>. We can see the images generated for each prompt and can also visualize the steps by displaying images saved in the callback object</p>
<div class="cell" data-outputid="df3109c2-d060-4760-a168-40a501848bc4">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">images <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts, callbacks<span class="op" style="color: #5E5E5E;">=</span>[lc], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"00db2403a2d94ced973b0072df15a1c7","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="bf277a09-9110-4ce0-8db9-187117aeaebd">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">imgs <span class="op" style="color: #5E5E5E;">=</span> [create_image(i) <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> images]</span>
<span id="cb48-2">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(imgs))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="ed34d5b0-081f-4831-92af-41b3dc9fd76a">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">rows, cols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts), <span class="bu" style="color: null;">len</span>(lc.imgs[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb49-2">imgs <span class="op" style="color: #5E5E5E;">=</span>[img <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompts)) <span class="cf" style="color: #003B4F;">for</span> img <span class="kw" style="color: #003B4F;">in</span> lc.imgs[i]]</span>
<span id="cb49-3">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span>rows, cols<span class="op" style="color: #5E5E5E;">=</span>cols)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>In this post, we have covered a brief introduction of Stable diffusion. After that, in later sections we understood each components of the Stable Diffusion model along with code. At the end, we implemented <code>SDPipeline</code> using the core components, generated images for multiple prompts and also visualized the images generated at particular timesteps.</p>
<p>I hope you enjoyed reading it. If there is any feedback, feel free to reach out on <a href="https://www.linkedin.com/in/prajwal-s-1416061b3/">Linkedin</a> or on <a href="prajwalsuresh13@gmail.com">mail</a></p>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<ul>
<li><a href="https://www.fast.ai/posts/part2-2022-preview.html">FastAI course</a></li>
<li><a href="https://aayushmnit.com/blog.html">Aayush Agrawal’s blog</a></li>
</ul>


</section>

 ]]></description>
  <category>Stable Diffusion</category>
  <guid>https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html</guid>
  <pubDate>Mon, 21 Nov 2022 18:30:00 GMT</pubDate>
  <media:content url="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
