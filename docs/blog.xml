<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Prajwal Suresh</title>
<link>https://prajwalsuresh.com/blog.html</link>
<atom:link href="https://prajwalsuresh.com/blog.xml" rel="self" type="application/rss+xml"/>
<description>Prajwal&#39;s personal website</description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Wed, 30 Nov 2022 18:30:00 GMT</lastBuildDate>
<item>
  <title>Stable diffusion - Dreambooth</title>
  <dc:creator>Prajwal Suresh</dc:creator>
  <link>https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Training a new concept to Stable Diffusion model using Dreambooth, generating images of the new concept using the trained model, and saving the pipeline to HuggingFace Hub</p>
</blockquote>
<br>
<div data-align="center">
<p><a href="https://colab.research.google.com/github/prajwal-suresh13/diffusion/blob/master/03_pintu_dreambooth.ipynb"> <img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/https:/colab.research.google.com/assets/colab-badge.svg" width="150"> </a></p>
</div>
<p><br></p>
<p>This is the third post on Stable Diffusion. Check out the previous posts through links below if you haven’t read them yet <br><br> 1. <strong>Part 1</strong> - <a href="https://prajwal-suresh13.github.io/website/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html">Stable diffusion - Introduction</a>.<br> 2. <strong>Part 2</strong> - <a href="https://prajwal-suresh13.github.io/website/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html">Stable diffusion - Negative Prompt, Image to Image &amp; Textual Inversion</a></p>
<p>In previous posts, we learnt about Stable Diffusion and its key components VAE, UNet, Scheduler &amp; CLIPTextModel. We also learnt the variations provided by the model such as Negative Prompt and Image to Image pipeline. We implemented all these and named our pipeline as <code>SDPipeline</code> . We also learnt how we can use Textual Inversion to train a new concept to the Stable Diffusion model.</p>
<p>In this post, we will be covering Dreambooth which is similar to Textual Inversion, train a new concept using it and push our model to HuggingFace Hub, from where anyone can use our fine-tuned model. This post is based on the HuggingFace Notebook found <a href="https://github.com/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb">here</a>. So let’s get started</p>
<section id="what-is-dreambooth" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is Dreambooth?</h1>
<p>Stable Diffusion results are amazing. But what if you want to generate images of subjects that are personal to you. Can it render images of your cute pet dog having fun at the beach? What about you being the main character of superhero movie saving the World?. Yes, you absolutely can personalize the Stable Diffusion model using Dreambooth</p>
<figure align="center" class="figure">
<figure class="figure">
<img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/https:/tryolabs.com/assets/blog/2022-10-25-the-guide-to-fine-tuning-stable-diffusion-with-your-own-images/stable-diffusion21-d3f64be213.jpg" width="380" height="280" style="float:right" class="figure-img">
<figcaption class="figure-caption">
<a href="https://tryolabs.com/blog/2022/10/25/the-guide-to-fine-tuning-stable-diffusion-with-your-own-images">Credits</a>
</figcaption>
</figure>
<figure class="figure">
<img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/02.png" width="280" height="280" style="float:left" class="figure-img">
</figure>
</figure>
<p>Dreambooth is an approach to teach a new concept by fine-tuning a pretrained text-to-image model such that it binds a unique token with the new concept of images. If you are aware of fine-tuning vision models, you might be thinking at this point that it will take hundreds of images and hours of training to fine-tune Stable Diffusion model, but to our surprise only a minimum of 5 images of the new subject is required!</p>
<p>The image on the right above is generated from the model which I fine-tuned for around 30 minutes on Tesla T4 GPU and 16GB RAM with just 6 images of my pet dog. Amazing isn’t it!</p>

<style>
      .parent {
        position: relative;
        top: 0;
        left: 0;
      }
      .image1 {
        position: relative;
        top: 0;
        left: 0;
        border: 1px solid #000000;
      }
      .image2 {
        position: absolute;
        top: 30px;
        left: 30px;
        border: 1px solid #000000;
      }
    </style>

<figure align="center" class="parent figure">
<img class="image1 figure-img" src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/https:/encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrC5Tpk9HcrkzqNa9TnHv5fkwvD8XOqjjAmv0mFMbo&amp;s" width="800" height="450"> <img class="image2 figure-img" src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/https:/dreambooth.github.io/DreamBooth_files/system.png" width="700">
<figcaption align="center" class="figure-caption">
<h4 class="anchored" data-anchor-id="what-is-dreambooth">
Fig 1: Dreambooth approach <a href="https://dreambooth.github.io/">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>Dreambooth approach of fine-tuning the text-to image diffusion model is as follows:<br> 1. We pair the input images of our interested subject and text prompt containing a unique identifier and fine tune the model. (Example - for unique identifier you can use a token which is less frequently used such as sks.So the prompt will be “A photo of sks dog”)<br> 2. We also pass the class of the subject as a prompt while fine-tuning the model and apply class-specific prior preservation loss (Example - “A photo of a dog”) <br></p>
<p>Prior Preservation doesn’t seem to make huge difference except when trained on faces. So prior preservation is not used while training in this post. Checkout this amazing detailed <a href="https://wandb.ai/psuraj/dreambooth/reports/Training-Stable-Diffusion-with-Dreambooth--VmlldzoyNzk0NDc3">article</a> in which they have experimented training dreambooth with different settings. You can follow the HuggingFace <a href="https://github.com/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb">notebook</a> in which they have implemented prior preservation</p>
<p>Let’s install the required libraries and load the <code>CompVis/stable-diffusion-v1-4</code> models in the next section</p>
</section>
<section id="getting-started" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Getting started</h1>
<p>At first, we will be connecting to Google Drive from Google Colab. This allows us to read images saved in Drive for training model and also save the trained model to the Drive</p>
<div class="cell" data-outputid="cde1a5ae-0973-4294-a35c-40486c737564">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> google.colab <span class="im" style="color: #00769E;">import</span> drive</span>
<span id="cb1-2">drive.mount(<span class="st" style="color: #20794D;">'/content/gdrive'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mounted at /content/gdrive</code></pre>
</div>
</div>
<p>We will be downloading the required libraries as we did in previous posts. We will be installing and importing an additional library named <code>bitsandbytes</code>. It provides an 8-bit optimizer which saves memory and increases speed while training. For more details checkout this <a href="https://github.com/TimDettmers/bitsandbytes">repo</a></p>
<p>We will also be using <code>accelerate</code> library which helps in distributed training, gradient accumulation and many more.</p>
<div class="cell" data-outputid="ad894817-a679-4aa6-bf43-8a5b10d0bc22" data-execution_count="1">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq accelerate transformers diffusers ftfy</span></code></pre></div>
</div>
<div class="cell" data-outputid="2b205681-f717-44a4-b00d-944140291b73" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>qq bitsandbytes</span></code></pre></div>
</div>
<p>We can authenticate our notebook to Huggingface services using the commands below. Since we will be pushing our model to HuggingFace Hub later, we will need write permissions. So you need to create an Access Token with write permission and use it to authenticate</p>
<div class="cell" data-outputid="fad27089-f24f-4e3a-b8d3-6d2a475ccab0" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> notebook_login</span>
<span id="cb5-2">notebook_login()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Token is valid.
Your token has been saved in your configured git credential helpers (store).
Your token has been saved to /root/.huggingface/token
Login successful</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb7-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb7-3"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb7-4"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb7-5"><span class="im" style="color: #00769E;">import</span> math</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb7-8"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb7-9"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> Dataset</span>
<span id="cb7-10"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> transforms <span class="im" style="color: #00769E;">as</span> tfms</span>
<span id="cb7-11"></span>
<span id="cb7-12"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel</span>
<span id="cb7-13"><span class="im" style="color: #00769E;">from</span> diffusers.optimization <span class="im" style="color: #00769E;">import</span> get_scheduler</span>
<span id="cb7-14"><span class="im" style="color: #00769E;">from</span> diffusers.pipelines.stable_diffusion <span class="im" style="color: #00769E;">import</span> StableDiffusionSafetyChecker</span>
<span id="cb7-15"></span>
<span id="cb7-16"><span class="im" style="color: #00769E;">from</span> accelerate <span class="im" style="color: #00769E;">import</span> Accelerator</span>
<span id="cb7-17"><span class="im" style="color: #00769E;">from</span> accelerate.logging <span class="im" style="color: #00769E;">import</span> get_logger</span>
<span id="cb7-18"><span class="im" style="color: #00769E;">from</span> accelerate.utils <span class="im" style="color: #00769E;">import</span> set_seed</span>
<span id="cb7-19"></span>
<span id="cb7-20"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer</span>
<span id="cb7-21"></span>
<span id="cb7-22"><span class="im" style="color: #00769E;">import</span> bitsandbytes <span class="im" style="color: #00769E;">as</span> bnb</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">def</span> image_grid(imgs, rows, cols):</span>
<span id="cb8-2">    w,h <span class="op" style="color: #5E5E5E;">=</span> imgs[<span class="dv" style="color: #AD0000;">0</span>].size</span>
<span id="cb8-3">    grid <span class="op" style="color: #5E5E5E;">=</span> Image.new(<span class="st" style="color: #20794D;">"RGB"</span>, size<span class="op" style="color: #5E5E5E;">=</span>(cols<span class="op" style="color: #5E5E5E;">*</span>w, rows<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb8-4">    <span class="cf" style="color: #003B4F;">for</span> i, img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(imgs):grid.paste(img, box<span class="op" style="color: #5E5E5E;">=</span>(i<span class="op" style="color: #5E5E5E;">%</span>cols<span class="op" style="color: #5E5E5E;">*</span>w, i<span class="op" style="color: #5E5E5E;">//</span>cols<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb8-5">    <span class="cf" style="color: #003B4F;">return</span> grid</span></code></pre></div>
</div>
<p>Next, we will initialize the core components of the Stable Diffusion model with the weights of <code>CompVis/stable-diffusion-v1-4</code> repo</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">model_path<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="5d64a83b-ac0f-49f5-df86-f7bed7a7b4da">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"text_encoder"</span>)</span>
<span id="cb10-2">vae          <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span>)</span>
<span id="cb10-3">unet         <span class="op" style="color: #5E5E5E;">=</span> UNet2DConditionModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>)</span>
<span id="cb10-4">tokenizer    <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"tokenizer"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d8e7356bd6b542fcb3875899149aeccf","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"1f3efdfd1a28474488a6a347a0bdee3f","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fe49a5dc59b04a31bc7833cf2c4e9cf8","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7ea9f1f31c984600bf904089524cc9f3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6cd43082ad2f41faa32808c3e312df7e","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ba64494b671449a391b8790929b39e54","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9fe6aa180f9b4823bd72a3555880617a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"71e0ce7708084f20a74e77b19c20f063","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9f3f7d6687e64b628d62dc6453a83016","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6e6e5c916c814d87a6ca4955895ee45c","version_major":2,"version_minor":0}
</script>
</div>
</div>
</section>
<section id="data" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Data</h1>
<p>We need to provide the images of our subject and the prompt with the unique token to fine-tune model. So let’s create a dataset which takes images and prompts as input. The dataset needs to convert the images into tensor which is done using PyTorch’s transforms. It also should return input_ids for the prompt which is provided by the Tokenizer. Both pixel values and prompt ids are returned as dictionary</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">class</span> DreamBoothDataset(Dataset):</span>
<span id="cb11-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, images_path, prompts , tokenizer, size <span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">512</span>, center_crop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb11-3">        <span class="va" style="color: #111111;">self</span>.size, <span class="va" style="color: #111111;">self</span>.center_crop <span class="op" style="color: #5E5E5E;">=</span> size, center_crop</span>
<span id="cb11-4">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> tokenizer</span>
<span id="cb11-5">        <span class="va" style="color: #111111;">self</span>.images_path <span class="op" style="color: #5E5E5E;">=</span> Path(images_path)</span>
<span id="cb11-6">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">self</span>.images_path.exists():</span>
<span id="cb11-7">            <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">ValueError</span>(<span class="st" style="color: #20794D;">"Images path doesn't exist"</span>)</span>
<span id="cb11-8"></span>
<span id="cb11-9">        <span class="va" style="color: #111111;">self</span>.images <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(Path(images_path).iterdir())</span>
<span id="cb11-10">        <span class="va" style="color: #111111;">self</span>.num_of_images <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.images)</span>
<span id="cb11-11">        <span class="va" style="color: #111111;">self</span>.prompts <span class="op" style="color: #5E5E5E;">=</span> prompts </span>
<span id="cb11-12"></span>
<span id="cb11-13">        <span class="va" style="color: #111111;">self</span>.image_transforms <span class="op" style="color: #5E5E5E;">=</span> tfms.Compose([</span>
<span id="cb11-14">            tfms.Resize(size, interpolation <span class="op" style="color: #5E5E5E;">=</span> tfms.InterpolationMode.BILINEAR),</span>
<span id="cb11-15">            tfms.CenterCrop(size) <span class="cf" style="color: #003B4F;">if</span> center_crop <span class="cf" style="color: #003B4F;">else</span> tfms.RandomCrop(size),</span>
<span id="cb11-16">            tfms.ToTensor(),</span>
<span id="cb11-17">            tfms.Normalize([<span class="fl" style="color: #AD0000;">0.5</span>],[<span class="fl" style="color: #AD0000;">0.5</span>])</span>
<span id="cb11-18">        ])  </span>
<span id="cb11-19">        </span>
<span id="cb11-20">            </span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__len__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb11-22">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.num_of_images    </span>
<span id="cb11-23"></span>
<span id="cb11-24">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, index):</span>
<span id="cb11-25">        example <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb11-26">        image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(<span class="va" style="color: #111111;">self</span>.images[index<span class="op" style="color: #5E5E5E;">%</span><span class="va" style="color: #111111;">self</span>.num_of_images]).convert(<span class="st" style="color: #20794D;">"RGB"</span>)   </span>
<span id="cb11-27">        example[<span class="st" style="color: #20794D;">"images"</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.image_transforms(image)  </span>
<span id="cb11-28">        example[<span class="st" style="color: #20794D;">"prompt_ids"</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer(<span class="va" style="color: #111111;">self</span>.prompts, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"do_not_pad"</span>, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, </span>
<span id="cb11-29">                                               max_length<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.tokenizer.model_max_length).input_ids </span>
<span id="cb11-30">        </span>
<span id="cb11-31">        <span class="cf" style="color: #003B4F;">return</span> example          </span></code></pre></div>
</div>
<p>Dataloader helps us to sample minibatches from our dataset. Since our dataset returns dictionary, we have to tell the Dataloader how it can combine and return minibatches of pixelvalues and prompt_ids.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;">def</span> collate_fn(examples):</span>
<span id="cb12-2">    pixels    <span class="op" style="color: #5E5E5E;">=</span> [example[<span class="st" style="color: #20794D;">"images"</span>] <span class="cf" style="color: #003B4F;">for</span> example <span class="kw" style="color: #003B4F;">in</span> examples]</span>
<span id="cb12-3">    input_ids <span class="op" style="color: #5E5E5E;">=</span> [example[<span class="st" style="color: #20794D;">"prompt_ids"</span>] <span class="cf" style="color: #003B4F;">for</span> example <span class="kw" style="color: #003B4F;">in</span> examples]</span>
<span id="cb12-4">    </span>
<span id="cb12-5">    pixels <span class="op" style="color: #5E5E5E;">=</span> torch.stack(pixels).to(memory_format<span class="op" style="color: #5E5E5E;">=</span>torch.contiguous_format).<span class="bu" style="color: null;">float</span>()</span>
<span id="cb12-6">    input_ids <span class="op" style="color: #5E5E5E;">=</span> tokenizer.pad({<span class="st" style="color: #20794D;">"input_ids"</span>:input_ids}, padding<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>).input_ids</span>
<span id="cb12-7"></span>
<span id="cb12-8">    batch <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb12-9">        <span class="st" style="color: #20794D;">"input_ids"</span>:input_ids,</span>
<span id="cb12-10">        <span class="st" style="color: #20794D;">"pixel_values"</span>:pixels</span>
<span id="cb12-11">    }</span>
<span id="cb12-12"></span>
<span id="cb12-13">    <span class="cf" style="color: #003B4F;">return</span> batch</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">train_dataset <span class="op" style="color: #5E5E5E;">=</span> DreamBoothDataset(</span>
<span id="cb13-2">        images_path <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"/content/gdrive/My Drive/pintu"</span>,</span>
<span id="cb13-3">        prompts <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"a photo of a sks dog"</span>,</span>
<span id="cb13-4">        tokenizer <span class="op" style="color: #5E5E5E;">=</span> tokenizer,</span>
<span id="cb13-5">        size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">512</span>,</span>
<span id="cb13-6">        center_crop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span></span>
<span id="cb13-7">    )</span>
<span id="cb13-8"></span>
<span id="cb13-9">train_dataloader <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.DataLoader(train_dataset, batch_size<span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate_fn)</span></code></pre></div>
</div>
<p>Now let’s view the our data. Below are the 6 images of my pet Pintu which are used for training</p>
<div class="cell" data-outputid="06425a6a-6aba-46fb-e6a5-0e50938bf8c3">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">images <span class="op" style="color: #5E5E5E;">=</span>[Image.<span class="bu" style="color: null;">open</span>(train_dataset.images[i]).resize((<span class="dv" style="color: #AD0000;">256</span>,<span class="dv" style="color: #AD0000;">256</span>)).convert(<span class="st" style="color: #20794D;">"RGB"</span>) <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(train_dataset))]</span>
<span id="cb14-2">image_grid(images, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(train_dataset))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored" data-anchor-id="data">
Fig 2: Images used to train Dreambooth model
</h4>
</figcaption>
</section>
<section id="training" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Training</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">5e-06</span></span>
<span id="cb15-2">max_train_steps <span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">500</span></span>
<span id="cb15-3">train_batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb15-4">gradient_accumulation_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb15-5">use_8bit_adam<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span></span>
<span id="cb15-6">output_dir <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"/content/gdrive/My Drive/pintu_dreambooth"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<p>The function <code>dreambooth_trainer</code> fine-tunes the Stable Diffusion model. We fine-tune the model for 500 steps at a learning rate of 5e-06. The <code>pixel_values</code> of the images from the dataloader are passed to VAE’s encoder to generate latents. The <code>prompt_ids</code> are passed to the text_encoder to generate embeddings. Random noise is added at each step and the loss is calculated. The loss is then backpropogated and the weights of the model are updated. After the training is completed, all the components of the model are saved to the <code>output_dir</code> path which in our case gets saved to Google Drive</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;">def</span> dreambooth_trainer(data, text_encoder, vae, unet):</span>
<span id="cb17-2"></span>
<span id="cb17-3">    accelerator<span class="op" style="color: #5E5E5E;">=</span>Accelerator(gradient_accumulation_steps<span class="op" style="color: #5E5E5E;">=</span>gradient_accumulation_steps)</span>
<span id="cb17-4">    set_seed(<span class="dv" style="color: #AD0000;">90000</span>)</span>
<span id="cb17-5">    unet.enable_gradient_checkpointing()</span>
<span id="cb17-6"></span>
<span id="cb17-7">    <span class="cf" style="color: #003B4F;">if</span> use_8bit_adam:</span>
<span id="cb17-8">        optimizer_class <span class="op" style="color: #5E5E5E;">=</span> bnb.optim.AdamW8bit</span>
<span id="cb17-9">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb17-10">        optimizer_class <span class="op" style="color: #5E5E5E;">=</span> torch.optim.AdamW</span>
<span id="cb17-11"></span>
<span id="cb17-12">    optimizer <span class="op" style="color: #5E5E5E;">=</span> optimizer_class(unet.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>learning_rate)</span>
<span id="cb17-13"></span>
<span id="cb17-14">    noise_scheduler <span class="op" style="color: #5E5E5E;">=</span> DDPMScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.00085</span>, beta_end <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>,</span>
<span id="cb17-15">                                    num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb17-16">    </span>
<span id="cb17-17">    </span>
<span id="cb17-18">    unet, optimizer, train_dataloader <span class="op" style="color: #5E5E5E;">=</span> accelerator.prepare(unet, optimizer, data)</span>
<span id="cb17-19"></span>
<span id="cb17-20">    text_encoder.to(torch_device)</span>
<span id="cb17-21">    vae.to(torch_device)</span>
<span id="cb17-22"></span>
<span id="cb17-23"></span>
<span id="cb17-24">    num_update_steps_per_epoch <span class="op" style="color: #5E5E5E;">=</span> math.ceil(<span class="bu" style="color: null;">len</span>(train_dataloader)<span class="op" style="color: #5E5E5E;">/</span>gradient_accumulation_steps)</span>
<span id="cb17-25">    num_train_epochs <span class="op" style="color: #5E5E5E;">=</span> math.ceil(max_train_steps<span class="op" style="color: #5E5E5E;">/</span>num_update_steps_per_epoch)</span>
<span id="cb17-26">    total_batch_size <span class="op" style="color: #5E5E5E;">=</span> train_batch_size <span class="op" style="color: #5E5E5E;">*</span>accelerator.num_processes <span class="op" style="color: #5E5E5E;">*</span> gradient_accumulation_steps</span>
<span id="cb17-27"></span>
<span id="cb17-28">    progress_bar <span class="op" style="color: #5E5E5E;">=</span> tqdm(<span class="bu" style="color: null;">range</span>(max_train_steps), disable<span class="op" style="color: #5E5E5E;">=</span><span class="kw" style="color: #003B4F;">not</span> accelerator.is_local_main_process)</span>
<span id="cb17-29">    progress_bar.set_description(<span class="st" style="color: #20794D;">"Steps"</span>)</span>
<span id="cb17-30">    global_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb17-31"></span>
<span id="cb17-32">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_train_epochs):</span>
<span id="cb17-33">        unet.train()</span>
<span id="cb17-34">        <span class="cf" style="color: #003B4F;">for</span> step, batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(train_dataloader):</span>
<span id="cb17-35">            <span class="cf" style="color: #003B4F;">with</span> accelerator.accumulate(unet):</span>
<span id="cb17-36">                <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb17-37">                    latents <span class="op" style="color: #5E5E5E;">=</span> vae.encode(batch[<span class="st" style="color: #20794D;">"pixel_values"</span>]).latent_dist.sample()</span>
<span id="cb17-38">                    latents<span class="op" style="color: #5E5E5E;">*=</span><span class="fl" style="color: #AD0000;">0.18215</span></span>
<span id="cb17-39">                </span>
<span id="cb17-40">                </span>
<span id="cb17-41">                noise <span class="op" style="color: #5E5E5E;">=</span> torch.randn(latents.shape).to(latents.device)</span>
<span id="cb17-42">                bsz <span class="op" style="color: #5E5E5E;">=</span> latents.shape[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb17-43">                timesteps <span class="op" style="color: #5E5E5E;">=</span> torch.randint(<span class="dv" style="color: #AD0000;">0</span>, noise_scheduler.config.num_train_timesteps,(bsz,), device<span class="op" style="color: #5E5E5E;">=</span>latents.device).<span class="bu" style="color: null;">long</span>()</span>
<span id="cb17-44">                noisy_latents <span class="op" style="color: #5E5E5E;">=</span> noise_scheduler.add_noise(latents, noise, timesteps)</span>
<span id="cb17-45"></span>
<span id="cb17-46">                <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb17-47">                    encoder_hidden_states <span class="op" style="color: #5E5E5E;">=</span> text_encoder(batch[<span class="st" style="color: #20794D;">"input_ids"</span>])[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb17-48"></span>
<span id="cb17-49">                noise_pred <span class="op" style="color: #5E5E5E;">=</span> unet(noisy_latents, timesteps, encoder_hidden_states).sample</span>
<span id="cb17-50"></span>
<span id="cb17-51">                loss <span class="op" style="color: #5E5E5E;">=</span> F.mse_loss(noise_pred, noise, reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"none"</span>).mean([<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>]).mean()</span>
<span id="cb17-52"></span>
<span id="cb17-53">                accelerator.backward(loss)</span>
<span id="cb17-54">                optimizer.step()</span>
<span id="cb17-55">                optimizer.zero_grad()</span>
<span id="cb17-56">            </span>
<span id="cb17-57">            <span class="cf" style="color: #003B4F;">if</span> accelerator.sync_gradients:</span>
<span id="cb17-58">                progress_bar.update(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb17-59">                global_step<span class="op" style="color: #5E5E5E;">+=</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb17-60"></span>
<span id="cb17-61"></span>
<span id="cb17-62">            logs <span class="op" style="color: #5E5E5E;">=</span> {<span class="st" style="color: #20794D;">"loss"</span>:loss.detach().item()}</span>
<span id="cb17-63">            progress_bar.set_postfix(<span class="op" style="color: #5E5E5E;">**</span>logs)</span>
<span id="cb17-64"></span>
<span id="cb17-65">            <span class="cf" style="color: #003B4F;">if</span> global_step<span class="op" style="color: #5E5E5E;">&gt;=</span>max_train_steps:<span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb17-66"></span>
<span id="cb17-67">        accelerator.wait_for_everyone()</span>
<span id="cb17-68"></span>
<span id="cb17-69">    <span class="cf" style="color: #003B4F;">if</span> accelerator.is_main_process:</span>
<span id="cb17-70">        pipeline<span class="op" style="color: #5E5E5E;">=</span>StableDiffusionPipeline(</span>
<span id="cb17-71">            text_encoder<span class="op" style="color: #5E5E5E;">=</span>text_encoder, vae<span class="op" style="color: #5E5E5E;">=</span>vae, unet<span class="op" style="color: #5E5E5E;">=</span>unet, </span>
<span id="cb17-72">            tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer,</span>
<span id="cb17-73">            scheduler <span class="op" style="color: #5E5E5E;">=</span> PNDMScheduler(</span>
<span id="cb17-74">                beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, skip_prk_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span></span>
<span id="cb17-75">            ),</span>
<span id="cb17-76">            safety_checker<span class="op" style="color: #5E5E5E;">=</span>StableDiffusionSafetyChecker.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-safety-checker"</span>),</span>
<span id="cb17-77">            feature_extractor<span class="op" style="color: #5E5E5E;">=</span>CLIPFeatureExtractor.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-base-patch32"</span>))</span>
<span id="cb17-78">        pipeline.save_pretrained(output_dir)</span></code></pre></div>
</div>
<div class="cell" data-outputid="88d1ec1b-150a-45ec-dcf8-82d7765461ea">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;">import</span> accelerate</span>
<span id="cb18-2">accelerate.notebook_launcher(dreambooth_trainer, args<span class="op" style="color: #5E5E5E;">=</span>(train_dataloader,text_encoder, vae, unet), num_processes <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="results" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results</h1>
<p>Now we can load the model from the saved directory and use it for inference. To get high quality images, you can use prompts from <a href="https://lexica.art/">Lexica</a> and replace with your unique identifier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">output_dir <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"/content/gdrive/My Drive/pintu_dreambooth"</span></span>
<span id="cb19-2">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span>
<span id="cb19-3"></span>
<span id="cb19-4">pipe <span class="op" style="color: #5E5E5E;">=</span> StableDiffusionPipeline.from_pretrained(</span>
<span id="cb19-5">        output_dir,</span>
<span id="cb19-6">        torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16,</span>
<span id="cb19-7">    ).to(torch_device)</span></code></pre></div>
</div>
<p>Below are some of the astonishing results generated by our fine-tuned model!</p>
<div class="cell" data-outputid="ea2ad3b3-60ad-4501-f0bb-c6c9d946942d">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"a beautiful portrait of a cute sks dog, beautiful detailed eyes, golden hour, standing on a beach, outdoors, professional award winning portrait photography, Zeiss 150mm f/ 2.8 Hasselblad"</span> </span>
<span id="cb20-2"></span>
<span id="cb20-3">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb20-4">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb20-5"></span>
<span id="cb20-6">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb20-7"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb20-8">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb20-9">    all_images.extend(images)</span>
<span id="cb20-10"></span>
<span id="cb20-11">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb20-12">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2dd2a22a56024ee191ebcdcb19f87728","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"239f00e46fa241c29e8538274fbe47be","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"12ec83ecdfba44ed986e4113158a2f7a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="17">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-19-output-4.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored" data-anchor-id="results">
Fig 3: A beautiful portrait of a cute sks dog, beautiful detailed eyes, golden hour, standing on a beach, outdoors, professional award winning portrait photography, Zeiss 150mm f/ 2.8 Hasselblad”
</h4>
</figcaption>
<div class="cell" data-outputid="d687e52a-e2d6-4dd3-ab37-287869d9b8af">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"portrait of a sks dog wearing a hat, realism, realistic, photorealism, f 3. 5, photography, octane render, trending on artstation, artstationhd, artstationhq, unreal engine, cinema 4 d, 8 k, detailed fur "</span> </span>
<span id="cb21-2"></span>
<span id="cb21-3">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb21-4">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb21-5"></span>
<span id="cb21-6"><span class="co" style="color: #5E5E5E;"># def dummy_checker(images, **kwargs): return images, False</span></span>
<span id="cb21-7"><span class="co" style="color: #5E5E5E;"># pipe.safety_checker = dummy_checker</span></span>
<span id="cb21-8"></span>
<span id="cb21-9">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb21-10"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb21-11">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb21-12">    all_images.extend(images)</span>
<span id="cb21-13"></span>
<span id="cb21-14">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb21-15">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2964ef31a2944bfead3c516c99ec2d86","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5641344831f34b9091cde4672a0e2086","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"190f2f426d254562a869b96ff1ec04cd","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-20-output-4.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 4: Portrait of a sks dog wearing a hat, realism, realistic, photorealism, f 3. 5, photography, octane render, trending on artstation, artstationhd, artstationhq, unreal engine, cinema 4 d, 8 k, detailed fur ”
</h4>
</figcaption>
<div class="cell" data-outputid="dbc63a4a-ab8a-437d-a1a1-3bffe911d255">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"sks dog as a realistic fantasy knight, closeup portrait art by donato giancola and greg rutkowski, digital art, trending on artstation, symmetry!! "</span> </span>
<span id="cb22-2"></span>
<span id="cb22-3">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb22-4">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb22-5"></span>
<span id="cb22-6"></span>
<span id="cb22-7">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb22-8"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb22-9">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb22-10">    all_images.extend(images)</span>
<span id="cb22-11"></span>
<span id="cb22-12">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb22-13">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a09444f1ad3f406e8c932d523c3a9438","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bb0abfe6bb46466da495b6ca03e3c1a1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c4ef9cb05fda438c89c798123de5d519","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="29">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-21-output-4.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 5: sks dog as a realistic fantasy knight, closeup portrait art by donato giancola and greg rutkowski, digital art, trending on artstation, symmetry!!
</h4>
</figcaption>
<div class="cell" data-outputid="edb661ed-25b7-4e6f-e201-398af142ceff">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"white sks dog, with reflection in the puddle, foggy old forest, very detailed, 4 k, professional photography "</span> </span>
<span id="cb23-2"></span>
<span id="cb23-3">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb23-4">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb23-5"></span>
<span id="cb23-6"></span>
<span id="cb23-7">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb23-8"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb23-9">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb23-10">    all_images.extend(images)</span>
<span id="cb23-11"></span>
<span id="cb23-12">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb23-13">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"946d7dcfe75249ca836e8ffe909ddc99","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"acc82a56491b4e8fa54f8d3de0bc69cd","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"91a89a7183de489bbc765cf5740c248c","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="25">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-22-output-4.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 6 : White sks dog, with reflection in the puddle, foggy old forest, very detailed, 4 k, professional photography
</h4>
</figcaption>
<div class="cell" data-outputid="b1a45dbc-8846-4e3e-c4b7-ae8f874a5a15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"framed renaissance portrait of a sks dog"</span> </span>
<span id="cb24-2"></span>
<span id="cb24-3">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb24-4">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb24-5"></span>
<span id="cb24-6"></span>
<span id="cb24-7">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb24-8"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb24-9">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb24-10">    all_images.extend(images)</span>
<span id="cb24-11"></span>
<span id="cb24-12">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb24-13">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b58e4cf6805a4b94924f022b4ab7a21b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"60e65f252493444299d7b0f6c18d193e","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7eafc311a0c244709df00eedefc981c2","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="36">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-23-output-4.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 7: Framed renaissance portrait of a sks dog
</h4>
</figcaption>
<p>In the below results, the prompt is truncated since CLIP can only handle prompts with maximum of 77 tokens</p>
<div class="cell" data-outputid="b1dd7189-e991-4697-d16c-c6ac0a21b7f6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"a sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, partial anatomy, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha"</span></span>
<span id="cb25-2">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb25-3">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb25-4"></span>
<span id="cb25-5"></span>
<span id="cb25-6">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb25-7"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb25-8">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb25-9">    all_images.extend(images)</span>
<span id="cb25-10"></span>
<span id="cb25-11">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb25-12">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a6c57de3c1b34f77b475f916761540ce","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0722c4031ac947afa2c10d2db67a7f6d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"98e6c54eb0b94198832327c75f3f08ad","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="37">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-24-output-7.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 8: A sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, partial anatomy, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha
</h4>
</figcaption>
<div class="cell" data-outputid="4f70024d-c843-4f4e-9c28-e1c594b5ac3f">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"a sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha"</span></span>
<span id="cb29-2">num_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb29-3">num_rows <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> </span>
<span id="cb29-4"></span>
<span id="cb29-5"></span>
<span id="cb29-6">all_images <span class="op" style="color: #5E5E5E;">=</span> [] </span>
<span id="cb29-7"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_rows):</span>
<span id="cb29-8">    images <span class="op" style="color: #5E5E5E;">=</span> pipe([prompt] <span class="op" style="color: #5E5E5E;">*</span> num_samples, num_inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">75</span>, guidance_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, seed <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'random'</span>).images</span>
<span id="cb29-9">    all_images.extend(images)</span>
<span id="cb29-10"></span>
<span id="cb29-11">grid <span class="op" style="color: #5E5E5E;">=</span> image_grid(all_images, num_samples, num_rows)</span>
<span id="cb29-12">grid </span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7fadabeeb011425bbd556b2f71905ec5","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"63624ca2bc654d0cb58ef9d8bdb74854","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"44c028735e854b1e8e2e5bb6c62f19b3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="38">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-25-output-7.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 9: A sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha
</h4>
</figcaption>
</section>
<section id="pushing-your-model-to-hub" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Pushing your model to hub</h1>
<p>HuggingFace Hub is a platform with over 60K models, 6K datasets and 6K demo apps(spaces) all open source and publicly available. The best part is that sharing and using any public model on the Hub is completely free of cost. Let’s next see how we can move our model from Google Drive to HuggingFace Hub. In order to push model to hub, make sure that you have used Access token with write permission while authenticating to HuggingFace</p>
<p>The <code>get_full_repo_name</code> returns the repository name for given model in user’s namespace ({username/model_name}).</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> get_full_repo_name</span>
<span id="cb33-2">model_name <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"pintu_dreambooth"</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="bad2c214-4582-4311-d891-0c1a95baf02a" data-execution_count="8">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">hub_model <span class="op" style="color: #5E5E5E;">=</span> get_full_repo_name(model_name)</span>
<span id="cb34-2">hub_model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>'prajwal13/pintu_dreambooth'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">output_dir <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"/content/gdrive/My Drive/pintu_dreambooth"</span></span>
<span id="cb36-2">images_folder <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"/content/gdrive/My Drive/pintu"</span></span>
<span id="cb36-3">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<p>We can then use <code>create_repo</code> to create an empty directory in HuggingFace hub and push our model and the images used for training to the empty repo.</p>
<div class="cell" data-outputid="0ebe4159-10a0-48aa-ba3a-b7673b7239d9">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> HfApi, create_repo</span>
<span id="cb37-2"></span>
<span id="cb37-3">create_repo(hub_model)</span>
<span id="cb37-4">api <span class="op" style="color: #5E5E5E;">=</span> HfApi()</span>
<span id="cb37-5">api.upload_folder(folder_path<span class="op" style="color: #5E5E5E;">=</span>output_dir, path_in_repo<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">""</span>, repo_id<span class="op" style="color: #5E5E5E;">=</span>hub_model)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>'https://huggingface.co/prajwal13/pintu_dreambooth/tree/main/'</code></pre>
</div>
</div>
<div class="cell" data-outputid="9690eb5f-cdc5-47dc-a613-977ff9646dc1">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">api.upload_folder(folder_path<span class="op" style="color: #5E5E5E;">=</span>images_folder, path_in_repo<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"concept_images"</span>,repo_id<span class="op" style="color: #5E5E5E;">=</span>hub_model)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>'https://huggingface.co/prajwal13/pintu_dreambooth/tree/main/concept_images'</code></pre>
</div>
</div>
<p>The last thing to do is create a model card so that our model can easily be found on the Hub. We will also add the images used to train in the model card</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> ModelCard</span>
<span id="cb41-2"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb41-3"></span>
<span id="cb41-4">images_upload <span class="op" style="color: #5E5E5E;">=</span> os.listdir(images_folder)</span>
<span id="cb41-5">image_string <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb41-6"></span>
<span id="cb41-7"><span class="cf" style="color: #003B4F;">for</span> i, image <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(images_upload):</span>
<span id="cb41-8">    image_string <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f'''</span><span class="sc" style="color: #5E5E5E;">{</span>image_string<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">![sks </span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">](https://huggingface.co/</span><span class="sc" style="color: #5E5E5E;">{</span>hub_model<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">/resolve/main/concept_images/</span><span class="sc" style="color: #5E5E5E;">{</span>image<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">)'''</span></span>
<span id="cb41-9"></span>
<span id="cb41-10">image_string</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">content <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb42-2"><span class="ss" style="color: #20794D;">---</span></span>
<span id="cb42-3"><span class="ss" style="color: #20794D;">license: mit</span></span>
<span id="cb42-4"><span class="ss" style="color: #20794D;">tags:</span></span>
<span id="cb42-5"><span class="ss" style="color: #20794D;">- pytorch</span></span>
<span id="cb42-6"><span class="ss" style="color: #20794D;">- diffusers</span></span>
<span id="cb42-7"><span class="ss" style="color: #20794D;">- dreambooth</span></span>
<span id="cb42-8"><span class="ss" style="color: #20794D;">---</span></span>
<span id="cb42-9"></span>
<span id="cb42-10"><span class="ss" style="color: #20794D;"># Model Card for Dreambooth model trained on My pet Pintu's images</span></span>
<span id="cb42-11"></span>
<span id="cb42-12"><span class="ss" style="color: #20794D;">This model is a diffusion model for unconditional image generation of my cute pet dog Pintu trained using Dreambooth concept. The token to use is sks .</span></span>
<span id="cb42-13"></span>
<span id="cb42-14"><span class="ss" style="color: #20794D;">## Usage</span></span>
<span id="cb42-15"></span>
<span id="cb42-16"><span class="ss" style="color: #20794D;">from diffusers import StableDiffusionPipeline</span></span>
<span id="cb42-17"><span class="ss" style="color: #20794D;">pipeline = StableDiffusionPipeline.from_pretrained(</span><span class="sc" style="color: #5E5E5E;">{</span>hub_model<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">)</span></span>
<span id="cb42-18"><span class="ss" style="color: #20794D;">image = pipeline('a photo of sks dog').images[0]</span></span>
<span id="cb42-19"><span class="ss" style="color: #20794D;">image</span></span>
<span id="cb42-20"></span>
<span id="cb42-21"><span class="ss" style="color: #20794D;">These are the images on which the dreambooth model is trained on</span></span>
<span id="cb42-22"></span>
<span id="cb42-23"><span class="sc" style="color: #5E5E5E;">{</span>image_string<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb42-24"></span>
<span id="cb42-25"><span class="ss" style="color: #20794D;">"""</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="afe9cc07-8127-46f3-ecfe-a7e7347093c5">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">card <span class="op" style="color: #5E5E5E;">=</span> ModelCard(content)</span>
<span id="cb43-2">card.push_to_hub(hub_model)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>'https://huggingface.co/prajwal13/pintu_dreambooth/blob/main/README.md'</code></pre>
</div>
</div>
<p>Now we can directly load the model from HuggingFace Hub by passing the repo id to the pipeline</p>
<div class="cell" data-outputid="218593dd-3526-4188-e0f3-3459f0c768dc" data-execution_count="22">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline</span>
<span id="cb45-2"></span>
<span id="cb45-3">pipeline <span class="op" style="color: #5E5E5E;">=</span> StableDiffusionPipeline.from_pretrained(hub_model).to(torch_device)</span>
<span id="cb45-4">image <span class="op" style="color: #5E5E5E;">=</span> pipeline(<span class="st" style="color: #20794D;">"a photo of sks dog wearing glasses, and standing near beach"</span>).images[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb45-5">image</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"5639083cdc9c4162aae5cd5faa789696"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"68480a11b2fc4946add1a2c815310bd2"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth_files/figure-html/cell-34-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Conclusion</h1>
<p>In this post, we learnt about Dreambooth and how we can personalize the Stable Diffusion model. We also learnt how to push our model to HuggingFace hub.</p>
<p>I hope you enjoyed reading it. If there is any feedback, feel free to reach out on <a href="https://www.linkedin.com/in/prajwal-s-1416061b3/">LinkedIn</a> or on <a href="mailto:prajwalsuresh13@gmail.com">mail</a></p>
</section>
<section id="references" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> References</h1>
<ul>
<li><a href="https://www.fast.ai/posts/part2-2022-preview.html">FastAI course</a></li>
<li><a href="https://dreambooth.github.io/">Dreambooth paper</a></li>
<li><a href="https://github.com/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb">HuggingFace Dreambooth notebook</a></li>
</ul>


</section>

 ]]></description>
  <category>Stable Diffusion</category>
  <guid>https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html</guid>
  <pubDate>Wed, 30 Nov 2022 18:30:00 GMT</pubDate>
  <media:content url="https://prajwalsuresh.com/posts/2022-12-01-Dreambooth/01.png" medium="image" type="image/png" height="148" width="144"/>
</item>
<item>
  <title>Stable diffusion - Negative Prompt, Image to Image &amp; Textual Inversion</title>
  <dc:creator>Prajwal Suresh</dc:creator>
  <link>https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Introduction to variations provided in 🤗 <a href="https://huggingface.co/">Hugging face</a> - <a href="https://github.com/huggingface/diffusers">Diffusers library</a> such as Negative prompt, Image to Image and training new concept using Textual Inversion</p>
</blockquote>
<br>
<div data-align="center">
<p><a href="https://colab.research.google.com/github/prajwal-suresh13/diffusion/blob/master/02_stable_diffusion_variations.ipynb"> <img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/https:/colab.research.google.com/assets/colab-badge.svg" width="150"> </a></p>
</div>
<p><br></p>
<p>This is the second post on Stable Diffusion. Check out the previous post through link below if you haven’t read it yet <br><br> 1. <strong>Part 1</strong> - <a href="https://prajwal-suresh13.github.io/website/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html">Stable diffusion - Introduction</a>.<br></p>
<p>In previous post, we learnt about Stable Diffusion and its key components VAE, UNet, Scheduler &amp; CLIPTextModel. We also built a pipeline named <code>SDPipeline</code> using these key components and generated images for multiple prompts. In this post, we will further modify our pipeline to integrate negative prompts and image to image. At the end, we will learn about Textual Inversion which is useful to train a new concept</p>
<p>At first, let’s download and import the required libraries, authenticate and login to HuggingFace</p>
<div class="cell" data-outputid="228d7dd1-4ac9-48fa-cf2c-b066c593be9f">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq accelerate transformers diffusers ftfy</span></code></pre></div>
</details>
</div>
<div class="cell" data-outputid="b0f51085-cf9c-49df-c52a-52147ec263a6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> notebook_login</span>
<span id="cb2-2">notebook_login()</span></code></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> autocast</span>
<span id="cb3-3"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> transforms <span class="im" style="color: #00769E;">as</span> tfms</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb3-6"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> logging</span>
<span id="cb3-7">logging.set_verbosity_error() <span class="co" style="color: #5E5E5E;"># Suppress unnecessary warning from CLIPTextModel</span></span>
<span id="cb3-8"></span>
<span id="cb3-9"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb3-12"><span class="im" style="color: #00769E;">from</span> matplotlib <span class="im" style="color: #00769E;">import</span> pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb3-13"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb3-14"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb3-15"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb3-16"><span class="im" style="color: #00769E;">from</span> fastdownload <span class="im" style="color: #00769E;">import</span> FastDownload</span>
<span id="cb3-17"></span>
<span id="cb3-18"><span class="im" style="color: #00769E;">from</span> IPython.display <span class="im" style="color: #00769E;">import</span> HTML</span>
<span id="cb3-19"><span class="im" style="color: #00769E;">from</span> base64 <span class="im" style="color: #00769E;">import</span> b64encode</span></code></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> image_grid(imgs, rows, cols):</span>
<span id="cb5-2">    w,h <span class="op" style="color: #5E5E5E;">=</span> imgs[<span class="dv" style="color: #AD0000;">0</span>].size</span>
<span id="cb5-3">    grid <span class="op" style="color: #5E5E5E;">=</span> Image.new(<span class="st" style="color: #20794D;">"RGB"</span>, size<span class="op" style="color: #5E5E5E;">=</span>(cols<span class="op" style="color: #5E5E5E;">*</span>w, rows<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;">for</span> i, img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(imgs):grid.paste(img, box<span class="op" style="color: #5E5E5E;">=</span>(i<span class="op" style="color: #5E5E5E;">%</span>cols<span class="op" style="color: #5E5E5E;">*</span>w, i<span class="op" style="color: #5E5E5E;">//</span>cols<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;">return</span> grid</span></code></pre></div>
</div>
<p>Below is the pipeline which we built in the previous post using the core components.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb6-2"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> SDPipeline():</span>
<span id="cb7-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model_path):</span>
<span id="cb7-3">        <span class="va" style="color: #111111;">self</span>.model_path<span class="op" style="color: #5E5E5E;">=</span>model_path</span>
<span id="cb7-4">        <span class="va" style="color: #111111;">self</span>.vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(model_path,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb7-5">        <span class="va" style="color: #111111;">self</span>.unet<span class="op" style="color: #5E5E5E;">=</span>UNet2DConditionModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb7-6">        <span class="va" style="color: #111111;">self</span>.text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"text_encoder"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb7-7">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"tokenizer"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16)</span>
<span id="cb7-8">        <span class="va" style="color: #111111;">self</span>.scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb7-9"></span>
<span id="cb7-10">    <span class="kw" style="color: #003B4F;">def</span> encode_text(<span class="va" style="color: #111111;">self</span>, prompts, maxlen<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-11">        <span class="cf" style="color: #003B4F;">if</span> maxlen <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: maxlen <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer.model_max_length</span>
<span id="cb7-12">        inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer(prompts, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>maxlen, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb7-13">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.text_encoder(inp.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>].half()</span>
<span id="cb7-14"></span>
<span id="cb7-15">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>,prompts, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-16">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb7-17"></span>
<span id="cb7-18">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb7-19">        uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb7-20">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb7-21">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb7-22"></span>
<span id="cb7-23">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb7-24"></span>
<span id="cb7-25">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb7-26">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb7-27"></span>
<span id="cb7-28">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(<span class="va" style="color: #111111;">self</span>.scheduler.timesteps)):</span>
<span id="cb7-29">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb7-30">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb7-31">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb7-32"></span>
<span id="cb7-33">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb7-34"></span>
<span id="cb7-35">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb7-36">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb7-37">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb7-38">        </span>
<span id="cb7-39">        </span>
<span id="cb7-40">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.latents_to_pil(latents)</span>
<span id="cb7-41"></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">def</span> create_image(t):</span>
<span id="cb8-2">    image <span class="op" style="color: #5E5E5E;">=</span> (t<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>).detach().cpu().permute(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">0</span>).numpy()</span>
<span id="cb8-3">    <span class="cf" style="color: #003B4F;">return</span> Image.fromarray((image<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">255</span>).<span class="bu" style="color: null;">round</span>().astype(<span class="st" style="color: #20794D;">"uint8"</span>))</span>
<span id="cb8-4"></span>
<span id="cb8-5"><span class="kw" style="color: #003B4F;">def</span> pil_to_latents(<span class="va" style="color: #111111;">self</span>,input_img):</span>
<span id="cb8-6">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb8-7">        latent <span class="op" style="color: #5E5E5E;">=</span> tfms.ToTensor()(input_img).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).to(torch_device)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb8-8">        latent <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.vae.encode(latent.half())</span>
<span id="cb8-9">    <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latent.latent_dist.sample()</span>
<span id="cb8-10"></span>
<span id="cb8-11"><span class="kw" style="color: #003B4F;">def</span> latents_to_pil(<span class="va" style="color: #111111;">self</span>, latents):</span>
<span id="cb8-12">    latents <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.18215</span>) <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb8-14">        images <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.vae.decode(latents).sample</span>
<span id="cb8-15">    pil_images <span class="op" style="color: #5E5E5E;">=</span>[create_image(image) <span class="cf" style="color: #003B4F;">for</span> image <span class="kw" style="color: #003B4F;">in</span> images]</span>
<span id="cb8-16">    <span class="cf" style="color: #003B4F;">return</span> pil_images</span>
<span id="cb8-17"></span>
<span id="cb8-18">SDPipeline.pil_to_latents <span class="op" style="color: #5E5E5E;">=</span> pil_to_latents</span>
<span id="cb8-19">SDPipeline.latents_to_pil  <span class="op" style="color: #5E5E5E;">=</span> latents_to_pil</span></code></pre></div>
</div>
<section id="negative-prompts" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Negative Prompts</h1>
<section id="what-is-negative-prompt" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="what-is-negative-prompt"><span class="header-section-number">1.1</span> What is Negative Prompt?</h2>
<p>Negative prompt is an additional capability which removes anything that the user doesn’t want to see in generated images. Suppose you don’t want a particular color or particular object to be part of the generated images, you can mention it in <code>negative_prompt</code> argument in the <code>StableDiffusionPipeline</code>.</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/https:/miro.medium.com/max/828/0*wnoQYUZJiCwhtUTg.webp" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 1 : Example for Negative Prompt <a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265">(Image Credit)</a>
</h4>
</figcaption>
</figure>
</section>
<section id="adding-negative-prompt-to-our-sdpipeline" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="adding-negative-prompt-to-our-sdpipeline"><span class="header-section-number">1.2</span> Adding negative prompt to our SDPipeline</h2>
<p>Let’s find how the negative prompt works and how can we integrate it into our pipeline.</p>
<p>In the previous post, we used an empty string, generated unconditional embeddings from it and used it along with text embeddings to implement Classifier-Free guidance as below</p>
<pre><code>uncond = self.encode_text([""]*batch_size, cond.shape[1])</code></pre>
<p>The way negative prompt works is that the empty string is replaced by the string containing the objects, styles or colors provided by the user as negative prompt. So to implement negative prompt, we can modify our code to use empty string if negative prompt is not provided else to use negative prompt provided by user</p>
<pre><code>if not negative_prompts:
    uncond = self.encode_text([""]*batch_size, cond.shape[1])
else:
    uncond = self.encode_text(neg_prompts, cond.shape[1])</code></pre>
<p>Let’s make those changes in the diffusion loop of our pipeline and check if our negative prompt implementation is working</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> diff_pipeline_negprompts(<span class="va" style="color: #111111;">self</span>,prompts,negative_prompts<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb11-2">        <span class="co" style="color: #5E5E5E;">#calculate number of prompts</span></span>
<span id="cb11-3">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb11-4">        </span>
<span id="cb11-5">        <span class="co" style="color: #5E5E5E;">#generate text_embeddings</span></span>
<span id="cb11-6">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb11-7">        </span>
<span id="cb11-8">        <span class="co" style="color: #5E5E5E;">#generate embedding for negative prompts if available else generate unconditional embeddings</span></span>
<span id="cb11-9">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> negative_prompts: uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb11-10">        <span class="cf" style="color: #003B4F;">else</span> : uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(negative_prompts, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb11-11">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb11-12"></span>
<span id="cb11-13">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb11-14">        </span>
<span id="cb11-15">        <span class="co" style="color: #5E5E5E;">#generate random noise and add noise</span></span>
<span id="cb11-16">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb11-17">        </span>
<span id="cb11-18">        <span class="co" style="color: #5E5E5E;">#Setting number of steps in scheduler</span></span>
<span id="cb11-19">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb11-20">        </span>
<span id="cb11-21">        <span class="co" style="color: #5E5E5E;">#Add noise to latents</span></span>
<span id="cb11-22">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb11-23"></span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(<span class="va" style="color: #111111;">self</span>.scheduler.timesteps)):</span>
<span id="cb11-25">            <span class="co" style="color: #5E5E5E;"># scale the latents for particular timestep</span></span>
<span id="cb11-26">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb11-27">            </span>
<span id="cb11-28">            <span class="co" style="color: #5E5E5E;">#predict noise using UNet</span></span>
<span id="cb11-29">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb11-30">                </span>
<span id="cb11-31">            <span class="co" style="color: #5E5E5E;">#Perform Classifier-Free Guidance    </span></span>
<span id="cb11-32">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb11-33">            </span>
<span id="cb11-34">            <span class="co" style="color: #5E5E5E;">#Removing predicted noise from the latents</span></span>
<span id="cb11-35">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb11-36">            </span>
<span id="cb11-37">            <span class="co" style="color: #5E5E5E;">#run callbacks if any are added</span></span>
<span id="cb11-38">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb11-39">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb11-40">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb11-41">        </span>
<span id="cb11-42">        <span class="co" style="color: #5E5E5E;">#after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated</span></span>
<span id="cb11-43">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.latents_to_pil(latents)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">SDPipeline.<span class="fu" style="color: #4758AB;">__call__</span> <span class="op" style="color: #5E5E5E;">=</span> diff_pipeline_negprompts</span></code></pre></div>
</div>
<p>We will generate two images with same prompt and for one of the images we will use a negative prompt</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">pipe <span class="op" style="color: #5E5E5E;">=</span> SDPipeline(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="e1d66cf2-929e-4d79-9650-a1a665e4ccc1">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb14-2">img1 <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"a photo of a forest with trees and bushes"</span>])[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb14-3">img2 <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"a photo of a forest with trees and bushes"</span>], negative_prompts<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"green"</span>])[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"850628b0986e4f99a75d266493e6adec","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ae7e125a4e90406d94cc79935936bd30","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="40611403-ec13-4d4b-ef90-8c1eb3742eae">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">imgs<span class="op" style="color: #5E5E5E;">=</span>[img1, img2]</span>
<span id="cb15-2">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 2: Negative Prompt Results for prompt “a photo of a forest with trees and bushes” and negative prompt=“green”“
</h4>
</figcaption>
<p>As you can see in the images above, the image on the left generated image of forest. When given negative prompt as color green, the model generated same image without green color in it (on the right)</p>
</section>
</section>
<section id="image-to-image" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Image To Image</h1>
<section id="adding-image-to-image-capability-to-sdpipeline" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="adding-image-to-image-capability-to-sdpipeline"><span class="header-section-number">2.1</span> Adding Image to Image capability to SDPipeline</h2>
<p>In the previous post, we learnt that we can also pass an image to our model and it generates new images using the provided image as the initial image along with the prompt. There are two scenarios 1) with image (Image to Image) 2) without image (Prompt to Image)</p>
<p>For scenario 1 which is Image to Image pipeline, we expect 2 parameters:<br> 1. <code>init_image</code> - This is used as the initial image<br> 2. <code>strength</code> - The value of this parameter is between 0 and 1<br></p>
<p>First, the provided init_image is passed to the VAE’s encoder to generate latents. We then need to add noise to the latents. Since we are using initial image we can reduce the number of inference steps. The <code>strength</code> parameter is used for this and higher its value higher the <code>inference_steps</code>. If we want our generated image to be similar to the init_image, the number of steps should be less so the <code>strength</code> parameter value should also be less. The amount of noise to be added is calculated by multiplying the <code>strength</code> and <code>inference_steps</code></p>
<p>For example, suppose we want our generated image to be similar to the initial image so we assign the <code>strength</code> parameter to 0.4 and <code>inference_steps</code> is equal to 50 . Then the noise added is for the step 30 (<code>50 -( 0.4 x 50) == 30</code>). The <code>inference_steps</code> is reduced to 20 steps (<code>50x0.4</code>)</p>
<p>For scenario 2 which is prompt to image, we need to create a random noise. The amount of noise for the first step is multiplied with <code>scheduler.init_noise_sigma</code> . The <code>inference_steps</code> remains same.</p>
<p>Let’s create a function which can handle both the scenarios. The function should return latents and timesteps. <br> 1. In case of Prompt to Image, random noise is created and multiplied with init_noise is returned. The timesteps is returned without modifying it<br> 2. In case of Image to Image, latents are created from init_image and amount of noise to be added is calculated using strength and returned. The timesteps is also reduced based on the strength and returned</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> create_latents(<span class="va" style="color: #111111;">self</span>, batch_size, init_image, strength, inference_steps):</span>
<span id="cb16-2">    </span>
<span id="cb16-3">    <span class="cf" style="color: #003B4F;">if</span> init_image <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb16-4">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb16-5">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb16-6">        <span class="cf" style="color: #003B4F;">return</span> latents, <span class="va" style="color: #111111;">self</span>.scheduler.timesteps.to(torch_device)</span>
<span id="cb16-7">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb16-8">        img_latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.pil_to_latents(init_image)</span>
<span id="cb16-9">        noise <span class="op" style="color: #5E5E5E;">=</span> torch.randn(img_latents.shape, generator<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, device<span class="op" style="color: #5E5E5E;">=</span>torch_device, dtype<span class="op" style="color: #5E5E5E;">=</span>img_latents.dtype)</span>
<span id="cb16-10"></span>
<span id="cb16-11">        init_timestep <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(inference_steps <span class="op" style="color: #5E5E5E;">*</span> strength)</span>
<span id="cb16-12">        timesteps <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.timesteps[<span class="op" style="color: #5E5E5E;">-</span>init_timestep]</span>
<span id="cb16-13">        timesteps <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([timesteps], device<span class="op" style="color: #5E5E5E;">=</span>torch_device)</span>
<span id="cb16-14"></span>
<span id="cb16-15">        init_latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.add_noise(img_latents, noise, timesteps)</span>
<span id="cb16-16"></span>
<span id="cb16-17">        t_start <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">max</span>(inference_steps <span class="op" style="color: #5E5E5E;">-</span> init_timestep, <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb16-18">        timesteps <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.timesteps[t_start:].to(torch_device)</span>
<span id="cb16-19">        </span>
<span id="cb16-20">        <span class="cf" style="color: #003B4F;">return</span> init_latents, timesteps</span>
<span id="cb16-21"></span>
<span id="cb16-22">SDPipeline.create_latents <span class="op" style="color: #5E5E5E;">=</span> create_latents</span></code></pre></div>
</div>
<p>Let’s replace the diffusion loop code to get latents and timesteps from the above function</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;">def</span> diff_pipeline_img2img(<span class="va" style="color: #111111;">self</span>,prompts,init_image<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, strength<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.8</span>, negative_prompts<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb17-2">        <span class="co" style="color: #5E5E5E;">#calculate number of prompts</span></span>
<span id="cb17-3">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb17-4">        </span>
<span id="cb17-5">        <span class="co" style="color: #5E5E5E;">#generate text_embeddings</span></span>
<span id="cb17-6">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb17-7">        </span>
<span id="cb17-8">        <span class="co" style="color: #5E5E5E;">#generate embedding for negative prompts if available else generate unconditional embeddings</span></span>
<span id="cb17-9">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> negative_prompts: uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb17-10">        <span class="cf" style="color: #003B4F;">else</span> : uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(negative_prompts, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb17-11">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb17-12">        </span>
<span id="cb17-13">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb17-14"></span>
<span id="cb17-15">        <span class="co" style="color: #5E5E5E;">#Setting number of steps in scheduler</span></span>
<span id="cb17-16">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb17-17">        </span>
<span id="cb17-18">        <span class="co" style="color: #5E5E5E;">#generate random noise and add noise</span></span>
<span id="cb17-19">        latents, timesteps <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.create_latents(batch_size, init_image, strength, inference_steps)</span>
<span id="cb17-20">        </span>
<span id="cb17-21">        </span>
<span id="cb17-22">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(timesteps)):</span>
<span id="cb17-23">            <span class="co" style="color: #5E5E5E;"># scale the latents for particular timestep</span></span>
<span id="cb17-24">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb17-25">            </span>
<span id="cb17-26">            <span class="co" style="color: #5E5E5E;">#predict noise using UNet</span></span>
<span id="cb17-27">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-28">                </span>
<span id="cb17-29">            <span class="co" style="color: #5E5E5E;">#Perform Classifier-Free Guidance    </span></span>
<span id="cb17-30">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb17-31">            </span>
<span id="cb17-32">            <span class="co" style="color: #5E5E5E;">#Removing predicted noise from the latents</span></span>
<span id="cb17-33">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb17-34">            </span>
<span id="cb17-35">            <span class="co" style="color: #5E5E5E;">#run callbacks if any are added</span></span>
<span id="cb17-36">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb17-37">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb17-38">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb17-39">        </span>
<span id="cb17-40">        <span class="co" style="color: #5E5E5E;">#after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated</span></span>
<span id="cb17-41">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.latents_to_pil(latents)</span>
<span id="cb17-42"></span>
<span id="cb17-43">SDPipeline.<span class="fu" style="color: #4758AB;">__call__</span> <span class="op" style="color: #5E5E5E;">=</span> diff_pipeline_img2img</span></code></pre></div>
</div>
<p>Now let’s download an image and pass it to our Image To Image pipeline</p>
<div class="cell" data-outputid="d83aa588-e356-427f-8573-92ee3e0d5359">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;">from</span> fastdownload <span class="im" style="color: #00769E;">import</span> FastDownload</span>
<span id="cb18-2">p <span class="op" style="color: #5E5E5E;">=</span> FastDownload().download(<span class="st" style="color: #20794D;">'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png'</span>)</span>
<span id="cb18-3">image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(p).convert(<span class="st" style="color: #20794D;">'RGB'</span>).resize((<span class="dv" style="color: #AD0000;">512</span>,<span class="dv" style="color: #AD0000;">512</span>))</span>
<span id="cb18-4">image</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="49152" class="" max="46150" style="width:300px; height:20px; vertical-align: middle;"></progress>
      106.50% [49152/46150 00:00&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-17-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 3: Initial Image for Image To Image Pipeline
</h4>
</figcaption>
<p>We can also add our Callback function to visualize how the image is generated</p>
<div class="cell" data-outputid="21fbd64c-dac4-4f79-8e3a-c4712953e976">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb19-2"><span class="kw" style="color: #003B4F;">class</span> Latents_Callback:</span>
<span id="cb19-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, prompts):</span>
<span id="cb19-4">        <span class="va" style="color: #111111;">self</span>.imgs <span class="op" style="color: #5E5E5E;">=</span> [[] <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompts))]</span>
<span id="cb19-5">    </span>
<span id="cb19-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, i, t, latents):</span>
<span id="cb19-7">        latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb19-8">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(latents.shape[<span class="dv" style="color: #AD0000;">0</span>]):</span>
<span id="cb19-9">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample[i]</span>
<span id="cb19-10">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> create_image(callback_imgs)</span>
<span id="cb19-11">            <span class="va" style="color: #111111;">self</span>.imgs[i].append(callback_imgs)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"828e334014b94b5796ba29eab096aeed","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a388bc78fb2f4f8fbc05b9da18113511","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>We are using a random sketch of wolf howling at moon as initial image. So we can use higher value of strength to generate good image</p>
<div class="cell" data-outputid="d61c79a4-26ec-4918-a2f0-71bc9be0758e">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">prompt <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"Wolf howling at the moon, photorealistic 4K"</span>]</span>
<span id="cb20-2">pipei2i <span class="op" style="color: #5E5E5E;">=</span> SDPipeline(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c564857265014adda83d9f96c2558e28","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"80022c09b8b04b0098eca913ba42e1a3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c1b49922bb0640bdb9ef7820cf97b24a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"048a260c7cd146d9ad12d8d55bdaa74b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"55cddc447aea4423b1ab8bb0cc539ec6","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f3771ee38586467b871057bed4988e8c","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2419039a38ee46f89baf2149d01050bb","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8923024c900a4576a9b61662a9162ce0","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="72ba68de-251f-4f32-8c87-544c0d648036">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">lc <span class="op" style="color: #5E5E5E;">=</span> Latents_Callback(prompt)</span>
<span id="cb21-2">img <span class="op" style="color: #5E5E5E;">=</span> pipei2i(prompt,init_image<span class="op" style="color: #5E5E5E;">=</span>image, callbacks<span class="op" style="color: #5E5E5E;">=</span>[lc], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>,inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb21-3">img[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bfec3d4e3c644f75b928070eff93e9a6","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 4: Generated Image for Image To Image pipeline
</h4>
</figcaption>
<div class="cell" data-outputid="a319560a-040e-4f6c-a05d-190c41e07c66" data-scrolled="true">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">rows, cols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompt), <span class="bu" style="color: null;">len</span>(lc.imgs[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb22-2">imgs <span class="op" style="color: #5E5E5E;">=</span>[img <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompt)) <span class="cf" style="color: #003B4F;">for</span> img <span class="kw" style="color: #003B4F;">in</span> lc.imgs[i]]</span>
<span id="cb22-3">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span>rows, cols<span class="op" style="color: #5E5E5E;">=</span>cols)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="using-both-image-to-image-and-negative-prompt" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Using both Image to Image and Negative Prompt</h1>
<p>Let’s use both Image to Image and Negative prompt in the next example. We will download a picture of sunflower vase and pass it to Image to Image pipeline to convert it to a Rose vase. We will remove yellow color from the generated image using negative prompt</p>
<div class="cell" data-outputid="28405071-a649-4ea9-c523-f44d3cc979fd">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;">from</span> fastdownload <span class="im" style="color: #00769E;">import</span> FastDownload</span>
<span id="cb23-2">p <span class="op" style="color: #5E5E5E;">=</span> FastDownload().download(<span class="st" style="color: #20794D;">'https://img.freepik.com/premium-photo/oil-painting-yellow-flowers-sunflowers-vase-yellow-oil-paints_175677-2889.jpg?w=2000'</span>)</span>
<span id="cb23-3">image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(p).convert(<span class="st" style="color: #20794D;">'RGB'</span>).resize((<span class="dv" style="color: #AD0000;">512</span>,<span class="dv" style="color: #AD0000;">512</span>))</span>
<span id="cb23-4">image</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="794624" class="" max="793835" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.10% [794624/793835 00:00&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-22-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored" data-anchor-id="using-both-image-to-image-and-negative-prompt">
Fig 5: Initial Image of Sunflower vase
</h4>
</figcaption>
<div class="cell" data-outputid="5e8847b6-0e3f-4457-86c4-1b4fa345ed8b">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">prompt <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"A photo of a rose flower vase, photorealistic 4K"</span>]</span>
<span id="cb24-2">negative_prompts <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'yellow, lowres, worst quality, low quality'</span>]</span>
<span id="cb24-3">img <span class="op" style="color: #5E5E5E;">=</span> pipei2i(prompt, negative_prompts<span class="op" style="color: #5E5E5E;">=</span>negative_prompts,init_image<span class="op" style="color: #5E5E5E;">=</span>image,inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, strength<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.6</span>)</span>
<span id="cb24-4">img[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"456a6798e75a4ee9bff3317252c34d94","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="57">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-23-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 6: Generated Image of Rose vase for initial image of sunflower vase and Negative Prompt as yellow
</h4>
</figcaption>
<p>Until now, we saw how we can use StableDiffusion to generate realistic images based on prompts. But what if we want to generate images for objects or person which the model has never seen. What if we want to generate our own images based on prompt. This can be done using process such as Textual Inversion and Dreambooth. Let’s learn about Textual inversion in the next section</p>
</section>
<section id="textual-inversion" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Textual Inversion</h1>
<p>Textual Inversion is a process to teach a new word to the text model and get its embeddings close to the visual representation of the image. This is achieved by adding a new token to the vocabulary, freezing the weights of all the models except the text encoder and train with the images.</p>
<p>This is a schematic representation of the process by the authors of the <a href="https://textual-inversion.github.io/">Textual Inversion paper</a></p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/https:/textual-inversion.github.io/static/images/training/training.JPG" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored" data-anchor-id="textual-inversion">
Fig 7 : Textual Inversion diagram <a href="https://textual-inversion.github.io/">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>We will be implementing Dreambooth in the next post which is similar to Textual Inversion. So we will use an already trained token using Textual Inversion and use it in inference. You can train your own tokens with photos you provide using <a href="https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion">this training script</a> or <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb">Google Colab notebook</a>.</p>
<p>We’ll try an example using embeddings trained for <a href="https://huggingface.co/sd-concepts-library/indian-watercolor-portraits">this style</a>.</p>
<div class="cell" data-outputid="0c159f62-553f-439a-b0f0-763c8deeb315">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline</span>
<span id="cb25-2">pipe <span class="op" style="color: #5E5E5E;">=</span> StableDiffusionPipeline.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, revision<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"fp16"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16) </span>
<span id="cb25-3">pipe <span class="op" style="color: #5E5E5E;">=</span> pipe.to(torch_device)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b91e012d7375432e948c2e5bb0c68e26","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"23d0f35739e849238fee7bf256cbe5eb","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b0837bbbbedb417b82cdb9b81c3bb0f8","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"169b4f80589147e992aa2af6e3574f4a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"46f9ab0a4339452cb859ccfad9962222","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b4a498941beb4840bf0afba501219fd7","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d1023ef18fc94d3ca4e0d7174e0c21ac","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"89f1d234c6f64b428a20b38846e79ae1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0e962814fa014936bfd21d9941225333","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"902fd9798f334a169be1b9f374b55d9f","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c55e1ae5dd414e2aaa83782300f37d7a","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"05b76a258cf942c089ed87e92ce024a4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"74f45681ef904ae7a3224d8906b1f56c","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6ed67f8e8af048cdba31489df9c95913","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f42f7baa5bf9462a8127443702de1e96","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8b06067bf19443f5b67d206459bb56df","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3800a7c286e348c7801a60da6bd4ede8","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Let’s download the embedding for the watercolor portrait style</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">embeds_url <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin"</span></span>
<span id="cb26-2">embeds_path <span class="op" style="color: #5E5E5E;">=</span> FastDownload().download(embeds_url)</span>
<span id="cb26-3">embeds_dict <span class="op" style="color: #5E5E5E;">=</span> torch.load(<span class="bu" style="color: null;">str</span>(embeds_path), map_location<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"cpu"</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="0ae8b824-d629-4599-d107-eb0ac1e367b0">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">tokenizer <span class="op" style="color: #5E5E5E;">=</span> pipe.tokenizer</span>
<span id="cb27-2">text_encoder <span class="op" style="color: #5E5E5E;">=</span> pipe.text_encoder</span>
<span id="cb27-3">new_token, embeds <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(embeds_dict.items()))</span>
<span id="cb27-4">embeds <span class="op" style="color: #5E5E5E;">=</span> embeds.to(text_encoder.dtype)</span>
<span id="cb27-5">new_token, embeds.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>('&lt;watercolor-portrait&gt;', torch.Size([768]))</code></pre>
</div>
</div>
<p>We can add the new token which in our case is <code>&lt;watercolor-portrait&gt;</code> to the tokenizer. We also resize the token embedding of text_encoder and add the embeddings of the <code>&lt;watercolor-portrait&gt;</code> to the text_encoder</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="cf" style="color: #003B4F;">assert</span> tokenizer.add_tokens(new_token) <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="st" style="color: #20794D;">"The token already exists!"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">text_encoder.resize_token_embeddings(<span class="bu" style="color: null;">len</span>(tokenizer))</span>
<span id="cb30-2">new_token_id <span class="op" style="color: #5E5E5E;">=</span> tokenizer.convert_tokens_to_ids(new_token)</span>
<span id="cb30-3">text_encoder.get_input_embeddings().weight.data[new_token_id] <span class="op" style="color: #5E5E5E;">=</span> embeds</span></code></pre></div>
</div>
<p>We can then use the new token in the prompt to generate images</p>
<div class="cell" data-outputid="bc7f398b-5345-4eb5-e076-56e5361287e7">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb31-2">image <span class="op" style="color: #5E5E5E;">=</span> pipe(<span class="st" style="color: #20794D;">"Einstein reading newspaper in the style of &lt;watercolor-portrait&gt;"</span>).images[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb31-3">image</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ce1ea175c082483a859225925f2286a1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations_files/figure-html/cell-29-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 8: Using new token generated using Textual Inversion in prompt
</h4>
</figcaption>
</section>
<section id="conclusion" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Conclusion</h1>
<p>In this post, we have covered a brief introduction on variation provided by the diffusers library such as negative prompt and Image To Image. We then implemented both in our <code>SDPipeline</code>. We also saw how we can train a new concept using Textual Inversion. In the <a href="https://prajwal-suresh13.github.io/website/posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html">next post</a> we will be learning about Dreambooth which is used to train a new concept and push our trained model to HuggingFace Hub</p>
<p>I hope you enjoyed reading it. If there is any feedback, feel free to reach out on <a href="https://www.linkedin.com/in/prajwal-s-1416061b3/">LinkedIn</a> or on <a href="mailto:prajwalsuresh13@gmail.com">mail</a></p>
</section>
<section id="references" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> References</h1>
<ul>
<li><a href="https://www.fast.ai/posts/part2-2022-preview.html">FastAI course</a></li>
<li><a href="https://aayushmnit.com/blog.html">Aayush Agrawal’s blog</a></li>
</ul>


</section>

 ]]></description>
  <category>Stable Diffusion</category>
  <guid>https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html</guid>
  <pubDate>Wed, 23 Nov 2022 18:30:00 GMT</pubDate>
  <media:content url="https://prajwalsuresh.com/posts/2022-11-24-StableDiffusionVariations/01.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Stable diffusion - Introduction</title>
  <dc:creator>Prajwal Suresh</dc:creator>
  <link>https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Introduction to using 🤗 <a href="https://huggingface.co/">Hugging face</a> - <a href="https://github.com/huggingface/diffusers">Diffusers library</a> for Stable Diffusion, looking into the core components and implementing StableDiffusionPipeline using it</p>
</blockquote>
<br>
<div data-align="center">
<p><a href="https://colab.research.google.com/github/prajwal-suresh13/diffusion/blob/master/01_stable_diffusion_introduction.ipynb"> <img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/colab.research.google.com/assets/colab-badge.svg" width="150"> </a></p>
</div>
<p><br></p>
<p>In this post I will give an introduction on using 🤗 Diffusers library to generate images using texts as input. This post is based on the knowledge acquired while doing the <a href="https://www.fast.ai/posts/part2-2022.html"><em>‘From Deep learning foundations to Stable Diffusion’</em></a> course by <a href="https://www.fast.ai/posts/part2-2022.html">FastAI</a> . The first few lessons of the FastAI course are publicly available <a href="https://www.fast.ai/posts/part2-2022-preview.html">here</a>. Until the rest of the lessons are available, you can also checkout the first part of the <a href="https://course.fast.ai/">course</a> which is one of the best Deep Learning courses</p>
<p>At first, let’s install the diffusers library</p>
<div class="cell" data-outputid="fb0ef25e-0a88-488c-ff87-366216258407">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq accelerate transformers diffusers ftfy</span></code></pre></div>
</div>
<section id="what-is-stable-diffusion" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is Stable Diffusion</h1>
<p>Stable Diffusion is a popular text-to-image model. It is a deep learning model which takes a sentence as input and generates an image which represents the text. You can find amazing examples of images generated by the model with just text as input on the <a href="https://lexica.art/">Lexica</a> website. Stable Diffusion model can also take an image and text as input, modify the provided image and generate new images based on text.</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/pbs.twimg.com/media/FePCGVNXkAIAGG2.jpg" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored" data-anchor-id="what-is-stable-diffusion">
Fig 1: Stable Diffusion <a href="https://jalammar.github.io/illustrated-stable-diffusion/">(Image Credit)</a>
</h4>
</figcaption>
</figure>
</section>
<section id="getting-started-with-huggingface-diffusers-library" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Getting started with HuggingFace diffusers library</h1>
<section id="creating-huggingface-account-and-accepting-the-model-license" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="creating-huggingface-account-and-accepting-the-model-license"><span class="header-section-number">2.1</span> Creating HuggingFace account and accepting the model license</h2>
<p>At first, you will have to create a HuggingFace account by going to this <a href="https://huggingface.co/join">link</a>. HuggingFace provides many pretrained models for Stable Diffusion. In order to use the model, you have to accept the model license. The pretrained model we are using is <code>CompVis/stable-diffusion-v1-4</code> .Click on this <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4?text=A+pikachu+fine+dining+with+a+view+to+the+Eiffel+Tower">link</a> to accept the model license</p>
</section>
<section id="token-generation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="token-generation"><span class="header-section-number">2.2</span> Token generation</h2>
<p>User Access Tokens are the preferred way by HuggingFace to authenticate an application or notebook to Huggingface services. You can learn more about it in their <a href="https://huggingface.co/docs/hub/security-tokens#:~:text=There%20are%20plenty%20of%20ways,git%20or%20with%20basic%20authentication.">documentation</a></p>
<p>To create an access token, click on top right icon, go to Settings. Select Access Tokens and New Token. Give the token a name and the role. For following along with this post read role is sufficient. Click on Generate a Token. You can find the newly created token under User Access Token. Copy the generated token. On running <code>notebook_login()</code> command, you will be prompted to enter the token which will provide access to HuggingFace services through Jupyter Notebook</p>
<div class="cell" data-outputid="f8a63e6d-fd3d-4b50-9127-e7816d0714bf">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> huggingface_hub <span class="im" style="color: #00769E;">import</span> notebook_login</span>
<span id="cb2-2">notebook_login()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Token is valid.
Your token has been saved in your configured git credential helpers (store).
Your token has been saved to /root/.huggingface/token
Login successful</code></pre>
</div>
</div>
</section>
<section id="stablediffusionpipeline" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="stablediffusionpipeline"><span class="header-section-number">2.3</span> StableDiffusionPipeline</h2>
<p>Next we will import the required libraries. We will set a variable <code>torch_device</code> to “cuda” if GPU is available else to “cpu”. The <code>image_grid</code> function is useful to visualize the results obtained from the StableDiffusionPipeline.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb4-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> autocast</span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> transforms <span class="im" style="color: #00769E;">as</span> tfms</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb4-6"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> logging</span>
<span id="cb4-7">logging.set_verbosity_error() <span class="co" style="color: #5E5E5E;"># Suppress unnecessary warning from CLIPTextModel</span></span>
<span id="cb4-8"></span>
<span id="cb4-9"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb4-12"><span class="im" style="color: #00769E;">from</span> matplotlib <span class="im" style="color: #00769E;">import</span> pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb4-13"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb4-14"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb4-15"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span></code></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;">#checks if gpu is available if not cpu is used</span></span>
<span id="cb5-2">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> image_grid(imgs, rows, cols):</span>
<span id="cb6-2">    w,h <span class="op" style="color: #5E5E5E;">=</span> imgs[<span class="dv" style="color: #AD0000;">0</span>].size</span>
<span id="cb6-3">    grid <span class="op" style="color: #5E5E5E;">=</span> Image.new(<span class="st" style="color: #20794D;">"RGB"</span>, size<span class="op" style="color: #5E5E5E;">=</span>(cols<span class="op" style="color: #5E5E5E;">*</span>w, rows<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;">for</span> i, img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(imgs):grid.paste(img, box<span class="op" style="color: #5E5E5E;">=</span>(i<span class="op" style="color: #5E5E5E;">%</span>cols<span class="op" style="color: #5E5E5E;">*</span>w, i<span class="op" style="color: #5E5E5E;">//</span>cols<span class="op" style="color: #5E5E5E;">*</span>h))</span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;">return</span> grid</span></code></pre></div>
</div>
<p>Next step is to import the StableDiffusionPipeline from diffusers library &amp; download the model weights. The model weights are stored in Huggingface repo. We have to provide the repo path to <code>from_pretrained</code>. In our case the repo path is <code>CompVis/stable-diffusion-v1-4</code> model. We then move it to GPU if available</p>
<div class="cell" data-outputid="952f0b7f-9ab9-42c9-f825-4e70065e41de">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> StableDiffusionPipeline</span>
<span id="cb7-2"></span>
<span id="cb7-3">pipe <span class="op" style="color: #5E5E5E;">=</span> StableDiffusionPipeline.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>).to(torch_device)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f96a3018da73424da8539112c9d13d93","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d96d08d4d7404acba88ee7397c000a0b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ea42ccbfe7ce4acb99df595c19f9894f","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"63845d86d488489e98a1b8c60ecb9c3d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"516755158b1c423e8232916ee5051f2b","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"95620e1bf8f945c0a2532a8fe4f99a40","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5e72883081004f5a9feab6b218cccb52","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"92121f8aa17446f19a463dfa6360a2bc","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3944d5dab6d448b892e8a21a3cea2499","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"16d34efa17674a08bf18f9e3e5165535","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0942e4625479496380238bc28689adf0","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fdf7483ba3b24bd1826f6f054e7e0673","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"628ebf54409f4ebdb52eac905632c248","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ac7cc317a55e4b25adb44db65d9677bd","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"06f9bc93acb244f49d82757cf87a6147","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4a47bbbd34094f09a4ef42f8cba93b28","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e8ec4855395f4f57a08fee013952b357","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Now let’s pass a textual prompt to the pretrained model to generate image</p>
<div class="cell" data-outputid="1a8e3a8f-4859-4826-90f3-750efdad4aca">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb8-2">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"a photo of a puppy wearing suit"</span></span>
<span id="cb8-3">pipe(prompt).images[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5e054372f8474c6ba8748606934405f9","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 2: An example of image generated by the StableDiffusion pipeline
</h4>
</figcaption>
<p>
</p>
<p>You can also pass multiple textual prompts and the pipeline will generate images for each prompt</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">prompts <span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'a photograph of a dog wearing hat and glasses'</span>,</span>
<span id="cb9-2">          <span class="st" style="color: #20794D;">'a photograph of an astronaut riding horse '</span>,</span>
<span id="cb9-3">          <span class="st" style="color: #20794D;">'an oil painting of dog'</span>]</span></code></pre></div>
</div>
<div class="cell" data-outputid="46eeab4e-66ad-4ecf-a552-bf14632c3a19">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb10-2">images <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts).images</span>
<span id="cb10-3"><span class="bu" style="color: null;">len</span>(images)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"04f155a5f8bd4e04940523e29db4bba4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>3</code></pre>
</div>
</div>
<div class="cell" data-outputid="b7a342cc-679b-41b4-ac40-8f56f1898dbe">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">image_grid(images, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 3: Images generated by pipeline for multiple prompts(A photograph of a dog wearing hat and glasses, A photograph of an astronaut riding horse, An oil painting of dog”)
</h4>
</figcaption>
</section>
</section>
<section id="callbacks" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Callbacks</h1>
<p>You might have observed that before producing the output, there is a progress bar with certain number of steps being completed after which image is generated. Let’s visualize what happens during each step using <code>latents_callback</code> function</p>
<div class="cell" data-outputid="36628492-16b3-48da-b739-a28102ceaa1a">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">vae <span class="op" style="color: #5E5E5E;">=</span> pipe.vae</span>
<span id="cb13-2">images <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb13-3"></span>
<span id="cb13-4"><span class="kw" style="color: #003B4F;">def</span> latents_callback(i, t, latents):</span>
<span id="cb13-5">    latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb13-6">    image <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb13-7">    image <span class="op" style="color: #5E5E5E;">=</span> (image <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb13-8">    image <span class="op" style="color: #5E5E5E;">=</span> image.cpu().permute(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>).numpy()</span>
<span id="cb13-9">    images.extend(pipe.numpy_to_pil(image))</span>
<span id="cb13-10"></span>
<span id="cb13-11"></span>
<span id="cb13-12">torch.manual_seed(<span class="dv" style="color: #AD0000;">9000</span>)</span>
<span id="cb13-13"><span class="cf" style="color: #003B4F;">for</span> prompt <span class="kw" style="color: #003B4F;">in</span> prompts:</span>
<span id="cb13-14">    final_images <span class="op" style="color: #5E5E5E;">=</span> pipe(prompt, callback<span class="op" style="color: #5E5E5E;">=</span>latents_callback, callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>).images[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb13-15">    images.append(final_images)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"35c068ae5e11412a8fc66d7f1d637810","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8e80a4d3b8b947fa9b24988a3215e2db","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b77a65acce4340cea3703c948ab7f89f","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="7c4be038-55a2-499f-efd1-75559a52b693">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">image_grid(images, rows<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(prompts),cols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored" data-anchor-id="callbacks">
Fig 4: Visualizing Diffusion steps
</h4>
</figcaption>
<p>You can see in the above image that at first step for all prompts, there is only random noise and as the steps are increased we start to get images representing the prompts. So how is the model able to generate images from random noise and text? How is StableDiffusion model working?. Let’s try to answer our questions by looking under the hood of StableDiffusionPipeline</p>
</section>
<section id="deep-dive" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Deep Dive</h1>
<p>Stable Diffusion is based on a progressive denoising algorithm that is able to create images from pure random noise. These models start with pure random noise and with each step it improves the quality of generated images. Models in this family are known as diffusion models. In case of our above examples in Fig 4, the model took 51 steps and as you can see it improves the quality of generated images with each step. We can define the number of steps the model can take with <code>num_inference_steps</code> parameter.</p>
<p>Stable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>. The difference between the Standard diffusion and latent diffusion models is that when generating high-resolution images in latent diffusion the model works on the to latent(compressed) representation of the images whereas the Standard diffusion model works on actual pixel space.</p>
<p>For example, consider an image of size 512x512 needs to be generated. The Standard diffusion model works on this pixel space which is of 512x512 and starts with noise of dimensions 512x512. This is expensive as it takes more memory. But in case of Latent Diffusion, the model works on noise which in our above examples is 64x64 and then scaled to 512x512 size images. This scaling is done by models known as Autoencoder which will be explained in next section.</p>
<p>The three main components in latent diffusion are <br></p>
<ol type="1">
<li>An autoencoder, in our case, a Variational Auto Encoder (VAE)<br></li>
<li>Text-encoder, in our case, a CLIP Text Enocder<br></li>
<li>U-Net<br></li>
</ol>
<section id="autoencoder" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="autoencoder"><span class="header-section-number">4.1</span> Autoencoder</h2>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/d3i71xaburhd42.cloudfront.net/b1786e74e233ac21f503f59d03f6af19a3699024/2-Figure1-1.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 5:Autoencoder <a href="https://www.semanticscholar.org/paper/A-Better-Autoencoder-for-Image%3A-Convolutional-Zhang/b1786e74e233ac21f503f59d03f6af19a3699024">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>Autoencoders are models which take images as input and produce same images as output. So you might think how is this model even useful if it is producing same output as input. It is useful because autoencoder is made up of two parts and after training autoencoder these two parts can be used independently. The two parts are<br> 1. Encoder - Takes image as input and converts it to low dimensional latent representation ie compressed data<br> 2. Decoder - Takes compressed data and converts it back into an image<br></p>
<p>These models can be used for various applications. For example, it can be used as a compression application, where the encoder squishes the image into lower dimensions which uses less memory for storing and when the image is required we can recreate by passing the squished data to decoder. It is to be noted that the compression and decompression by the encoder-decoder is not lossless.</p>
<p>The Latent diffusion models as mentioned in the previous section works on the latent space. This latent space is produced by Encoder of Variational Autoencoder(VAE) and once we have our desired latent outputs produced by the model through diffusion process, we can convert them back to high-resolution images using VAE’s decoder</p>
<p>In case of Image to Image StableDiffusion Pipeline, the input image is passed through the VAE’s encoder and the latent inputs obtained combined with random noise are used to perform diffusion. The latent outputs produced by the diffusion process is then converted back to high resolution image using VAE’s decoder</p>
<p>In case of Text to Image StableDiffusion pipeline, only random noise is used to perform diffusion and the latent outputs produced is converted back high-resolution image using VAE’s decoder</p>
<p>Now let’s look into the VAE code. HuggingFace Diffusers library already provides us with a pretrained Autoencoder to use which can be imported as below</p>
<div class="cell" data-outputid="de34fcde-22ab-43c6-c4bd-780f84ff32f7">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL</span>
<span id="cb15-2">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span>)</span>
<span id="cb15-3">vae.to(torch_device)</span></code></pre></div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> pil_to_latents(input_img):</span>
<span id="cb16-2">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb16-3">        latent <span class="op" style="color: #5E5E5E;">=</span> vae.encode(tfms.ToTensor()(input_img).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).to(torch_device)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb16-4">    <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latent.latent_dist.sample()</span>
<span id="cb16-5"></span>
<span id="cb16-6"><span class="kw" style="color: #003B4F;">def</span> create_image(t):</span>
<span id="cb16-7">    image <span class="op" style="color: #5E5E5E;">=</span> (t<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>).detach().cpu().permute(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">0</span>).numpy()</span>
<span id="cb16-8">    <span class="cf" style="color: #003B4F;">return</span> Image.fromarray((image<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">255</span>).<span class="bu" style="color: null;">round</span>().astype(<span class="st" style="color: #20794D;">"uint8"</span>))</span>
<span id="cb16-9"></span>
<span id="cb16-10"><span class="kw" style="color: #003B4F;">def</span> latents_to_pil(latents):</span>
<span id="cb16-11">    latents <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.18215</span>) <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb16-12">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb16-13">        images <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample</span>
<span id="cb16-14">    pil_images <span class="op" style="color: #5E5E5E;">=</span>[create_image(image) <span class="cf" style="color: #003B4F;">for</span> image <span class="kw" style="color: #003B4F;">in</span> images]</span>
<span id="cb16-15">    <span class="cf" style="color: #003B4F;">return</span> pil_images</span></code></pre></div>
</details>
</div>
<p>Let’s download an image from internet, compress the image using Encoder and view the latent representation. We will use <code>pil_to_latents</code> helper function to do it</p>
<div class="cell" data-outputid="4ad6affd-867f-4aac-e839-17031a61d003">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="op" style="color: #5E5E5E;">!</span>curl <span class="op" style="color: #5E5E5E;">--</span>output macaw.jpg <span class="st" style="color: #20794D;">'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'</span></span>
<span id="cb17-2">input_image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(<span class="st" style="color: #20794D;">'macaw.jpg'</span>).resize((<span class="dv" style="color: #AD0000;">512</span>, <span class="dv" style="color: #AD0000;">512</span>))</span>
<span id="cb17-3">input_image</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 62145  100 62145    0     0   240k      0 --:--:-- --:--:-- --:--:--  240k</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="64254c93-176a-40f8-cff4-889c28be06a8">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">encoded <span class="op" style="color: #5E5E5E;">=</span> pil_to_latents(input_image)</span>
<span id="cb19-2">fig, axs <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">4</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb19-3"><span class="cf" style="color: #003B4F;">for</span> c <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb19-4">    axs[c].imshow(encoded[<span class="dv" style="color: #AD0000;">0</span>][c].cpu(), cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Greys'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<figcaption align="center">
<h4 class="anchored">
Fig 6: Latent Representation of the Input image
</h4>
</figcaption>
<div class="cell" data-outputid="3d0180b5-d954-4cb7-d0b3-e660f0305ee7" data-scrolled="true">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">encoded.shape,tfms.ToTensor()(input_image).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>(torch.Size([1, 4, 64, 64]), torch.Size([1, 3, 512, 512]))</code></pre>
</div>
</div>
<p>As you can see the VAE has compressed 3x512x512 image into 4x64x64 image. That is a compression ratio of 48x. These latent representation has captured information about the original image and on passing them to the VAE’s decoder provide the original image. For this we use the <code>latents_to_pil</code> function.</p>
<div class="cell" data-outputid="0d76a0f5-2a02-4e06-dda8-701e636e5b00">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">decoded <span class="op" style="color: #5E5E5E;">=</span> latents_to_pil(encoded)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb22-2">decoded</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So we understood how Stable diffusion can create images from random noise using diffusion process. But how is it able to generate images representing the text? Let’s find out in the next section.</p>
</section>
<section id="text-encoder" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="text-encoder"><span class="header-section-number">4.2</span> Text Encoder</h2>
<p>Deep Learning models only understand numbers and not text. So inorder to convert the text data into numbers which our models can work with Tokenizers and Text Encoders are used</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/developers.google.com/static/machine-learning/guides/text-classification/images/EmbeddingLayer.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 7 : Tokenizer and Text Encoder <a href="https://developers.google.com/machine-learning/guides/text-classification/step-3">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>This process of converting text to numbers is done in two steps:<br> 1. Tokenizer - The prompt provided is broken down into words and then it uses a lookup table to convert them into a number. As you can see in the Fig 7 , the input sentences are converted to numbers using the lookup table<br> 2. Text Encoder - This converts numbers into a high dimensional vectors which captures the meaning of the words. These are called embeddings. In Fig 7, the Text Encoder is represented as Embedding layer.<br></p>
<section id="what-is-embedding" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="what-is-embedding"><span class="header-section-number">4.2.1</span> What is embedding?</h3>
<p>Embedding is a high dimensional vector that allows words with similar meaning to have the same representation. It can approximate the meaning of the word. An embedding with 50 values holds the capability of representing 50 unique features. This is known as embedding size. In the Fig 7, the embedding size used to represent each word is 4</p>
</section>
<section id="how-clip-model-works" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="how-clip-model-works"><span class="header-section-number">4.2.2</span> How CLIP model works</h3>
<p>The embedding from the text encoder in the Fig 7 represents the meaning of each word in the given prompt. But we also need the same embeddings to represent an image which is described in the prompt. We need a model that connects text and images. CLIP model acts as bridge that connects texts and images</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/cdn.dida.do/blog/20210621_fg_clip/contrastive_pretraining.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 8 : CLIP model <h7><a href="https://arxiv.org/abs/2103.00020">(Image Credit)</a></h7>
</h4>
</figcaption>
</figure>
<p>The CLIP model consists of two sub-models called encoders:<br> 1. Text Encoder - Converts text to embeddings<br> 2. Image encoder - Converts Image to embedding<br></p>
<p>The embeddings produced by both the encoders needs to be similar if both text and image represent same thing and should be different otherwise</p>
<p>For example, lets consider the embedding size as 4. Suppose the text “pepper the aussie pup” when passed to Text encoder produces embeddings (0, 0.5, 0.8, 0.1) then the embeddings generated when image of the Pepper the Aussie Pup is passed to the Image encoder should be similar to that produced by the text encoder ie (0.05, 0.52, 0.78, 0.13). You can learn more about the CLIP model <a href="https://openai.com/blog/clip/">here</a></p>
<p>We use the pretrained Text Encoder of the CLIP model to generate embeddings for our prompts. Let’s look into the CLIPTextEncoder code</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'A picture of a puppy'</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTokenizer, CLIPTextModel</span>
<span id="cb24-2"></span>
<span id="cb24-3">tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-large-patch14"</span>)</span>
<span id="cb24-4">text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-large-patch14"</span>)</span>
<span id="cb24-5">text_encoder <span class="op" style="color: #5E5E5E;">=</span> text_encoder.to(torch_device)</span></code></pre></div>
</div>
<p>Tokenizer expects 77 length vector. So the padding token 49407 is repeated till the length of 77 is reached. The embedding size of the CLIPTextModel is 768.</p>
<div class="cell" data-outputid="1e8a5042-9f27-464b-b124-d497d95ceb51">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">text_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer(prompt, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>tokenizer.model_max_length, trucation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb25-2">text_input[<span class="st" style="color: #20794D;">'input_ids'</span>][<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([49406,   320,  1674,   539,   320,  6829, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
        49407, 49407, 49407, 49407, 49407, 49407, 49407])</code></pre>
</div>
</div>
<p>We can find to which word each number is mapped using tokenizer’s decoder.</p>
<div class="cell" data-outputid="c7891799-2e58-4971-cc10-3ba27512f4df">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> text_input[<span class="st" style="color: #20794D;">'input_ids'</span>][<span class="dv" style="color: #AD0000;">0</span>][:<span class="dv" style="color: #AD0000;">8</span>]:<span class="bu" style="color: null;">print</span>(t, tokenizer.decoder.get(<span class="bu" style="color: null;">int</span>(t)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(49406) &lt;|startoftext|&gt;
tensor(320) a&lt;/w&gt;
tensor(1674) picture&lt;/w&gt;
tensor(539) of&lt;/w&gt;
tensor(320) a&lt;/w&gt;
tensor(6829) puppy&lt;/w&gt;
tensor(49407) &lt;|endoftext|&gt;
tensor(49407) &lt;|endoftext|&gt;</code></pre>
</div>
</div>
<p>The numbers from Tokenizer is passed to Text encoder to generate embeddings</p>
<div class="cell" data-outputid="e67cc76f-46e3-4a55-9ed7-b6c81dadd3ca">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">output_embeddings <span class="op" style="color: #5E5E5E;">=</span> text_encoder(text_input.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb29-2">output_embeddings</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],
         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],
         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],
         ...,
         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],
         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],
         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],
       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="classifier--free-guidance" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="classifier--free-guidance"><span class="header-section-number">4.2.3</span> Classifier- Free Guidance</h3>
<p>Classifier-Free Guidance is a method to increase the adherence of the output to the text provided as input. Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. The default value of guidance is 7.5</p>
<p>In order to implement this, along with the embeddings from the prompt we also pass embeddings for an empty string.This is known as Unconditional embedding. This helps the model to go in which ever direction it wants as long as it results in a reasonably-looking image. The prediction are then calculated as follows</p>
<pre><code>prediction = unconditional_output + guidance(text_output - unconditional_output)
</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">max_length <span class="op" style="color: #5E5E5E;">=</span> text_input.input_ids.shape[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb32-2">uncond_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer([<span class="st" style="color: #20794D;">""</span>] <span class="op" style="color: #5E5E5E;">*</span> <span class="bu" style="color: null;">len</span>(prompts), padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>max_length, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb32-3">uncond_embeddings <span class="op" style="color: #5E5E5E;">=</span> text_encoder(uncond_input.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb32-4">emb <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond_embeddings, output_embeddings])</span></code></pre></div>
</div>
</section>
</section>
<section id="unet" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="unet"><span class="header-section-number">4.3</span> UNet</h2>
<p>The UNet is the model which produces latent outputs by removing noise.</p>
<p>UNet model takes three inputs: <br> 1. Noisy latent or Noise - If initial image is provided, noisy latents are produced by adding VAE encoder’s latents and noise otherwise it takes pure noise as input and produces new image based on the textual description<br> 2. Timestep - Timestep is the number of times the diffusion process is repeated. It takes as input the current timestep<br> 3. Embeddings - Embeddings generated by CLIP for the input prompts and unconditional embeddings<br></p>
<p>The UNet model predicts the noise which is then subtracted from the Noisy latents. This process is repeated for the timesteps mentioned and at the end, the latent outputs produced are passed to VAE’s decoder to generate High-resolution image</p>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/miro.medium.com/max/720/0*4CMZbQvwXaSjQRRX.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 9 : UNet <a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8#:~:text=Stable%20diffusion%20only%20uses%20a,are%20similar%20in%20latent%20space.">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>Let’s look into the code for UNet. Along with UNet we also import a scheduler. Scheduler determines the amount of noise to be added to latent at a given step in diffusion process</p>
<div class="cell" data-outputid="58c66dde-e462-4864-f190-295d27059629">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> UNet2DConditionModel, LMSDiscreteScheduler</span>
<span id="cb33-2"></span>
<span id="cb33-3">scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb33-4">scheduler.set_timesteps(<span class="dv" style="color: #AD0000;">51</span>)</span>
<span id="cb33-5"></span>
<span id="cb33-6">unet <span class="op" style="color: #5E5E5E;">=</span> UNet2DConditionModel.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>)</span>
<span id="cb33-7">unet.to(torch_device)</span></code></pre></div>
</div>
<p>You can see the amount of noise added in each step below. This is called sigmas. The noise added is highest at first step and is gradually decreased in the following steps until it becomes zero at the last step</p>
<div class="cell" data-outputid="5d458232-4a69-4fb3-95cc-658d38e1f380">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="bu" style="color: null;">print</span>(scheduler.sigmas)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([14.6146, 12.9679, 11.5454, 10.3129,  9.2414,  8.3072,  7.4900,  6.7731,
         6.1422,  5.5854,  5.0924,  4.6547,  4.2650,  3.9169,  3.6051,  3.3251,
         3.0728,  2.8449,  2.6383,  2.4507,  2.2797,  2.1235,  1.9804,  1.8488,
         1.7276,  1.6156,  1.5118,  1.4154,  1.3255,  1.2415,  1.1629,  1.0890,
         1.0193,  0.9535,  0.8912,  0.8319,  0.7754,  0.7213,  0.6695,  0.6195,
         0.5712,  0.5242,  0.4784,  0.4333,  0.3885,  0.3437,  0.2981,  0.2505,
         0.1989,  0.1379,  0.0292,  0.0000])</code></pre>
</div>
</div>
<p>Let’s add noise to the latents obtained from VAE’s encoder earlier and see how U-Net removes noise</p>
<div class="cell" data-outputid="ab6a7c91-f609-4598-b6a4-6005e568f2bb">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">noise <span class="op" style="color: #5E5E5E;">=</span> torch.randn_like(encoded)</span>
<span id="cb36-2">encoded_and_noised <span class="op" style="color: #5E5E5E;">=</span> scheduler.add_noise(encoded, noise, timesteps<span class="op" style="color: #5E5E5E;">=</span>torch.tensor([scheduler.timesteps[<span class="dv" style="color: #AD0000;">40</span>]]))</span>
<span id="cb36-3">latents_to_pil(encoded_and_noised)[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="f27d1314-aa60-4c0d-e070-73c990b9587d">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="co" style="color: #5E5E5E;">#Unconditional prompt</span></span>
<span id="cb37-2">prompt<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">""</span>]</span>
<span id="cb37-3"></span>
<span id="cb37-4"><span class="co" style="color: #5E5E5E;">## tokenizing and getting embeddings from clip model</span></span>
<span id="cb37-5">text_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer(prompt, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>tokenizer.model_max_length, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb37-6"></span>
<span id="cb37-7"><span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb37-8">    text_embeddings<span class="op" style="color: #5E5E5E;">=</span>text_encoder(text_input.input_ids.to(<span class="st" style="color: #20794D;">"cuda"</span>))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb37-9"></span>
<span id="cb37-10"><span class="co" style="color: #5E5E5E;">#Using U-Net to predict noise</span></span>
<span id="cb37-11">latent_model_input <span class="op" style="color: #5E5E5E;">=</span> torch.cat([encoded_and_noised.to(<span class="st" style="color: #20794D;">"cuda"</span>).<span class="bu" style="color: null;">float</span>()])</span>
<span id="cb37-12"><span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb37-13">    noise_pred <span class="op" style="color: #5E5E5E;">=</span> unet(latent_model_input, <span class="dv" style="color: #AD0000;">40</span>, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>text_embeddings)[<span class="st" style="color: #20794D;">"sample"</span>]</span>
<span id="cb37-14"></span>
<span id="cb37-15"><span class="co" style="color: #5E5E5E;">#Visualize after subtracting noise</span></span>
<span id="cb37-16">latents_to_pil(encoded_and_noised <span class="op" style="color: #5E5E5E;">-</span>noise_pred)[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>You can see that the output image is clearer than the provided noisy image.</p>
</section>
</section>
<section id="implementing-stablediffusionpipeline" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Implementing StableDiffusionPipeline</h1>
<figure align="center" class="figure">
<img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/https:/miro.medium.com/max/720/0*z5eQUBRBVtgD3Vgv.png" width="800" class="figure-img">
<figcaption align="center" class="figure-caption">
<h4 class="anchored">
Fig 10 : Diffusion process <a href="https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8">(Image Credit)</a>
</h4>
</figcaption>
</figure>
<p>
Now that we have understood the core components of the StableDiffusion, let’s summarize the diffusion process<br> 1. The prompts provided by the end user is converted to embeddings by the CLIPTextModel<br> 2. If input image is provided, <br>  - It is converted to latents by the VAE’s encoder and combined with noise from the Scheduler. <br> Else,<br>  - Random noise is generated and combined with noise from the Scheduler <br> 3. The embeddings (textual and unconditional) and the latents are provided as input to the UNet along with the current timestep <br> 4. UNet predicts noise and this noise is removed from the latents. This process is repeated N times ie num_inference_steps <br> 5. The latents produced by the UNet are passed to VAE’s decoder to get high resolution image <br>
</p>
<p>Finally, let’s try to implement <code>StableDiffusionPipeline</code> using these core components. Let’s call our pipeline as <code>SDPipeline</code></p>
<section id="initialize-models" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="initialize-models"><span class="header-section-number">5.1</span> Initialize models</h2>
<p>First lets import the model required from diffusers and transformers library. We then create a class <code>SDPipeline</code> and the <code>__init__</code> method takes model_path as input which in our case is <code>CompVis/stable-diffusion-v1-4</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb38-2"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">class</span> SDPipeline():</span>
<span id="cb39-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model_path):</span>
<span id="cb39-3">        <span class="va" style="color: #111111;">self</span>.model_path<span class="op" style="color: #5E5E5E;">=</span>model_path</span>
<span id="cb39-4">        <span class="va" style="color: #111111;">self</span>.vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(model_path,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb39-5">        <span class="va" style="color: #111111;">self</span>.unet<span class="op" style="color: #5E5E5E;">=</span>UNet2DConditionModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb39-6">        <span class="va" style="color: #111111;">self</span>.text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"text_encoder"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb39-7">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(model_path, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"tokenizer"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16)</span>
<span id="cb39-8">        <span class="va" style="color: #111111;">self</span>.scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span></code></pre></div>
</div>
</section>
<section id="tokenize-texts" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="tokenize-texts"><span class="header-section-number">5.2</span> Tokenize Texts</h2>
<p>As explained under Classifier-Free Guidance, we not only pass the embeddings of the prompt but also embeddings for empty string. So let’s create a function which tokenizes and provides embeddings. We can make use of the dynamic nature of the Python language to add this function dynamically to class <code>SDPipeline</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="kw" style="color: #003B4F;">def</span> encode_text(<span class="va" style="color: #111111;">self</span>, prompts, maxlen<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb40-2">    <span class="cf" style="color: #003B4F;">if</span> maxlen <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: maxlen <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer.model_max_length</span>
<span id="cb40-3">    inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tokenizer(prompts, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>maxlen, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb40-4">    <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.text_encoder(inp.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>].half()</span>
<span id="cb40-5"></span>
<span id="cb40-6">SDPipeline.encode_text <span class="op" style="color: #5E5E5E;">=</span> encode_text</span></code></pre></div>
</div>
</section>
<section id="diffusion-loop" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="diffusion-loop"><span class="header-section-number">5.3</span> Diffusion Loop</h2>
<p>The diffusion loop includes all the steps which we summarized at the starting of this section. We can also add this to class <code>SDPipeline</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="kw" style="color: #003B4F;">def</span> diff_pipeline(<span class="va" style="color: #111111;">self</span>,prompts, guidance<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, inference_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">51</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, callbacks<span class="op" style="color: #5E5E5E;">=</span>[], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb41-2">        <span class="co" style="color: #5E5E5E;">#calculate number of prompts</span></span>
<span id="cb41-3">        batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts)</span>
<span id="cb41-4">        </span>
<span id="cb41-5">        <span class="co" style="color: #5E5E5E;">#generate text_embeddings</span></span>
<span id="cb41-6">        cond   <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text(prompts)</span>
<span id="cb41-7">        </span>
<span id="cb41-8">        <span class="co" style="color: #5E5E5E;">#generate unconditional embeddings and concat them</span></span>
<span id="cb41-9">        uncond <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.encode_text([<span class="st" style="color: #20794D;">""</span>]<span class="op" style="color: #5E5E5E;">*</span>batch_size, cond.shape[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb41-10">        emb    <span class="op" style="color: #5E5E5E;">=</span> torch.cat([uncond, cond])</span>
<span id="cb41-11">        <span class="cf" style="color: #003B4F;">if</span> seed : torch.manual_seed(seed)</span>
<span id="cb41-12">        </span>
<span id="cb41-13">        <span class="co" style="color: #5E5E5E;">#generate random noise and add noise</span></span>
<span id="cb41-14">        latents <span class="op" style="color: #5E5E5E;">=</span> torch.randn((batch_size, <span class="va" style="color: #111111;">self</span>.unet.in_channels, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">512</span><span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">8</span>))</span>
<span id="cb41-15">        </span>
<span id="cb41-16">        <span class="co" style="color: #5E5E5E;">#Setting number of steps in scheduler</span></span>
<span id="cb41-17">        <span class="va" style="color: #111111;">self</span>.scheduler.set_timesteps(inference_steps)</span>
<span id="cb41-18">        </span>
<span id="cb41-19">        <span class="co" style="color: #5E5E5E;">#Add noise to latents</span></span>
<span id="cb41-20">        latents <span class="op" style="color: #5E5E5E;">=</span> latents.to(torch_device).half() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.scheduler.init_noise_sigma</span>
<span id="cb41-21"></span>
<span id="cb41-22">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tqdm(<span class="va" style="color: #111111;">self</span>.scheduler.timesteps)):</span>
<span id="cb41-23">            <span class="co" style="color: #5E5E5E;"># scale the latents for particular timestep</span></span>
<span id="cb41-24">            inp <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.scale_model_input(torch.cat([latents]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>), t)</span>
<span id="cb41-25">            </span>
<span id="cb41-26">            <span class="co" style="color: #5E5E5E;">#predict noise using UNet</span></span>
<span id="cb41-27">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():u,c <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unet(inp, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb41-28">                </span>
<span id="cb41-29">            <span class="co" style="color: #5E5E5E;">#Perform Classifier-Free Guidance    </span></span>
<span id="cb41-30">            pred <span class="op" style="color: #5E5E5E;">=</span> u <span class="op" style="color: #5E5E5E;">+</span> guidance<span class="op" style="color: #5E5E5E;">*</span>(c<span class="op" style="color: #5E5E5E;">-</span>u)</span>
<span id="cb41-31">            </span>
<span id="cb41-32">            <span class="co" style="color: #5E5E5E;">#Removing predicted noise from the latents</span></span>
<span id="cb41-33">            latents <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.scheduler.step(pred, t, latents).prev_sample</span>
<span id="cb41-34">            </span>
<span id="cb41-35">            <span class="co" style="color: #5E5E5E;">#run callbacks if any are added</span></span>
<span id="cb41-36">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(callbacks):</span>
<span id="cb41-37">                <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span>callback_steps<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb41-38">                    <span class="cf" style="color: #003B4F;">for</span> callback <span class="kw" style="color: #003B4F;">in</span> callbacks: callback(i, t, latents)</span>
<span id="cb41-39">        </span>
<span id="cb41-40">        <span class="co" style="color: #5E5E5E;">#after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated</span></span>
<span id="cb41-41">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb41-42">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.vae.decode(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.18125</span> <span class="op" style="color: #5E5E5E;">*</span> latents).sample</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">SDPipeline.<span class="fu" style="color: #4758AB;">__call__</span> <span class="op" style="color: #5E5E5E;">=</span> diff_pipeline</span></code></pre></div>
</div>
</section>
<section id="adding-callback" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="adding-callback"><span class="header-section-number">5.4</span> Adding Callback</h2>
<p>Let’s create a callback as well, which will help to visualize the steps like we did at the starting . We will create a class which stores intermediate steps images. Since they are latents we use VAE to decode and display for us</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>,subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"vae"</span> ,torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb43-2"><span class="kw" style="color: #003B4F;">class</span> Latents_Callback:</span>
<span id="cb43-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, prompts):</span>
<span id="cb43-4">        <span class="va" style="color: #111111;">self</span>.imgs <span class="op" style="color: #5E5E5E;">=</span> [[] <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompts))]</span>
<span id="cb43-5">    </span>
<span id="cb43-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, i, t, latents):</span>
<span id="cb43-7">        latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb43-8">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(latents.shape[<span class="dv" style="color: #AD0000;">0</span>]):</span>
<span id="cb43-9">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample[i]</span>
<span id="cb43-10">            callback_imgs <span class="op" style="color: #5E5E5E;">=</span> create_image(callback_imgs)</span>
<span id="cb43-11">            <span class="va" style="color: #111111;">self</span>.imgs[i].append(callback_imgs)</span></code></pre></div>
</div>
</section>
<section id="running-our-sdpipeline" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="running-our-sdpipeline"><span class="header-section-number">5.5</span> Running our SDPipeline</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">prompts <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb44-2">    <span class="st" style="color: #20794D;">'a photograph of an astronaut riding a horse'</span>,</span>
<span id="cb44-3">    <span class="st" style="color: #20794D;">'an oil painting of an astronaut riding a horse in the style of grant wood'</span>,</span>
<span id="cb44-4">    <span class="st" style="color: #20794D;">'a painting of dog wearing suit'</span></span>
<span id="cb44-5">]</span></code></pre></div>
</div>
<p>Initializing callbacks and SDPipeline objects</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">lc <span class="op" style="color: #5E5E5E;">=</span> Latents_Callback(prompts)</span></code></pre></div>
</div>
<div class="cell" data-outputid="c834a081-77be-4cea-f069-c0dec736166a">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">pipe <span class="op" style="color: #5E5E5E;">=</span> SDPipeline(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a1c1f87c8a6942e18f6d0fe62400a1c2","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bf768efb17549d0bec616fbaf9d15c9","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ef25f6769af44ee8a9de78523d1ebe9d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"81ab569df6bf44139250ea42db8b5b75","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ef4d487ab4d64cb7b5a3f24aa90a1809","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9938302db04e4511b25604a679e1c387","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Let’s pass the prompts and the callback object to the SDPipeline object <code>pipe</code>. We can see the images generated for each prompt and can also visualize the steps by displaying images saved in the callback object</p>
<div class="cell" data-outputid="df3109c2-d060-4760-a168-40a501848bc4">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">images <span class="op" style="color: #5E5E5E;">=</span> pipe(prompts, callbacks<span class="op" style="color: #5E5E5E;">=</span>[lc], callback_steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"00db2403a2d94ced973b0072df15a1c7","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-outputid="bf277a09-9110-4ce0-8db9-187117aeaebd">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">imgs <span class="op" style="color: #5E5E5E;">=</span> [create_image(i) <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> images]</span>
<span id="cb48-2">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, cols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(imgs))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="ed34d5b0-081f-4831-92af-41b3dc9fd76a">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">rows, cols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(prompts), <span class="bu" style="color: null;">len</span>(lc.imgs[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb49-2">imgs <span class="op" style="color: #5E5E5E;">=</span>[img <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(prompts)) <span class="cf" style="color: #003B4F;">for</span> img <span class="kw" style="color: #003B4F;">in</span> lc.imgs[i]]</span>
<span id="cb49-3">image_grid(imgs, rows<span class="op" style="color: #5E5E5E;">=</span>rows, cols<span class="op" style="color: #5E5E5E;">=</span>cols)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<p><img src="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>In this post, we have covered a brief introduction of Stable diffusion. After that, in later sections we understood each components of the Stable Diffusion model along with code. At the end, we implemented <code>SDPipeline</code> using the core components, generated images for multiple prompts and also visualized the images generated at particular timesteps.. In the <a href="https://prajwal-suresh13.github.io/website/posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html">next post</a> we will be looking into additional functionality such as Negative prompt, Image to Image and Textual Inversion</p>
<p>I hope you enjoyed reading it. If there is any feedback, feel free to reach out on <a href="https://www.linkedin.com/in/prajwal-s-1416061b3/">LinkedIn</a> or on <a href="mailto:prajwalsuresh13@gmail.com">mail</a></p>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<ul>
<li><a href="https://www.fast.ai/posts/part2-2022-preview.html">FastAI course</a></li>
<li><a href="https://aayushmnit.com/blog.html">Aayush Agrawal’s blog</a></li>
</ul>


</section>

 ]]></description>
  <category>Stable Diffusion</category>
  <guid>https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html</guid>
  <pubDate>Mon, 21 Nov 2022 18:30:00 GMT</pubDate>
  <media:content url="https://prajwalsuresh.com/posts/2022-11-22-StableDiffusionIntro/01.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
