[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Prajwal Suresh",
    "section": "",
    "text": "Hi there, I‚Äôm Prajwal Suresh. I‚Äôm currently exploring Deep learning, machine learning and data engineering technologies. This is where I write about my learning journey and explorations\n\n\nExperienceSkillsEducation\n\n\n\n\nBig Data Engineer\nTata Consultancy Services Private Limited, Bengaluru, India\n\n\nAugust 2021 - Present\n\n\n\nManaging 275 nodes Hadoop Cloudera cluster, handling 10PiB of data and managing 250+ users effectively\nAutomating tasks using Ansible and project space creation for users\n\n\n\n\n\nProgramming\nPython, C++, Java\n\n\n\n\nMachine Learning\nPyTorch, fastai, Scikit-Learn, Pandas, Numpy, MatplotLib, OpenCV, HuggingFace\n\n\n\n\nBig Data\nHadoop, Spark, Docker, Airflow, Cloudera, Cloud, Linux, SQL, Ansible\n\n\n\n\nWeb Development\nDjango, Flask, PHP, HTML, CSS, JavaScript\n\n\n\n\n\n\nSapthagiri College of Engineering, Bengaluru, India\nBachelor of Engineering in Computer Science & Engineering | GPA: 8.55/10\n\n\n2017-2021\n\n\n\n\nMES PU College of Arts, Commerce & Science, Bengaluru, India\nPre-University Education | Percentage: 87.16%\n\n\n2017\n\n\n\n\nSt Mary‚Äôs High School, Bengaluru, India\nTenth Standard | Percentage: 97.6%\n\n\n2015"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Variations\n\n\n6 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Introduction\n\n\n13 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "",
    "text": "Introduction to using ü§ó Hugging face - Diffusers library for Stable Diffusion, looking into the core components and implementing StableDiffusionPipeline using it\nIn this post I will give an introduction on using ü§ó Diffusers library to generate images using texts as input. This blog is based on the knowledge acquired while doing the ‚ÄòFrom Deep learning foundations to Stable Diffusion‚Äô course by FastAI . The first few lessons of the FastAI course are publicly available here. Until the rest of the lessons are available, you can also checkout the first part of the course which is one of the best Deep Learning courses\nAt first, let‚Äôs install the diffusers library"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#creating-huggingface-account-and-accepting-the-model-license",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#creating-huggingface-account-and-accepting-the-model-license",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "2.1 Creating HuggingFace account and accepting the model license",
    "text": "2.1 Creating HuggingFace account and accepting the model license\nAt first, you will have to create a HuggingFace account by going to this link. HuggingFace provides many pretrained models for Stable Diffusion. In order to use the model, you have to accept the model license. The pretrained model we are using is CompVis/stable-diffusion-v1-4 .Click on this link to accept the model license"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#token-generation",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#token-generation",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "2.2 Token generation",
    "text": "2.2 Token generation\nUser Access Tokens are the preferred way by HuggingFace to authenticate an application or notebook to Huggingface services. You can learn more about it in their documentation\nTo create an access token, click on top right icon, go to Settings. Select Access Tokens and New Token. Give the token a name and the role. For following along with this post read role is sufficient. Click on Generate a Token. You can find the newly created token under User Access Token. Copy the generated token. On running notebook_login() command, you will be prompted to enter the token which will provide access to HuggingFace services through Jupyter Notebook\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#stablediffusionpipeline",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#stablediffusionpipeline",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "2.3 StableDiffusionPipeline",
    "text": "2.3 StableDiffusionPipeline\nNext we will import the required libraries. We will set a variable torch_device to ‚Äúcuda‚Äù if GPU is available else to ‚Äúcpu‚Äù. The image_grid function is useful to visualize the results obtained from the StableDiffusionPipeline.\n\n\nCode\nimport torch\nfrom torch import autocast\nfrom torchvision import transforms as tfms\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom transformers import logging\nlogging.set_verbosity_error() # Suppress unnecessary warning from CLIPTextModel\n\nfrom diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\n\n#checks if gpu is available if not cpu is used\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols*w, rows*h))\n    for i, img in enumerate(imgs):grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nNext step is to import the StableDiffusionPipeline from diffusers library & download the model weights. The model weights are stored in Huggingface repo. We have to provide the repo path to from_pretrained. In our case the repo path is CompVis/stable-diffusion-v1-4 model. We then move it to GPU if available\n\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(torch_device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let‚Äôs pass a textual prompt to the pretrained model to generate image\n\ntorch.manual_seed(9000)\nprompt = \"a photo of a puppy wearing suit\"\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 2: An example of image generated by the StableDiffusion pipeline\n\n\n\n\nYou can also pass multiple textual prompts and the pipeline will generate images for each prompt\n\nprompts =['a photograph of a dog wearing hat and glasses',\n          'a photograph of an astronaut riding horse ',\n          'an oil painting of dog']\n\n\ntorch.manual_seed(9000)\nimages = pipe(prompts).images\nlen(images)\n\n\n\n\n3\n\n\n\nimage_grid(images, rows=1, cols=3)\n\n\n\n\n\n\nFig 3: Images generated by pipeline for multiple prompts(A photograph of a dog wearing hat and glasses, A photograph of an astronaut riding horse, An oil painting of dog‚Äù)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#autoencoder",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#autoencoder",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "4.1 Autoencoder",
    "text": "4.1 Autoencoder\n\n\n\n\nFig 5:Autoencoder (Image Credit)\n\n\n\nAutoencoders are models which take images as input and produce same images as output. So you might think how is this model even useful if it is producing same output as input. It is useful because autoencoder is made up of two parts and after training autoencoder these two parts can be used independently. The two parts are 1. Encoder - Takes image as input and converts it to low dimensional latent representation ie compressed data 2. Decoder - Takes compressed data and converts it back into an image\nThese models can be used for various applications. For example, it can be used as a compression application, where the encoder squishes the image into lower dimensions which uses less memory for storing and when the image is required we can recreate by passing the squished data to decoder. It is to be noted that the compression and decompression by the encoder-decoder is not lossless.\nThe Latent diffusion models as mentioned in the previous section works on the latent space. This latent space is produced by Encoder of Variational Autoencoder(VAE) and once we have our desired latent outputs produced by the model through diffusion process, we can convert them back to high-resolution images using VAE‚Äôs decoder\nIn case of Image to Image StableDiffusion Pipeline, the input image is passed through the VAE‚Äôs encoder and the latent inputs obtained combined with random noise are used to perform diffusion. The latent outputs produced by the diffusion process is then converted back to high resolution image using VAE‚Äôs decoder\nIn case of Text to Image StableDiffusion pipeline, only random noise is used to perform diffusion and the latent outputs produced is converted back high-resolution image using VAE‚Äôs decoder\nNow let‚Äôs look into the VAE code. HuggingFace Diffusers library already provides us with a pretrained Autoencoder to use which can be imported as below\n\nfrom diffusers import AutoencoderKL\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\nvae.to(torch_device)\n\n\n\nCode\ndef pil_to_latents(input_img):\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_img).unsqueeze(0).to(torch_device)*2-1)\n    return 0.18215 * latent.latent_dist.sample()\n\ndef create_image(t):\n    image = (t/2 +0.5).clamp(0,1).detach().cpu().permute(1,2,0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\ndef latents_to_pil(latents):\n    latents = (1/0.18215) * latents\n    with torch.no_grad():\n        images = vae.decode(latents).sample\n    pil_images =[create_image(image) for image in images]\n    return pil_images\n\n\nLet‚Äôs download an image from internet, compress the image using Encoder and view the latent representation. We will use pil_to_latents helper function to do it\n\n!curl --output macaw.jpg 'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'\ninput_image = Image.open('macaw.jpg').resize((512, 512))\ninput_image\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 62145  100 62145    0     0   240k      0 --:--:-- --:--:-- --:--:--  240k\n\n\n\n\n\n\n\n\n\n\nencoded = pil_to_latents(input_image)\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(encoded[0][c].cpu(), cmap='Greys')\n\n\n\n\n\n\nFig 6: Latent Representation of the Input image\n\n\n\nencoded.shape,tfms.ToTensor()(input_image).unsqueeze(0).shape\n\n(torch.Size([1, 4, 64, 64]), torch.Size([1, 3, 512, 512]))\n\n\nAs you can see the VAE has compressed 3x512x512 image into 4x64x64 image. That is a compression ratio of 48x. These latent representation has captured information about the original image and on passing them to the VAE‚Äôs decoder provide the original image. For this we use the latents_to_pil function.\n\ndecoded = latents_to_pil(encoded)[0]\ndecoded\n\n\n\n\n\n\n\n\nSo we understood how Stable diffusion can create images from random noise using diffusion process. But how is it able to generate images representing the text? Let‚Äôs find out in the next section."
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#text-encoder",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#text-encoder",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "4.2 Text Encoder",
    "text": "4.2 Text Encoder\nDeep Learning models only understand numbers and not text. So inorder to convert the text data into numbers which our models can work with Tokenizers and Text Encoders are used\n\n\n\n\nFig 7 : Tokenizer and Text Encoder (Image Credit)\n\n\n\nThis process of converting text to numbers is done in two steps: 1. Tokenizer - The prompt provided is broken down into words and then it uses a lookup table to convert them into a number. As you can see in the Fig 7 , the input sentences are converted to numbers using the lookup table 2. Text Encoder - This converts numbers into a high dimensional vectors which captures the meaning of the words. These are called embeddings. In Fig 7, the Text Encoder is represented as Embedding layer.\n\n4.2.1 What is embedding?\nEmbedding is a high dimensional vector that allows words with similar meaning to have the same representation. It can approximate the meaning of the word. An embedding with 50 values holds the capability of representing 50 unique features. This is known as embedding size. In the Fig 7, the embedding size used to represent each word is 4\n\n\n4.2.2 How CLIP model works\nThe embedding from the text encoder in the Fig 7 represents the meaning of each word in the given prompt. But we also need the same embeddings to represent an image which is described in the prompt. We need a model that connects text and images. CLIP model acts as bridge that connects texts and images\n\n\n\n\nFig 8 : CLIP model (Image Credit)\n\n\n\nThe CLIP model consists of two sub-models called encoders: 1. Text Encoder - Converts text to embeddings 2. Image encoder - Converts Image to embedding\nThe embeddings produced by both the encoders needs to be similar if both text and image represent same thing and should be different otherwise\nFor example, lets consider the embedding size as 4. Suppose the text ‚Äúpepper the aussie pup‚Äù when passed to Text encoder produces embeddings (0, 0.5, 0.8, 0.1) then the embeddings generated when image of the Pepper the Aussie Pup is passed to the Image encoder should be similar to that produced by the text encoder ie (0.05, 0.52, 0.78, 0.13). You can learn more about the CLIP model here\nWe use the pretrained Text Encoder of the CLIP model to generate embeddings for our prompts. Let‚Äôs look into the CLIPTextEncoder code\n\nprompt = 'A picture of a puppy'\n\n\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = text_encoder.to(torch_device)\n\nTokenizer expects 77 length vector. So the padding token 49407 is repeated till the length of 77 is reached. The embedding size of the CLIPTextModel is 768.\n\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, trucation=True, return_tensors=\"pt\")\ntext_input['input_ids'][0]\n\ntensor([49406,   320,  1674,   539,   320,  6829, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407])\n\n\nWe can find to which word each number is mapped using tokenizer‚Äôs decoder.\n\nfor t in text_input['input_ids'][0][:8]:print(t, tokenizer.decoder.get(int(t)))\n\ntensor(49406) <|startoftext|>\ntensor(320) a</w>\ntensor(1674) picture</w>\ntensor(539) of</w>\ntensor(320) a</w>\ntensor(6829) puppy</w>\ntensor(49407) <|endoftext|>\ntensor(49407) <|endoftext|>\n\n\nThe numbers from Tokenizer is passed to Text encoder to generate embeddings\n\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\noutput_embeddings\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],\n         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],\n         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],\n       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n\n\n\n\n4.2.3 Classifier- Free Guidance\nClassifier-Free Guidance is a method to increase the adherence of the output to the text provided as input. Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. The default value of guidance is 7.5\nIn order to implement this, along with the embeddings from the prompt we also pass embeddings for an empty string.This is known as Unconditional embedding. This helps the model to go in which ever direction it wants as long as it results in a reasonably-looking image. The prediction are then calculated as follows\nprediction = unconditional_output + guidance(text_output - unconditional_output)\n\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"] * len(prompts), padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\nemb = torch.cat([uncond_embeddings, output_embeddings])"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#unet",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#unet",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "4.3 UNet",
    "text": "4.3 UNet\nThe UNet is the model which produces latent outputs by removing noise.\nUNet model takes three inputs:  1. Noisy latent or Noise - If initial image is provided, noisy latents are produced by adding VAE encoder‚Äôs latents and noise otherwise it takes pure noise as input and produces new image based on the textual description 2. Timestep - Timestep is the number of times the diffusion process is repeated. It takes as input the current timestep 3. Embeddings - Embeddings generated by CLIP for the input prompts and unconditional embeddings\nThe UNet model predicts the noise which is then subtracted from the Noisy latents. This process is repeated for the timesteps mentioned and at the end, the latent outputs produced are passed to VAE‚Äôs decoder to generate High-resolution image\n\n\n\n\nFig 9 : UNet (Image Credit)\n\n\n\nLet‚Äôs look into the code for UNet. Along with UNet we also import a scheduler. Scheduler determines the amount of noise to be added to latent at a given step in diffusion process\n\nfrom diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(51)\n\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\nunet.to(torch_device)\n\nYou can see the amount of noise added in each step below. This is called sigmas. The noise added is highest at first step and is gradually decreased in the following steps until it becomes zero at the last step\n\nprint(scheduler.sigmas)\n\ntensor([14.6146, 12.9679, 11.5454, 10.3129,  9.2414,  8.3072,  7.4900,  6.7731,\n         6.1422,  5.5854,  5.0924,  4.6547,  4.2650,  3.9169,  3.6051,  3.3251,\n         3.0728,  2.8449,  2.6383,  2.4507,  2.2797,  2.1235,  1.9804,  1.8488,\n         1.7276,  1.6156,  1.5118,  1.4154,  1.3255,  1.2415,  1.1629,  1.0890,\n         1.0193,  0.9535,  0.8912,  0.8319,  0.7754,  0.7213,  0.6695,  0.6195,\n         0.5712,  0.5242,  0.4784,  0.4333,  0.3885,  0.3437,  0.2981,  0.2505,\n         0.1989,  0.1379,  0.0292,  0.0000])\n\n\nLet‚Äôs add noise to the latents obtained from VAE‚Äôs encoder earlier and see how U-Net removes noise\n\nnoise = torch.randn_like(encoded)\nencoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[40]]))\nlatents_to_pil(encoded_and_noised)[0]\n\n\n\n\n\n\n\n\n\n#Unconditional prompt\nprompt=[\"\"]\n\n## tokenizing and getting embeddings from clip model\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    text_embeddings=text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n\n#Using U-Net to predict noise\nlatent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()])\nwith torch.no_grad():\n    noise_pred = unet(latent_model_input, 40, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n#Visualize after subtracting noise\nlatents_to_pil(encoded_and_noised -noise_pred)[0]\n\n\n\n\n\n\n\n\nYou can see that the output image is clearer than the provided noisy image."
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#intialize-models",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#intialize-models",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "5.1 Intialize models",
    "text": "5.1 Intialize models\nFirst lets import the model required from diffusers and transformers library. We then create a class SDPipeline and the __init__ method takes model_path as input which in our case is CompVis/stable-diffusion-v1-4\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n\nclass SDPipeline():\n    def __init__(self, model_path):\n        self.model_path=model_path\n        self.vae = AutoencoderKL.from_pretrained(model_path,subfolder=\"vae\" ,torch_dtype=torch.float16).to(torch_device)\n        self.unet=UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16).to(torch_device)\n        self.text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(torch_device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\", torch_dtype=torch.float16)\n        self.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#tokenize-texts",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#tokenize-texts",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "5.2 Tokenize Texts",
    "text": "5.2 Tokenize Texts\nAs explained under Classifier-Free Guidance, we not only pass the embeddings of the prompt but also embeddings for empty string. So let‚Äôs create a function which tokenizes and provides embeddings. We can make use of the dynamic nature of the Python language to add this function dynamically to class SDPipeline\n\ndef encode_text(self, prompts, maxlen=None):\n    if maxlen is None: maxlen = self.tokenizer.model_max_length\n    inp = self.tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return self.text_encoder(inp.input_ids.to(torch_device))[0].half()\n\nSDPipeline.encode_text = encode_text"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#diffusion-loop",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#diffusion-loop",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "5.3 Diffusion Loop",
    "text": "5.3 Diffusion Loop\nThe diffusion loop includes all the steps which we summarized at the starting of this section. We can also add this to class SDPipeline\n\ndef diff_pipeline(self,prompts, guidance=7.5, inference_steps=51, seed=100, callbacks=[], callback_steps=None):\n        #calculate number of prompts\n        batch_size = len(prompts)\n        \n        #generate text_embeddings\n        cond   = self.encode_text(prompts)\n        \n        #generate unconditional embeddings and concat them\n        uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\n        emb    = torch.cat([uncond, cond])\n        if seed : torch.manual_seed(seed)\n        \n        #generate random noise and add noise\n        latents = torch.randn((batch_size, self.unet.in_channels, 512//8, 512//8))\n        \n        #Setting number of steps in scheduler\n        self.scheduler.set_timesteps(inference_steps)\n        \n        #Add noise to latents\n        latents = latents.to(torch_device).half() * self.scheduler.init_noise_sigma\n\n        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n            # scale the latents for particular timestep\n            inp = self.scheduler.scale_model_input(torch.cat([latents]*2), t)\n            \n            #predict noise using UNet\n            with torch.no_grad():u,c = self.unet(inp, t, encoder_hidden_states=emb).sample.chunk(2)\n                \n            #Perform Classifier-Free Guidance    \n            pred = u + guidance*(c-u)\n            \n            #Removing predicted noise from the latents\n            latents = self.scheduler.step(pred, t, latents).prev_sample\n            \n            #run callbacks if any are added\n            if len(callbacks):\n                if i%callback_steps==0:\n                    for callback in callbacks: callback(i, t, latents)\n        \n        #after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated\n        with torch.no_grad():\n            return self.vae.decode(1/0.18125 * latents).sample\n\n\nSDPipeline.__call__ = diff_pipeline"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#adding-callback",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#adding-callback",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "5.4 Adding Callback",
    "text": "5.4 Adding Callback\nLet‚Äôs create a callback as well, which will help to visualize the steps like we did at the starting . We will create a class which stores intermediate steps images. Since they are latents we use VAE to decode and display for us\n\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\",subfolder=\"vae\" ,torch_dtype=torch.float16).to(torch_device)\nclass Latents_Callback:\n    def __init__(self, prompts):\n        self.imgs = [[] for _ in range(len(prompts))]\n    \n    def __call__(self, i, t, latents):\n        latents = 1 / 0.18215 * latents\n        for i in range(latents.shape[0]):\n            callback_imgs = vae.decode(latents).sample[i]\n            callback_imgs = create_image(callback_imgs)\n            self.imgs[i].append(callback_imgs)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#running-our-sdpipeline",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#running-our-sdpipeline",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "5.5 Running our SDPipeline",
    "text": "5.5 Running our SDPipeline\n\nprompts = [\n    'a photograph of an astronaut riding a horse',\n    'an oil painting of an astronaut riding a horse in the style of grant wood',\n    'a painting of dog wearing suit'\n]\n\nInitializing callbacks and SDPipeline objects\n\nlc = Latents_Callback(prompts)\n\n\npipe = SDPipeline(\"CompVis/stable-diffusion-v1-4\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs pass the prompts and the callback object to the SDPipeline object pipe. We can see the images generated for each prompt and can also visualize the steps by displaying images saved in the callback object\n\nimages = pipe(prompts, callbacks=[lc], callback_steps=12)\n\n\n\n\n\nimgs = [create_image(i) for i in images]\nimage_grid(imgs, rows=1, cols = len(imgs))\n\n\n\n\n\nrows, cols = len(prompts), len(lc.imgs[0])\nimgs =[img for i in range(len(prompts)) for img in lc.imgs[i]]\nimage_grid(imgs, rows=rows, cols=cols)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to prajwalsuresh.com",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n      \nHi there, I‚Äôm Prajwal Suresh. I‚Äôm currently exploring Deep learning, Machine learning and data engineering technologies. This is where I write about my learning journey and explorations\nRead more about me here."
  },
  {
    "objectID": "index.html#latest-blogs",
    "href": "index.html#latest-blogs",
    "title": "Welcome to prajwalsuresh.com",
    "section": "Latest Blogs",
    "text": "Latest Blogs\nClick here to check out more blogs.\n\n\n\n\n  \n\n\n\n\nStable diffusion using ü§ó Hugging Face - Variations\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nPrajwal Suresh\n\n\n\n\n\n\n  \n\n\n\n\nStable diffusion using ü§ó Hugging Face - Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nPrajwal Suresh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html",
    "href": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations",
    "section": "",
    "text": "Introduction to variations provided in ü§ó Hugging face - Diffusers library such as Negative prompt, Image to Image and training new concept using Textual Inversion\nThis is the second post on Stable Diffusion. Check out the previous post through link below if you haven‚Äôt read it yet 1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.\nIn previous post, we learnt about Stable Diffusion and its key components VAE, UNet, Scheduler & CLIPTextModel. We also built a pipeline named SDPipeline using these key components and generated images for multiple prompts. In this post, we will further modify our pipeline to work for negative prompts and image to image. At the end, we will learn about Textual Inversion which is useful to train a new concept\nAt first, let‚Äôs download and import the required libraries, authenticate and login to HuggingFace\nBelow is the pipeline which we built in the previous post using the core components."
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html#what-is-negative-prompt",
    "href": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html#what-is-negative-prompt",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations",
    "section": "1.1 What is Negative Prompt?",
    "text": "1.1 What is Negative Prompt?\nNegative prompt is an additional capability which removes anything that the user doesn‚Äôt want to see in generated images. Suppose you don‚Äôt want a particular color or particular object to be part of the generated images, you can mention it in negative_prompt argument in the StableDiffusionPipeline.\n\n\n\n\nFig 1 : Example for Negative Prompt (Image Credit)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-negative-prompt-to-our-sdpipeline",
    "href": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-negative-prompt-to-our-sdpipeline",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations",
    "section": "1.2 Adding negative prompt to our SDPipeline",
    "text": "1.2 Adding negative prompt to our SDPipeline\nLet‚Äôs find how the negative prompt works and how can we integrate it into our pipeline.\nIn the previous post, we used an empty string, generated unconditional embeddings from it and used it along with text embeddings to implement Classifier-Free guidance as below\nuncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\nThe way negative prompt works is that the empty string is replaced by the string containing the objects, styles or colors provided by the user which they don‚Äôt want to be present in the generated images. So to implement negative prompt, we can modify our code to use empty string if negative prompts is not provided else to use negative prompt provided by user\nif not negative_prompts:\n    uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\nelse:\n    uncond = self.encode_text(neg_prompts, cond.shape[1])\nLet‚Äôs make those changes in the diffusion loop of our pipeline and check if our negative prompt implementation is working\n\ndef diff_pipeline_negprompts(self,prompts,negative_prompts=None, guidance=7.5, inference_steps=51, seed=100, callbacks=[], callback_steps=None):\n        #calculate number of prompts\n        batch_size = len(prompts)\n        \n        #generate text_embeddings\n        cond   = self.encode_text(prompts)\n        \n        #generate embedding for negative prompts if available else generate unconditional embeddings\n        if not negative_prompts: uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\n        else : uncond = self.encode_text(negative_prompts, cond.shape[1])\n        emb    = torch.cat([uncond, cond])\n\n        if seed : torch.manual_seed(seed)\n        \n        #generate random noise and add noise\n        latents = torch.randn((batch_size, self.unet.in_channels, 512//8, 512//8))\n        \n        #Setting number of steps in scheduler\n        self.scheduler.set_timesteps(inference_steps)\n        \n        #Add noise to latents\n        latents = latents.to(torch_device).half() * self.scheduler.init_noise_sigma\n\n        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n            # scale the latents for particular timestep\n            inp = self.scheduler.scale_model_input(torch.cat([latents]*2), t)\n            \n            #predict noise using UNet\n            with torch.no_grad():u,c = self.unet(inp, t, encoder_hidden_states=emb).sample.chunk(2)\n                \n            #Perform Classifier-Free Guidance    \n            pred = u + guidance*(c-u)\n            \n            #Removing predicted noise from the latents\n            latents = self.scheduler.step(pred, t, latents).prev_sample\n            \n            #run callbacks if any are added\n            if len(callbacks):\n                if i%callback_steps==0:\n                    for callback in callbacks: callback(i, t, latents)\n        \n        #after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated\n        return self.latents_to_pil(latents)\n\n\nSDPipeline.__call__ = diff_pipeline_negprompts\n\nWe will generate two images with same prompt and for one of the images we will use a negative prompt\n\npipe = SDPipeline(\"CompVis/stable-diffusion-v1-4\")\n\n\ntorch.manual_seed(9000)\nimg1 = pipe(prompts=[\"a photo of a forest with trees and bushes\"])[0]\nimg2 = pipe(prompts=[\"a photo of a forest with trees and bushes\"], negative_prompts=[\"green\"])[0]\n\n\n\n\n\n\n\n\nimgs=[img1, img2]\nimage_grid(imgs, rows=1, cols=2)\n\n\n\n\n\n\n\n\n\n\nFig 2: Negative Prompt Results for prompt ‚Äúa photo of a forest with trees and bushes‚Äù and negative prompt=‚Äúgreen‚Äù‚Äú\n\n\nAs you can see in the images above, the image on the left generated image of forest. When given negative prompt as color green, the model generated same image without green color in it (on the right)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-image-to-image-capability-to-sdpipeline",
    "href": "posts/2022-11-22-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-image-to-image-capability-to-sdpipeline",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations",
    "section": "2.1 Adding Image to Image capability to SDPipeline",
    "text": "2.1 Adding Image to Image capability to SDPipeline\nIn the previous post, we learnt that we can also pass an image to our model and it generates new images using the provided image as the initial image along with the prompt. There are two scenarios 1) with image (Image to Image) 2) without image (Prompt to Image)\nFor scenario 1 which is Image to Image pipeline, we expect 2 parameters: 1. init_image - This is used as the initial image 2. strength - The value of this parameter is between 0 and 1\nFirst, the provided init_image is passed to the VAE‚Äôs encoder to generate latents. We then need to add noise to the latents. Since we are using initial image we can reduce the number of inference steps. The strength parameter is used for this and higher its value higher the inference_steps. If we want our generated image to be similar to the init_image, the number of steps should be less so the strength parameter value should also be less. The amount of noise to be added is calculated by multiplying the strength and inference_steps\nFor example, suppose we want our generated image to be similar to the initial image so we assign the strength parameter to 0.4 and inference_steps is equal to 50 . Then the noise added is for the step 30 (50 -( 0.4 x 50) == 30). The inference_steps is reduced to 20 steps (50x0.4)\nFor scenario 2 which is prompt to image, we need to create a random noise. The amount of noise for the first step is multiplied with scheduler.init_noise_sigma . The inference_steps remains same.\nLet‚Äôs create a function which can handle both the scenarios. The function should return latents and timesteps.  1. In case of Prompt to Image, random noise is created and multiplied with init_noise is returned. The timesteps is returned without modifying it 2. In case of Image to Image, latents are created from init_image and amount of noise to be added is calculated using strength and returned. The timesteps is also reduced based on the strength and returned\n\ndef create_latents(self, batch_size, init_image, strength, inference_steps):\n    \n    if init_image is None:\n        latents = torch.randn((batch_size, self.unet.in_channels, 512//8, 512//8))\n        latents = latents.to(torch_device).half() * self.scheduler.init_noise_sigma\n        return latents, self.scheduler.timesteps.to(torch_device)\n    else:\n        img_latents = self.pil_to_latents(init_image)\n        noise = torch.randn(img_latents.shape, generator=None, device=torch_device, dtype=img_latents.dtype)\n\n        init_timestep = int(inference_steps * strength)\n        timesteps = self.scheduler.timesteps[-init_timestep]\n        timesteps = torch.tensor([timesteps], device=torch_device)\n\n        init_latents = self.scheduler.add_noise(img_latents, noise, timesteps)\n\n        t_start = max(inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:].to(torch_device)\n        \n        return init_latents, timesteps\n\nSDPipeline.create_latents = create_latents\n\nLet‚Äôs replace the diffusion loop code to get latents and timesteps from the above function\n\ndef diff_pipeline_img2img(self,prompts,init_image=None, strength=0.8, negative_prompts=None, guidance=7.5, inference_steps=51, seed=100, callbacks=[], callback_steps=None):\n        #calculate number of prompts\n        batch_size = len(prompts)\n        \n        #generate text_embeddings\n        cond   = self.encode_text(prompts)\n        \n        #generate embedding for negative prompts if available else generate unconditional embeddings\n        if not negative_prompts: uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\n        else : uncond = self.encode_text(negative_prompts, cond.shape[1])\n        emb    = torch.cat([uncond, cond])\n        \n        if seed : torch.manual_seed(seed)\n\n        #Setting number of steps in scheduler\n        self.scheduler.set_timesteps(inference_steps)\n        \n        #generate random noise and add noise\n        latents, timesteps = self.create_latents(batch_size, init_image, strength, inference_steps)\n        \n        \n        for i, t in enumerate(tqdm(timesteps)):\n            # scale the latents for particular timestep\n            inp = self.scheduler.scale_model_input(torch.cat([latents]*2), t)\n            \n            #predict noise using UNet\n            with torch.no_grad():u,c = self.unet(inp, t, encoder_hidden_states=emb).sample.chunk(2)\n                \n            #Perform Classifier-Free Guidance    \n            pred = u + guidance*(c-u)\n            \n            #Removing predicted noise from the latents\n            latents = self.scheduler.step(pred, t, latents).prev_sample\n            \n            #run callbacks if any are added\n            if len(callbacks):\n                if i%callback_steps==0:\n                    for callback in callbacks: callback(i, t, latents)\n        \n        #after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated\n        return self.latents_to_pil(latents)\n\nSDPipeline.__call__ = diff_pipeline_img2img\n\nNow let‚Äôs download an image pass it to our Image To Image pipeline\n\nfrom fastdownload import FastDownload\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\nimage = Image.open(p).convert('RGB').resize((512,512))\nimage\n\n\n\n\n\n\n    \n      \n      106.50% [49152/46150 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\nFig 3: Initial Image for Image To Image Pipeline\n\n\nWe can also add our Callback function to visualize how the image is generated\n\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\",subfolder=\"vae\" ,torch_dtype=torch.float16).to(torch_device)\nclass Latents_Callback:\n    def __init__(self, prompts):\n        self.imgs = [[] for _ in range(len(prompts))]\n    \n    def __call__(self, i, t, latents):\n        latents = 1 / 0.18215 * latents\n        for i in range(latents.shape[0]):\n            callback_imgs = vae.decode(latents).sample[i]\n            callback_imgs = create_image(callback_imgs)\n            self.imgs[i].append(callback_imgs)\n\n\n\n\n\n\n\nWe are using a random sketch of wolf howling at moon as initial image. So we can use higher value of strength to generate good image\n\nprompt = [\"Wolf howling at the moon, photorealistic 4K\"]\npipei2i = SDPipeline(\"CompVis/stable-diffusion-v1-4\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlc = Latents_Callback(prompt)\nimg = pipei2i(prompt,init_image=image, callbacks=[lc], callback_steps=8,inference_steps=50)\nimg[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 4: Generated Image for Image To Image pipeline\n\n\n\nrows, cols = len(prompt), len(lc.imgs[0])\nimgs =[img for i in range(len(prompt)) for img in lc.imgs[i]]\nimage_grid(imgs, rows=rows, cols=cols)"
  }
]