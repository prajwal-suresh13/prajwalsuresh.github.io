[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Prajwal Suresh",
    "section": "",
    "text": "Hi there, I’m Prajwal Suresh. I’m currently exploring Data Engineering, Deep learning, machine learning technologies. This is where I write about my learning journey and explorations\n\nExperienceSkillsEducation\n\n\n\n\nAzure Data Engineer\nNetworth Data Products Private Limited, Bengaluru, India\n\n\nJanuary 2023 - Present\n\n\n\nServed as an Data Engineer, implementing modern data solutions for data extraction, transformation and loading data from source systems into the Azure Data Lake using Azure cloud services.\nOrchestrated end-to-end data pipelines using Azure Data Factory, integrating Azure Data Lake for storage, Azure Databricks for data transformation, and Azure Synapse for serving data to dashboards and downstream teams, reducing manual intervention by 60%\nMigrated legacy data pipelines to Medallion architecture, improving data quality, scalability, processing time and consistency, reducing data ingestion time by 30% and operational costs by 50%\nImplemented incremental data processing using SQL and PySpark on Azure Databricks, reducing storage and compute costs by 20%\nUtilized Delta Lake’s schema evolution capabilities to adapt to changing data, ensuring data pipelines remained scalable and flexible and reducing schema management overhead\nDeveloped generic pipeline templates to get data from various types of source (API, SQL, Excel, ADLS, CSV, Parquet)\nCollaborated with business stakeholders regularly to gather and refine data requirements, ensuring 100% alignment between data outputs and business objectives.\nPartnered with Power BI developers to translate business requirement into actionable Gold layer datasets, enabling the creation of 20+ interactive dashboards.\n\n\n\nAssistant System Engineer\nTata Consultancy Services Private Limited, Bengaluru, India\n\n\nAugust 2021 - January 2023\n\n\n\nDeveloped ETL pipelines in Azure Data Factory, improving data availability by 40%\nInvolved in migration of on-premises databases to Azure SQL Database, ensuring data integrity and achieving 20% improvement in query performance.\nAutomated data pipeline monitoring and error handling, achieving 99% pipeline success rate\nDeveloped PySpark and SQL scripts in Azure Databricks to transform data\n\n\n\n\n\nProgramming & Scripting\nPython, C++, SQL\n\n\n\n\nCloud\nMicrosoft Azure Cloud Platform, Azure Data Factory, Azure Databricks, ADLS Gen 2, Azure Synapse, Azure Logic app, Azure Key Vault\n\n\n\n\nMachine Learning\nPyTorch, fastai, Scikit-Learn, Pandas, Numpy, MatplotLib, OpenCV, HuggingFace\n\n\n\n\nWeb Development\nDjango, Flask, HTML, CSS\n\n\n\n\n\n\nSapthagiri College of Engineering, Bengaluru, India\nBachelor of Engineering in Computer Science & Engineering | GPA: 8.55/10\n\n\n2017-2021\n\n\n\n\nMES PU College of Arts, Commerce & Science, Bengaluru, India\nPre-University Education | Percentage: 87.16%\n\n\n2017\n\n\n\n\nSt Mary’s High School, Bengaluru, India\nTenth Standard | Percentage: 97.6%\n\n\n2015"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStable diffusion - Dreambooth\n\n\n8 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html",
    "href": "posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html",
    "title": "Stable diffusion - Dreambooth",
    "section": "",
    "text": "Training a new concept to Stable Diffusion model using Dreambooth, generating images of the new concept using the trained model, and saving the pipeline to HuggingFace Hub\n\n\n\n  \n\n\nIn this post, we will be covering Dreambooth which is similar to Textual Inversion, train a new concept using it and push our model to HuggingFace Hub, from where anyone can use our fine-tuned model. This post is based on the HuggingFace Notebook found here. So let’s get started\n\n1 What is Dreambooth?\nStable Diffusion results are amazing. But what if you want to generate images of subjects that are personal to you. Can it render images of your cute pet dog having fun at the beach? What about you being the main character of superhero movie saving the World?. Yes, you absolutely can personalize the Stable Diffusion model using Dreambooth\n\n\n\n\nCredits\n\n\n\n\n\n\nDreambooth is an approach to teach a new concept by fine-tuning a pretrained text-to-image model such that it binds a unique token with the new concept of images. If you are aware of fine-tuning vision models, you might be thinking at this point that it will take hundreds of images and hours of training to fine-tune Stable Diffusion model, but to our surprise only a minimum of 5 images of the new subject is required!\nThe image on the right above is generated from the model which I fine-tuned for around 30 minutes on Tesla T4 GPU and 16GB RAM with just 6 images of my pet dog. Amazing isn’t it!\n\n\n\n\n \n\n\nFig 1: Dreambooth approach (Image Credit)\n\n\n\nDreambooth approach of fine-tuning the text-to image diffusion model is as follows: 1. We pair the input images of our interested subject and text prompt containing a unique identifier and fine tune the model. (Example - for unique identifier you can use a token which is less frequently used such as sks.So the prompt will be “A photo of sks dog”) 2. We also pass the class of the subject as a prompt while fine-tuning the model and apply class-specific prior preservation loss (Example - “A photo of a dog”) \nPrior Preservation doesn’t seem to make huge difference except when trained on faces. So prior preservation is not used while training in this post. Checkout this amazing detailed article in which they have experimented training dreambooth with different settings. You can follow the HuggingFace notebook in which they have implemented prior preservation\nLet’s install the required libraries and load the CompVis/stable-diffusion-v1-4 models in the next section\n\n\n2 Getting started\nAt first, we will be connecting to Google Drive from Google Colab. This allows us to read images saved in Drive for training model and also save the trained model to the Drive\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nWe will be downloading the required libraries as we did in previous posts. We will be installing and importing an additional library named bitsandbytes. It provides an 8-bit optimizer which saves memory and increases speed while training. For more details checkout this repo\nWe will also be using accelerate library which helps in distributed training, gradient accumulation and many more.\n\n!pip install -Uq accelerate transformers diffusers ftfy\n\n\n!pip install -qq bitsandbytes\n\nWe can authenticate our notebook to Huggingface services using the commands below. Since we will be pushing our model to HuggingFace Hub later, we will need write permissions. So you need to create an Access Token with write permission and use it to authenticate\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful\n\n\n\n\nCode\nfrom PIL import Image\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms as tfms\n\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\n\nfrom transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n\nimport bitsandbytes as bnb\n\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols*w, rows*h))\n    for i, img in enumerate(imgs):grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nNext, we will initialize the core components of the Stable Diffusion model with the weights of CompVis/stable-diffusion-v1-4 repo\n\nmodel_path=\"CompVis/stable-diffusion-v1-4\"\n\n\ntext_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\")\nvae          = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\nunet         = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\ntokenizer    = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 Data\nWe need to provide the images of our subject and the prompt with the unique token to fine-tune model. So let’s create a dataset which takes images and prompts as input. The dataset needs to convert the images into tensor which is done using PyTorch’s transforms. It also should return input_ids for the prompt which is provided by the Tokenizer. Both pixel values and prompt ids are returned as dictionary\n\nclass DreamBoothDataset(Dataset):\n    def __init__(self, images_path, prompts , tokenizer, size =512, center_crop=False):\n        self.size, self.center_crop = size, center_crop\n        self.tokenizer = tokenizer\n        self.images_path = Path(images_path)\n        if not self.images_path.exists():\n            raise ValueError(\"Images path doesn't exist\")\n\n        self.images = list(Path(images_path).iterdir())\n        self.num_of_images = len(self.images)\n        self.prompts = prompts \n\n        self.image_transforms = tfms.Compose([\n            tfms.Resize(size, interpolation = tfms.InterpolationMode.BILINEAR),\n            tfms.CenterCrop(size) if center_crop else tfms.RandomCrop(size),\n            tfms.ToTensor(),\n            tfms.Normalize([0.5],[0.5])\n        ])  \n        \n            \n    def __len__(self):\n        return self.num_of_images    \n\n    def __getitem__(self, index):\n        example = {}\n        image = Image.open(self.images[index%self.num_of_images]).convert(\"RGB\")   \n        example[\"images\"] = self.image_transforms(image)  \n        example[\"prompt_ids\"] = self.tokenizer(self.prompts, padding=\"do_not_pad\", truncation=True, \n                                               max_length=self.tokenizer.model_max_length).input_ids \n        \n        return example          \n\nDataloader helps us to sample minibatches from our dataset. Since our dataset returns dictionary, we have to tell the Dataloader how it can combine and return minibatches of pixelvalues and prompt_ids.\n\ndef collate_fn(examples):\n    pixels    = [example[\"images\"] for example in examples]\n    input_ids = [example[\"prompt_ids\"] for example in examples]\n    \n    pixels = torch.stack(pixels).to(memory_format=torch.contiguous_format).float()\n    input_ids = tokenizer.pad({\"input_ids\":input_ids}, padding=True, return_tensors=\"pt\").input_ids\n\n    batch = {\n        \"input_ids\":input_ids,\n        \"pixel_values\":pixels\n    }\n\n    return batch\n\n\ntrain_dataset = DreamBoothDataset(\n        images_path = \"/content/gdrive/My Drive/pintu\",\n        prompts = \"a photo of a sks dog\",\n        tokenizer = tokenizer,\n        size=512,\n        center_crop=True\n    )\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= 1, shuffle=True, collate_fn=collate_fn)\n\nNow let’s view the our data. Below are the 6 images of my pet Pintu which are used for training\n\nimages =[Image.open(train_dataset.images[i]).resize((256,256)).convert(\"RGB\") for i in range(len(train_dataset))]\nimage_grid(images, rows=1, cols=len(train_dataset))\n\n\n\n\n\n\nFig 2: Images used to train Dreambooth model\n\n\n\n\n4 Training\n\nlearning_rate=5e-06\nmax_train_steps =500\ntrain_batch_size=1\ngradient_accumulation_steps=2\nuse_8bit_adam=True\noutput_dir = \"/content/gdrive/My Drive/pintu_dreambooth\"\n\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nThe function dreambooth_trainer fine-tunes the Stable Diffusion model. We fine-tune the model for 500 steps at a learning rate of 5e-06. The pixel_values of the images from the dataloader are passed to VAE’s encoder to generate latents. The prompt_ids are passed to the text_encoder to generate embeddings. Random noise is added at each step and the loss is calculated. The loss is then backpropogated and the weights of the model are updated. After the training is completed, all the components of the model are saved to the output_dir path which in our case gets saved to Google Drive\n\ndef dreambooth_trainer(data, text_encoder, vae, unet):\n\n    accelerator=Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n    set_seed(90000)\n    unet.enable_gradient_checkpointing()\n\n    if use_8bit_adam:\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    optimizer = optimizer_class(unet.parameters(), lr=learning_rate)\n\n    noise_scheduler = DDPMScheduler(beta_start= 0.00085, beta_end = 0.012, beta_schedule=\"scaled_linear\",\n                                    num_train_timesteps=1000)\n    \n    \n    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, data)\n\n    text_encoder.to(torch_device)\n    vae.to(torch_device)\n\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader)/gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps/num_update_steps_per_epoch)\n    total_batch_size = train_batch_size *accelerator.num_processes * gradient_accumulation_steps\n\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        unet.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(unet):\n                with torch.no_grad():\n                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n                    latents*=0.18215\n                \n                \n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,(bsz,), device=latents.device).long()\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                with torch.no_grad():\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1,2,3]).mean()\n\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step+=1\n\n\n            logs = {\"loss\":loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step>=max_train_steps:break\n\n        accelerator.wait_for_everyone()\n\n    if accelerator.is_main_process:\n        pipeline=StableDiffusionPipeline(\n            text_encoder=text_encoder, vae=vae, unet=unet, \n            tokenizer=tokenizer,\n            scheduler = PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n            ),\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"))\n        pipeline.save_pretrained(output_dir)\n\n\nimport accelerate\naccelerate.notebook_launcher(dreambooth_trainer, args=(train_dataloader,text_encoder, vae, unet), num_processes = 1)\n\n\n\n5 Results\nNow we can load the model from the saved directory and use it for inference. To get high quality images, you can use prompts from Lexica and replace with your unique identifier.\n\noutput_dir = \"/content/gdrive/My Drive/pintu_dreambooth\"\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npipe = StableDiffusionPipeline.from_pretrained(\n        output_dir,\n        torch_dtype=torch.float16,\n    ).to(torch_device)\n\nBelow are some of the astonishing results generated by our fine-tuned model!\n\n\nCode\nprompt = \"a beautiful portrait of a cute sks dog, beautiful detailed eyes, golden hour, standing on a beach, outdoors, professional award winning portrait photography, Zeiss 150mm f/ 2.8 Hasselblad\" \n\nnum_samples = 3 \nnum_rows = 3 \n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 3: A beautiful portrait of a cute sks dog, beautiful detailed eyes, golden hour, standing on a beach, outdoors, professional award winning portrait photography, Zeiss 150mm f/ 2.8 Hasselblad”\n\n\n\n\nCode\nprompt = \"portrait of a sks dog wearing a hat, realism, realistic, photorealism, f 3. 5, photography, octane render, trending on artstation, artstationhd, artstationhq, unreal engine, cinema 4 d, 8 k, detailed fur \" \n\nnum_samples = 3 \nnum_rows = 3 \n\n# def dummy_checker(images, **kwargs): return images, False\n# pipe.safety_checker = dummy_checker\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 4: Portrait of a sks dog wearing a hat, realism, realistic, photorealism, f 3. 5, photography, octane render, trending on artstation, artstationhd, artstationhq, unreal engine, cinema 4 d, 8 k, detailed fur ”\n\n\n\n\nCode\nprompt = \"sks dog as a realistic fantasy knight, closeup portrait art by donato giancola and greg rutkowski, digital art, trending on artstation, symmetry!! \" \n\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 5: sks dog as a realistic fantasy knight, closeup portrait art by donato giancola and greg rutkowski, digital art, trending on artstation, symmetry!!\n\n\n\n\nCode\nprompt = \"white sks dog, with reflection in the puddle, foggy old forest, very detailed, 4 k, professional photography \" \n\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 6 : White sks dog, with reflection in the puddle, foggy old forest, very detailed, 4 k, professional photography\n\n\n\n\nCode\nprompt = \"framed renaissance portrait of a sks dog\" \n\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 7: Framed renaissance portrait of a sks dog\n\n\nIn the below results, the prompt is truncated since CLIP can only handle prompts with maximum of 77 tokens\n\n\nCode\nprompt = \"a sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, partial anatomy, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\"\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\n\n\n\n\n\nFig 8: A sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, partial anatomy, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\n\n\n\n\nCode\nprompt = \"a sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\"\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\n\n\n\n\n\nFig 9: A sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\n\n\n\n\n6 Pushing your model to hub\nHuggingFace Hub is a platform with over 60K models, 6K datasets and 6K demo apps(spaces) all open source and publicly available. The best part is that sharing and using any public model on the Hub is completely free of cost. Let’s next see how we can move our model from Google Drive to HuggingFace Hub. In order to push model to hub, make sure that you have used Access token with write permission while authenticating to HuggingFace\nThe get_full_repo_name returns the repository name for given model in user’s namespace ({username/model_name}).\n\nfrom huggingface_hub import get_full_repo_name\nmodel_name = \"pintu_dreambooth\"\n\n\nhub_model = get_full_repo_name(model_name)\nhub_model\n\n'prajwal13/pintu_dreambooth'\n\n\n\noutput_dir = \"/content/gdrive/My Drive/pintu_dreambooth\"\nimages_folder = \"/content/gdrive/My Drive/pintu\"\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nWe can then use create_repo to create an empty directory in HuggingFace hub and push our model and the images used for training to the empty repo.\n\nfrom huggingface_hub import HfApi, create_repo\n\ncreate_repo(hub_model)\napi = HfApi()\napi.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=hub_model)\n\n'https://huggingface.co/prajwal13/pintu_dreambooth/tree/main/'\n\n\n\napi.upload_folder(folder_path=images_folder, path_in_repo=\"concept_images\",repo_id=hub_model)\n\n'https://huggingface.co/prajwal13/pintu_dreambooth/tree/main/concept_images'\n\n\nThe last thing to do is create a model card so that our model can easily be found on the Hub. We will also add the images used to train in the model card\n\nfrom huggingface_hub import ModelCard\nimport os\n\nimages_upload = os.listdir(images_folder)\nimage_string = \"\"\n\nfor i, image in enumerate(images_upload):\n    image_string = f'''{image_string}![sks {i}](https://huggingface.co/{hub_model}/resolve/main/concept_images/{image})'''\n\nimage_string\n\n\ncontent = f\"\"\"\n---\nlicense: mit\ntags:\n- pytorch\n- diffusers\n- dreambooth\n---\n\n# Model Card for Dreambooth model trained on My pet Pintu's images\n\nThis model is a diffusion model for unconditional image generation of my cute pet dog Pintu trained using Dreambooth concept. The token to use is sks .\n\n## Usage\n\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from_pretrained({hub_model})\nimage = pipeline('a photo of sks dog').images[0]\nimage\n\nThese are the images on which the dreambooth model is trained on\n\n{image_string}\n\n\"\"\"\n\n\ncard = ModelCard(content)\ncard.push_to_hub(hub_model)\n\n'https://huggingface.co/prajwal13/pintu_dreambooth/blob/main/README.md'\n\n\nNow we can directly load the model from HuggingFace Hub by passing the repo id to the pipeline\n\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(hub_model).to(torch_device)\nimage = pipeline(\"a photo of sks dog wearing glasses, and standing near beach\").images[0]\nimage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 Conclusion\nIn this post, we learnt about Dreambooth and how we can personalize the Stable Diffusion model. We also learnt how to push our model to HuggingFace hub.\nI hope you enjoyed reading it. If there is any feedback, feel free to reach out on LinkedIn or on mail\n\n\n8 References\n\nFastAI course\nDreambooth paper\nHuggingFace Dreambooth notebook"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to prajwal-suresh13.github.io",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n      \nHi there, I’m Prajwal Suresh. I’m currently exploring Deep learning, Machine learning and data engineering technologies. This is where I write about my learning journey and explorations\nRead more about me here."
  },
  {
    "objectID": "index.html#latest-blogs",
    "href": "index.html#latest-blogs",
    "title": "Welcome to prajwal-suresh13.github.io",
    "section": "Latest Blogs",
    "text": "Latest Blogs\nClick here to check out more blogs.\n\n\n\n\n  \n\n\n\n\nStable diffusion - Dreambooth\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nPrajwal Suresh\n\n\n\n\n\n\nNo matching items"
  }
]