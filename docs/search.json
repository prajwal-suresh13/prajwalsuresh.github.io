[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Prajwal Suresh",
    "section": "",
    "text": "Hi there, I‚Äôm Prajwal Suresh. I‚Äôm currently exploring Deep learning, machine learning and data engineering technologies. This is where I write about my learning journey and explorations\n\n\nExperienceSkillsEducation\n\n\n\n\nBig Data Engineer\nTata Consultancy Services Private Limited, Bengaluru, India\n\n\nAugust 2021 - Present\n\n\n\nManaging 275 nodes Hadoop Cloudera cluster, handling 10PiB of data and managing 250+ users effectively\nAutomating tasks using Ansible and project space creation for users\n\n\n\n\n\nProgramming\nPython, C++, Java\n\n\n\n\nMachine Learning\nPyTorch, fastai, Scikit-Learn, Pandas, Numpy, MatplotLib, OpenCV, HuggingFace\n\n\n\n\nBig Data\nHadoop, Spark, Docker, Airflow, Cloudera, Cloud, Linux, SQL, Ansible\n\n\n\n\nWeb Development\nDjango, Flask, PHP, HTML, CSS, JavaScript\n\n\n\n\n\n\nSapthagiri College of Engineering, Bengaluru, India\nBachelor of Engineering in Computer Science & Engineering | GPA: 8.55/10\n\n\n2017-2021\n\n\n\n\nMES PU College of Arts, Commerce & Science, Bengaluru, India\nPre-University Education | Percentage: 87.16%\n\n\n2017\n\n\n\n\nSt Mary‚Äôs High School, Bengaluru, India\nTenth Standard | Percentage: 97.6%\n\n\n2015"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStable diffusion - Dreambooth\n\n\n8 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion - Negative Prompt, Image to Image & Textual Inversion\n\n\n7 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion - Introduction\n\n\n13 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html",
    "title": "Stable diffusion - Introduction",
    "section": "",
    "text": "Introduction to using ü§ó Hugging face - Diffusers library for Stable Diffusion, looking into the core components and implementing StableDiffusionPipeline using it\nIn this post I will give an introduction on using ü§ó Diffusers library to generate images using texts as input. This post is based on the knowledge acquired while doing the ‚ÄòFrom Deep learning foundations to Stable Diffusion‚Äô course by FastAI . The first few lessons of the FastAI course are publicly available here. Until the rest of the lessons are available, you can also checkout the first part of the course which is one of the best Deep Learning courses\nAt first, let‚Äôs install the diffusers library"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#creating-huggingface-account-and-accepting-the-model-license",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#creating-huggingface-account-and-accepting-the-model-license",
    "title": "Stable diffusion - Introduction",
    "section": "2.1 Creating HuggingFace account and accepting the model license",
    "text": "2.1 Creating HuggingFace account and accepting the model license\nAt first, you will have to create a HuggingFace account by going to this link. HuggingFace provides many pretrained models for Stable Diffusion. In order to use the model, you have to accept the model license. The pretrained model we are using is CompVis/stable-diffusion-v1-4 .Click on this link to accept the model license"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#token-generation",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#token-generation",
    "title": "Stable diffusion - Introduction",
    "section": "2.2 Token generation",
    "text": "2.2 Token generation\nUser Access Tokens are the preferred way by HuggingFace to authenticate an application or notebook to Huggingface services. You can learn more about it in their documentation\nTo create an access token, click on top right icon, go to Settings. Select Access Tokens and New Token. Give the token a name and the role. For following along with this post read role is sufficient. Click on Generate a Token. You can find the newly created token under User Access Token. Copy the generated token. On running notebook_login() command, you will be prompted to enter the token which will provide access to HuggingFace services through Jupyter Notebook\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#stablediffusionpipeline",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#stablediffusionpipeline",
    "title": "Stable diffusion - Introduction",
    "section": "2.3 StableDiffusionPipeline",
    "text": "2.3 StableDiffusionPipeline\nNext we will import the required libraries. We will set a variable torch_device to ‚Äúcuda‚Äù if GPU is available else to ‚Äúcpu‚Äù. The image_grid function is useful to visualize the results obtained from the StableDiffusionPipeline.\n\n\nCode\nimport torch\nfrom torch import autocast\nfrom torchvision import transforms as tfms\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom transformers import logging\nlogging.set_verbosity_error() # Suppress unnecessary warning from CLIPTextModel\n\nfrom diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\n\n#checks if gpu is available if not cpu is used\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols*w, rows*h))\n    for i, img in enumerate(imgs):grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nNext step is to import the StableDiffusionPipeline from diffusers library & download the model weights. The model weights are stored in Huggingface repo. We have to provide the repo path to from_pretrained. In our case the repo path is CompVis/stable-diffusion-v1-4 model. We then move it to GPU if available\n\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(torch_device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let‚Äôs pass a textual prompt to the pretrained model to generate image\n\ntorch.manual_seed(9000)\nprompt = \"a photo of a puppy wearing suit\"\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 2: An example of image generated by the StableDiffusion pipeline\n\n\n\n\nYou can also pass multiple textual prompts and the pipeline will generate images for each prompt\n\nprompts =['a photograph of a dog wearing hat and glasses',\n          'a photograph of an astronaut riding horse ',\n          'an oil painting of dog']\n\n\ntorch.manual_seed(9000)\nimages = pipe(prompts).images\nlen(images)\n\n\n\n\n3\n\n\n\nimage_grid(images, rows=1, cols=3)\n\n\n\n\n\n\nFig 3: Images generated by pipeline for multiple prompts(A photograph of a dog wearing hat and glasses, A photograph of an astronaut riding horse, An oil painting of dog‚Äù)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#autoencoder",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#autoencoder",
    "title": "Stable diffusion - Introduction",
    "section": "4.1 Autoencoder",
    "text": "4.1 Autoencoder\n\n\n\n\nFig 5:Autoencoder (Image Credit)\n\n\n\nAutoencoders are models which take images as input and produce same images as output. So you might think how is this model even useful if it is producing same output as input. It is useful because autoencoder is made up of two parts and after training autoencoder these two parts can be used independently. The two parts are 1. Encoder - Takes image as input and converts it to low dimensional latent representation ie compressed data 2. Decoder - Takes compressed data and converts it back into an image\nThese models can be used for various applications. For example, it can be used as a compression application, where the encoder squishes the image into lower dimensions which uses less memory for storing and when the image is required we can recreate by passing the squished data to decoder. It is to be noted that the compression and decompression by the encoder-decoder is not lossless.\nThe Latent diffusion models as mentioned in the previous section works on the latent space. This latent space is produced by Encoder of Variational Autoencoder(VAE) and once we have our desired latent outputs produced by the model through diffusion process, we can convert them back to high-resolution images using VAE‚Äôs decoder\nIn case of Image to Image StableDiffusion Pipeline, the input image is passed through the VAE‚Äôs encoder and the latent inputs obtained combined with random noise are used to perform diffusion. The latent outputs produced by the diffusion process is then converted back to high resolution image using VAE‚Äôs decoder\nIn case of Text to Image StableDiffusion pipeline, only random noise is used to perform diffusion and the latent outputs produced is converted back high-resolution image using VAE‚Äôs decoder\nNow let‚Äôs look into the VAE code. HuggingFace Diffusers library already provides us with a pretrained Autoencoder to use which can be imported as below\n\nfrom diffusers import AutoencoderKL\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\nvae.to(torch_device)\n\n\n\nCode\ndef pil_to_latents(input_img):\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_img).unsqueeze(0).to(torch_device)*2-1)\n    return 0.18215 * latent.latent_dist.sample()\n\ndef create_image(t):\n    image = (t/2 +0.5).clamp(0,1).detach().cpu().permute(1,2,0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\ndef latents_to_pil(latents):\n    latents = (1/0.18215) * latents\n    with torch.no_grad():\n        images = vae.decode(latents).sample\n    pil_images =[create_image(image) for image in images]\n    return pil_images\n\n\nLet‚Äôs download an image from internet, compress the image using Encoder and view the latent representation. We will use pil_to_latents helper function to do it\n\n!curl --output macaw.jpg 'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'\ninput_image = Image.open('macaw.jpg').resize((512, 512))\ninput_image\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 62145  100 62145    0     0   240k      0 --:--:-- --:--:-- --:--:--  240k\n\n\n\n\n\n\n\n\n\n\nencoded = pil_to_latents(input_image)\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(encoded[0][c].cpu(), cmap='Greys')\n\n\n\n\n\n\nFig 6: Latent Representation of the Input image\n\n\n\nencoded.shape,tfms.ToTensor()(input_image).unsqueeze(0).shape\n\n(torch.Size([1, 4, 64, 64]), torch.Size([1, 3, 512, 512]))\n\n\nAs you can see the VAE has compressed 3x512x512 image into 4x64x64 image. That is a compression ratio of 48x. These latent representation has captured information about the original image and on passing them to the VAE‚Äôs decoder provide the original image. For this we use the latents_to_pil function.\n\ndecoded = latents_to_pil(encoded)[0]\ndecoded\n\n\n\n\n\n\n\n\nSo we understood how Stable diffusion can create images from random noise using diffusion process. But how is it able to generate images representing the text? Let‚Äôs find out in the next section."
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#text-encoder",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#text-encoder",
    "title": "Stable diffusion - Introduction",
    "section": "4.2 Text Encoder",
    "text": "4.2 Text Encoder\nDeep Learning models only understand numbers and not text. So inorder to convert the text data into numbers which our models can work with Tokenizers and Text Encoders are used\n\n\n\n\nFig 7 : Tokenizer and Text Encoder (Image Credit)\n\n\n\nThis process of converting text to numbers is done in two steps: 1. Tokenizer - The prompt provided is broken down into words and then it uses a lookup table to convert them into a number. As you can see in the Fig 7 , the input sentences are converted to numbers using the lookup table 2. Text Encoder - This converts numbers into a high dimensional vectors which captures the meaning of the words. These are called embeddings. In Fig 7, the Text Encoder is represented as Embedding layer.\n\n4.2.1 What is embedding?\nEmbedding is a high dimensional vector that allows words with similar meaning to have the same representation. It can approximate the meaning of the word. An embedding with 50 values holds the capability of representing 50 unique features. This is known as embedding size. In the Fig 7, the embedding size used to represent each word is 4\n\n\n4.2.2 How CLIP model works\nThe embedding from the text encoder in the Fig 7 represents the meaning of each word in the given prompt. But we also need the same embeddings to represent an image which is described in the prompt. We need a model that connects text and images. CLIP model acts as bridge that connects texts and images\n\n\n\n\nFig 8 : CLIP model (Image Credit)\n\n\n\nThe CLIP model consists of two sub-models called encoders: 1. Text Encoder - Converts text to embeddings 2. Image encoder - Converts Image to embedding\nThe embeddings produced by both the encoders needs to be similar if both text and image represent same thing and should be different otherwise\nFor example, lets consider the embedding size as 4. Suppose the text ‚Äúpepper the aussie pup‚Äù when passed to Text encoder produces embeddings (0, 0.5, 0.8, 0.1) then the embeddings generated when image of the Pepper the Aussie Pup is passed to the Image encoder should be similar to that produced by the text encoder ie (0.05, 0.52, 0.78, 0.13). You can learn more about the CLIP model here\nWe use the pretrained Text Encoder of the CLIP model to generate embeddings for our prompts. Let‚Äôs look into the CLIPTextEncoder code\n\nprompt = 'A picture of a puppy'\n\n\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = text_encoder.to(torch_device)\n\nTokenizer expects 77 length vector. So the padding token 49407 is repeated till the length of 77 is reached. The embedding size of the CLIPTextModel is 768.\n\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, trucation=True, return_tensors=\"pt\")\ntext_input['input_ids'][0]\n\ntensor([49406,   320,  1674,   539,   320,  6829, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407])\n\n\nWe can find to which word each number is mapped using tokenizer‚Äôs decoder.\n\nfor t in text_input['input_ids'][0][:8]:print(t, tokenizer.decoder.get(int(t)))\n\ntensor(49406) <|startoftext|>\ntensor(320) a</w>\ntensor(1674) picture</w>\ntensor(539) of</w>\ntensor(320) a</w>\ntensor(6829) puppy</w>\ntensor(49407) <|endoftext|>\ntensor(49407) <|endoftext|>\n\n\nThe numbers from Tokenizer is passed to Text encoder to generate embeddings\n\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\noutput_embeddings\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],\n         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],\n         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],\n       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n\n\n\n\n4.2.3 Classifier- Free Guidance\nClassifier-Free Guidance is a method to increase the adherence of the output to the text provided as input. Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. The default value of guidance is 7.5\nIn order to implement this, along with the embeddings from the prompt we also pass embeddings for an empty string.This is known as Unconditional embedding. This helps the model to go in which ever direction it wants as long as it results in a reasonably-looking image. The prediction are then calculated as follows\nprediction = unconditional_output + guidance(text_output - unconditional_output)\n\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"] * len(prompts), padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\nemb = torch.cat([uncond_embeddings, output_embeddings])"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#unet",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#unet",
    "title": "Stable diffusion - Introduction",
    "section": "4.3 UNet",
    "text": "4.3 UNet\nThe UNet is the model which produces latent outputs by removing noise.\nUNet model takes three inputs:  1. Noisy latent or Noise - If initial image is provided, noisy latents are produced by adding VAE encoder‚Äôs latents and noise otherwise it takes pure noise as input and produces new image based on the textual description 2. Timestep - Timestep is the number of times the diffusion process is repeated. It takes as input the current timestep 3. Embeddings - Embeddings generated by CLIP for the input prompts and unconditional embeddings\nThe UNet model predicts the noise which is then subtracted from the Noisy latents. This process is repeated for the timesteps mentioned and at the end, the latent outputs produced are passed to VAE‚Äôs decoder to generate High-resolution image\n\n\n\n\nFig 9 : UNet (Image Credit)\n\n\n\nLet‚Äôs look into the code for UNet. Along with UNet we also import a scheduler. Scheduler determines the amount of noise to be added to latent at a given step in diffusion process\n\nfrom diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(51)\n\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\nunet.to(torch_device)\n\nYou can see the amount of noise added in each step below. This is called sigmas. The noise added is highest at first step and is gradually decreased in the following steps until it becomes zero at the last step\n\nprint(scheduler.sigmas)\n\ntensor([14.6146, 12.9679, 11.5454, 10.3129,  9.2414,  8.3072,  7.4900,  6.7731,\n         6.1422,  5.5854,  5.0924,  4.6547,  4.2650,  3.9169,  3.6051,  3.3251,\n         3.0728,  2.8449,  2.6383,  2.4507,  2.2797,  2.1235,  1.9804,  1.8488,\n         1.7276,  1.6156,  1.5118,  1.4154,  1.3255,  1.2415,  1.1629,  1.0890,\n         1.0193,  0.9535,  0.8912,  0.8319,  0.7754,  0.7213,  0.6695,  0.6195,\n         0.5712,  0.5242,  0.4784,  0.4333,  0.3885,  0.3437,  0.2981,  0.2505,\n         0.1989,  0.1379,  0.0292,  0.0000])\n\n\nLet‚Äôs add noise to the latents obtained from VAE‚Äôs encoder earlier and see how U-Net removes noise\n\nnoise = torch.randn_like(encoded)\nencoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[40]]))\nlatents_to_pil(encoded_and_noised)[0]\n\n\n\n\n\n\n\n\n\n#Unconditional prompt\nprompt=[\"\"]\n\n## tokenizing and getting embeddings from clip model\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    text_embeddings=text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n\n#Using U-Net to predict noise\nlatent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()])\nwith torch.no_grad():\n    noise_pred = unet(latent_model_input, 40, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n#Visualize after subtracting noise\nlatents_to_pil(encoded_and_noised -noise_pred)[0]\n\n\n\n\n\n\n\n\nYou can see that the output image is clearer than the provided noisy image."
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#initialize-models",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#initialize-models",
    "title": "Stable diffusion - Introduction",
    "section": "5.1 Initialize models",
    "text": "5.1 Initialize models\nFirst lets import the model required from diffusers and transformers library. We then create a class SDPipeline and the __init__ method takes model_path as input which in our case is CompVis/stable-diffusion-v1-4\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n\nclass SDPipeline():\n    def __init__(self, model_path):\n        self.model_path=model_path\n        self.vae = AutoencoderKL.from_pretrained(model_path,subfolder=\"vae\" ,torch_dtype=torch.float16).to(torch_device)\n        self.unet=UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16).to(torch_device)\n        self.text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(torch_device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\", torch_dtype=torch.float16)\n        self.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#tokenize-texts",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#tokenize-texts",
    "title": "Stable diffusion - Introduction",
    "section": "5.2 Tokenize Texts",
    "text": "5.2 Tokenize Texts\nAs explained under Classifier-Free Guidance, we not only pass the embeddings of the prompt but also embeddings for empty string. So let‚Äôs create a function which tokenizes and provides embeddings. We can make use of the dynamic nature of the Python language to add this function dynamically to class SDPipeline\n\ndef encode_text(self, prompts, maxlen=None):\n    if maxlen is None: maxlen = self.tokenizer.model_max_length\n    inp = self.tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return self.text_encoder(inp.input_ids.to(torch_device))[0].half()\n\nSDPipeline.encode_text = encode_text"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#diffusion-loop",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#diffusion-loop",
    "title": "Stable diffusion - Introduction",
    "section": "5.3 Diffusion Loop",
    "text": "5.3 Diffusion Loop\nThe diffusion loop includes all the steps which we summarized at the starting of this section. We can also add this to class SDPipeline\n\ndef diff_pipeline(self,prompts, guidance=7.5, inference_steps=51, seed=100, callbacks=[], callback_steps=None):\n        #calculate number of prompts\n        batch_size = len(prompts)\n        \n        #generate text_embeddings\n        cond   = self.encode_text(prompts)\n        \n        #generate unconditional embeddings and concat them\n        uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\n        emb    = torch.cat([uncond, cond])\n        if seed : torch.manual_seed(seed)\n        \n        #generate random noise and add noise\n        latents = torch.randn((batch_size, self.unet.in_channels, 512//8, 512//8))\n        \n        #Setting number of steps in scheduler\n        self.scheduler.set_timesteps(inference_steps)\n        \n        #Add noise to latents\n        latents = latents.to(torch_device).half() * self.scheduler.init_noise_sigma\n\n        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n            # scale the latents for particular timestep\n            inp = self.scheduler.scale_model_input(torch.cat([latents]*2), t)\n            \n            #predict noise using UNet\n            with torch.no_grad():u,c = self.unet(inp, t, encoder_hidden_states=emb).sample.chunk(2)\n                \n            #Perform Classifier-Free Guidance    \n            pred = u + guidance*(c-u)\n            \n            #Removing predicted noise from the latents\n            latents = self.scheduler.step(pred, t, latents).prev_sample\n            \n            #run callbacks if any are added\n            if len(callbacks):\n                if i%callback_steps==0:\n                    for callback in callbacks: callback(i, t, latents)\n        \n        #after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated\n        with torch.no_grad():\n            return self.vae.decode(1/0.18125 * latents).sample\n\n\nSDPipeline.__call__ = diff_pipeline"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#adding-callback",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#adding-callback",
    "title": "Stable diffusion - Introduction",
    "section": "5.4 Adding Callback",
    "text": "5.4 Adding Callback\nLet‚Äôs create a callback as well, which will help to visualize the steps like we did at the starting . We will create a class which stores intermediate steps images. Since they are latents we use VAE to decode and display for us\n\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\",subfolder=\"vae\" ,torch_dtype=torch.float16).to(torch_device)\nclass Latents_Callback:\n    def __init__(self, prompts):\n        self.imgs = [[] for _ in range(len(prompts))]\n    \n    def __call__(self, i, t, latents):\n        latents = 1 / 0.18215 * latents\n        for i in range(latents.shape[0]):\n            callback_imgs = vae.decode(latents).sample[i]\n            callback_imgs = create_image(callback_imgs)\n            self.imgs[i].append(callback_imgs)"
  },
  {
    "objectID": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#running-our-sdpipeline",
    "href": "posts/2022-11-22-StableDiffusionIntro/01_stable_diffusion_introduction.html#running-our-sdpipeline",
    "title": "Stable diffusion - Introduction",
    "section": "5.5 Running our SDPipeline",
    "text": "5.5 Running our SDPipeline\n\nprompts = [\n    'a photograph of an astronaut riding a horse',\n    'an oil painting of an astronaut riding a horse in the style of grant wood',\n    'a painting of dog wearing suit'\n]\n\nInitializing callbacks and SDPipeline objects\n\nlc = Latents_Callback(prompts)\n\n\npipe = SDPipeline(\"CompVis/stable-diffusion-v1-4\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs pass the prompts and the callback object to the SDPipeline object pipe. We can see the images generated for each prompt and can also visualize the steps by displaying images saved in the callback object\n\nimages = pipe(prompts, callbacks=[lc], callback_steps=12)\n\n\n\n\n\nimgs = [create_image(i) for i in images]\nimage_grid(imgs, rows=1, cols = len(imgs))\n\n\n\n\n\nrows, cols = len(prompts), len(lc.imgs[0])\nimgs =[img for i in range(len(prompts)) for img in lc.imgs[i]]\nimage_grid(imgs, rows=rows, cols=cols)"
  },
  {
    "objectID": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html",
    "href": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html",
    "title": "Stable diffusion - Negative Prompt, Image to Image & Textual Inversion",
    "section": "",
    "text": "Introduction to variations provided in ü§ó Hugging face - Diffusers library such as Negative prompt, Image to Image and training new concept using Textual Inversion\nThis is the second post on Stable Diffusion. Check out the previous post through link below if you haven‚Äôt read it yet  1. Part 1 - Stable diffusion - Introduction.\nIn previous post, we learnt about Stable Diffusion and its key components VAE, UNet, Scheduler & CLIPTextModel. We also built a pipeline named SDPipeline using these key components and generated images for multiple prompts. In this post, we will further modify our pipeline to integrate negative prompts and image to image. At the end, we will learn about Textual Inversion which is useful to train a new concept\nAt first, let‚Äôs download and import the required libraries, authenticate and login to HuggingFace\nBelow is the pipeline which we built in the previous post using the core components."
  },
  {
    "objectID": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html#what-is-negative-prompt",
    "href": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html#what-is-negative-prompt",
    "title": "Stable diffusion - Negative Prompt, Image to Image & Textual Inversion",
    "section": "1.1 What is Negative Prompt?",
    "text": "1.1 What is Negative Prompt?\nNegative prompt is an additional capability which removes anything that the user doesn‚Äôt want to see in generated images. Suppose you don‚Äôt want a particular color or particular object to be part of the generated images, you can mention it in negative_prompt argument in the StableDiffusionPipeline.\n\n\n\n\nFig 1 : Example for Negative Prompt (Image Credit)"
  },
  {
    "objectID": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-negative-prompt-to-our-sdpipeline",
    "href": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-negative-prompt-to-our-sdpipeline",
    "title": "Stable diffusion - Negative Prompt, Image to Image & Textual Inversion",
    "section": "1.2 Adding negative prompt to our SDPipeline",
    "text": "1.2 Adding negative prompt to our SDPipeline\nLet‚Äôs find how the negative prompt works and how can we integrate it into our pipeline.\nIn the previous post, we used an empty string, generated unconditional embeddings from it and used it along with text embeddings to implement Classifier-Free guidance as below\nuncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\nThe way negative prompt works is that the empty string is replaced by the string containing the objects, styles or colors provided by the user as negative prompt. So to implement negative prompt, we can modify our code to use empty string if negative prompt is not provided else to use negative prompt provided by user\nif not negative_prompts:\n    uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\nelse:\n    uncond = self.encode_text(neg_prompts, cond.shape[1])\nLet‚Äôs make those changes in the diffusion loop of our pipeline and check if our negative prompt implementation is working\n\ndef diff_pipeline_negprompts(self,prompts,negative_prompts=None, guidance=7.5, inference_steps=51, seed=100, callbacks=[], callback_steps=None):\n        #calculate number of prompts\n        batch_size = len(prompts)\n        \n        #generate text_embeddings\n        cond   = self.encode_text(prompts)\n        \n        #generate embedding for negative prompts if available else generate unconditional embeddings\n        if not negative_prompts: uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\n        else : uncond = self.encode_text(negative_prompts, cond.shape[1])\n        emb    = torch.cat([uncond, cond])\n\n        if seed : torch.manual_seed(seed)\n        \n        #generate random noise and add noise\n        latents = torch.randn((batch_size, self.unet.in_channels, 512//8, 512//8))\n        \n        #Setting number of steps in scheduler\n        self.scheduler.set_timesteps(inference_steps)\n        \n        #Add noise to latents\n        latents = latents.to(torch_device).half() * self.scheduler.init_noise_sigma\n\n        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n            # scale the latents for particular timestep\n            inp = self.scheduler.scale_model_input(torch.cat([latents]*2), t)\n            \n            #predict noise using UNet\n            with torch.no_grad():u,c = self.unet(inp, t, encoder_hidden_states=emb).sample.chunk(2)\n                \n            #Perform Classifier-Free Guidance    \n            pred = u + guidance*(c-u)\n            \n            #Removing predicted noise from the latents\n            latents = self.scheduler.step(pred, t, latents).prev_sample\n            \n            #run callbacks if any are added\n            if len(callbacks):\n                if i%callback_steps==0:\n                    for callback in callbacks: callback(i, t, latents)\n        \n        #after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated\n        return self.latents_to_pil(latents)\n\n\nSDPipeline.__call__ = diff_pipeline_negprompts\n\nWe will generate two images with same prompt and for one of the images we will use a negative prompt\n\npipe = SDPipeline(\"CompVis/stable-diffusion-v1-4\")\n\n\ntorch.manual_seed(9000)\nimg1 = pipe(prompts=[\"a photo of a forest with trees and bushes\"])[0]\nimg2 = pipe(prompts=[\"a photo of a forest with trees and bushes\"], negative_prompts=[\"green\"])[0]\n\n\n\n\n\n\n\n\nimgs=[img1, img2]\nimage_grid(imgs, rows=1, cols=2)\n\n\n\n\n\n\n\n\n\n\nFig 2: Negative Prompt Results for prompt ‚Äúa photo of a forest with trees and bushes‚Äù and negative prompt=‚Äúgreen‚Äù‚Äú\n\n\nAs you can see in the images above, the image on the left generated image of forest. When given negative prompt as color green, the model generated same image without green color in it (on the right)"
  },
  {
    "objectID": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-image-to-image-capability-to-sdpipeline",
    "href": "posts/2022-11-24-StableDiffusionVariations/02_stable_diffusion_variations.html#adding-image-to-image-capability-to-sdpipeline",
    "title": "Stable diffusion - Negative Prompt, Image to Image & Textual Inversion",
    "section": "2.1 Adding Image to Image capability to SDPipeline",
    "text": "2.1 Adding Image to Image capability to SDPipeline\nIn the previous post, we learnt that we can also pass an image to our model and it generates new images using the provided image as the initial image along with the prompt. There are two scenarios 1) with image (Image to Image) 2) without image (Prompt to Image)\nFor scenario 1 which is Image to Image pipeline, we expect 2 parameters: 1. init_image - This is used as the initial image 2. strength - The value of this parameter is between 0 and 1\nFirst, the provided init_image is passed to the VAE‚Äôs encoder to generate latents. We then need to add noise to the latents. Since we are using initial image we can reduce the number of inference steps. The strength parameter is used for this and higher its value higher the inference_steps. If we want our generated image to be similar to the init_image, the number of steps should be less so the strength parameter value should also be less. The amount of noise to be added is calculated by multiplying the strength and inference_steps\nFor example, suppose we want our generated image to be similar to the initial image so we assign the strength parameter to 0.4 and inference_steps is equal to 50 . Then the noise added is for the step 30 (50 -( 0.4 x 50) == 30). The inference_steps is reduced to 20 steps (50x0.4)\nFor scenario 2 which is prompt to image, we need to create a random noise. The amount of noise for the first step is multiplied with scheduler.init_noise_sigma . The inference_steps remains same.\nLet‚Äôs create a function which can handle both the scenarios. The function should return latents and timesteps.  1. In case of Prompt to Image, random noise is created and multiplied with init_noise is returned. The timesteps is returned without modifying it 2. In case of Image to Image, latents are created from init_image and amount of noise to be added is calculated using strength and returned. The timesteps is also reduced based on the strength and returned\n\ndef create_latents(self, batch_size, init_image, strength, inference_steps):\n    \n    if init_image is None:\n        latents = torch.randn((batch_size, self.unet.in_channels, 512//8, 512//8))\n        latents = latents.to(torch_device).half() * self.scheduler.init_noise_sigma\n        return latents, self.scheduler.timesteps.to(torch_device)\n    else:\n        img_latents = self.pil_to_latents(init_image)\n        noise = torch.randn(img_latents.shape, generator=None, device=torch_device, dtype=img_latents.dtype)\n\n        init_timestep = int(inference_steps * strength)\n        timesteps = self.scheduler.timesteps[-init_timestep]\n        timesteps = torch.tensor([timesteps], device=torch_device)\n\n        init_latents = self.scheduler.add_noise(img_latents, noise, timesteps)\n\n        t_start = max(inference_steps - init_timestep, 0)\n        timesteps = self.scheduler.timesteps[t_start:].to(torch_device)\n        \n        return init_latents, timesteps\n\nSDPipeline.create_latents = create_latents\n\nLet‚Äôs replace the diffusion loop code to get latents and timesteps from the above function\n\ndef diff_pipeline_img2img(self,prompts,init_image=None, strength=0.8, negative_prompts=None, guidance=7.5, inference_steps=51, seed=100, callbacks=[], callback_steps=None):\n        #calculate number of prompts\n        batch_size = len(prompts)\n        \n        #generate text_embeddings\n        cond   = self.encode_text(prompts)\n        \n        #generate embedding for negative prompts if available else generate unconditional embeddings\n        if not negative_prompts: uncond = self.encode_text([\"\"]*batch_size, cond.shape[1])\n        else : uncond = self.encode_text(negative_prompts, cond.shape[1])\n        emb    = torch.cat([uncond, cond])\n        \n        if seed : torch.manual_seed(seed)\n\n        #Setting number of steps in scheduler\n        self.scheduler.set_timesteps(inference_steps)\n        \n        #generate random noise and add noise\n        latents, timesteps = self.create_latents(batch_size, init_image, strength, inference_steps)\n        \n        \n        for i, t in enumerate(tqdm(timesteps)):\n            # scale the latents for particular timestep\n            inp = self.scheduler.scale_model_input(torch.cat([latents]*2), t)\n            \n            #predict noise using UNet\n            with torch.no_grad():u,c = self.unet(inp, t, encoder_hidden_states=emb).sample.chunk(2)\n                \n            #Perform Classifier-Free Guidance    \n            pred = u + guidance*(c-u)\n            \n            #Removing predicted noise from the latents\n            latents = self.scheduler.step(pred, t, latents).prev_sample\n            \n            #run callbacks if any are added\n            if len(callbacks):\n                if i%callback_steps==0:\n                    for callback in callbacks: callback(i, t, latents)\n        \n        #after loop is completed, pass the latents to VAE's decoder and return the High-resolution image generated\n        return self.latents_to_pil(latents)\n\nSDPipeline.__call__ = diff_pipeline_img2img\n\nNow let‚Äôs download an image and pass it to our Image To Image pipeline\n\nfrom fastdownload import FastDownload\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\nimage = Image.open(p).convert('RGB').resize((512,512))\nimage\n\n\n\n\n\n\n    \n      \n      106.50% [49152/46150 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\nFig 3: Initial Image for Image To Image Pipeline\n\n\nWe can also add our Callback function to visualize how the image is generated\n\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\",subfolder=\"vae\" ,torch_dtype=torch.float16).to(torch_device)\nclass Latents_Callback:\n    def __init__(self, prompts):\n        self.imgs = [[] for _ in range(len(prompts))]\n    \n    def __call__(self, i, t, latents):\n        latents = 1 / 0.18215 * latents\n        for i in range(latents.shape[0]):\n            callback_imgs = vae.decode(latents).sample[i]\n            callback_imgs = create_image(callback_imgs)\n            self.imgs[i].append(callback_imgs)\n\n\n\n\n\n\n\nWe are using a random sketch of wolf howling at moon as initial image. So we can use higher value of strength to generate good image\n\nprompt = [\"Wolf howling at the moon, photorealistic 4K\"]\npipei2i = SDPipeline(\"CompVis/stable-diffusion-v1-4\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlc = Latents_Callback(prompt)\nimg = pipei2i(prompt,init_image=image, callbacks=[lc], callback_steps=8,inference_steps=50)\nimg[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 4: Generated Image for Image To Image pipeline\n\n\n\nrows, cols = len(prompt), len(lc.imgs[0])\nimgs =[img for i in range(len(prompt)) for img in lc.imgs[i]]\nimage_grid(imgs, rows=rows, cols=cols)"
  },
  {
    "objectID": "posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html",
    "href": "posts/2022-12-01-Dreambooth/03_pintu_dreambooth.html",
    "title": "Stable diffusion - Dreambooth",
    "section": "",
    "text": "Training a new concept to Stable Diffusion model using Dreambooth, generating images of the new concept using the trained model, and saving the pipeline to HuggingFace Hub\n\n\n\n  \n\n\nThis is the third post on Stable Diffusion. Check out the previous posts through links below if you haven‚Äôt read them yet  1. Part 1 - Stable diffusion - Introduction. 2. Part 2 - Stable diffusion - Negative Prompt, Image to Image & Textual Inversion\nIn previous posts, we learnt about Stable Diffusion and its key components VAE, UNet, Scheduler & CLIPTextModel. We also learnt the variations provided by the model such as Negative Prompt and Image to Image pipeline. We implemented all these and named our pipeline as SDPipeline . We also learnt how we can use Textual Inversion to train a new concept to the Stable Diffusion model.\nIn this post, we will be covering Dreambooth which is similar to Textual Inversion, train a new concept using it and push our model to HuggingFace Hub, from where anyone can use our fine-tuned model. This post is based on the HuggingFace Notebook found here. So let‚Äôs get started\n\n1 What is Dreambooth?\nStable Diffusion results are amazing. But what if you want to generate images of subjects that are personal to you. Can it render images of your cute pet dog having fun at the beach? What about you being the main character of superhero movie saving the World?. Yes, you absolutely can personalize the Stable Diffusion model using Dreambooth\n\n\n\n\nCredits\n\n\n\n\n\n\nDreambooth is an approach to teach a new concept by fine-tuning a pretrained text-to-image model such that it binds a unique token with the new concept of images. If you are aware of fine-tuning vision models, you might be thinking at this point that it will take hundreds of images and hours of training to fine-tune Stable Diffusion model, but to our surprise only a minimum of 5 images of the new subject is required!\nThe image on the right above is generated from the model which I fine-tuned for around 30 minutes on Tesla T4 GPU and 16GB RAM with just 6 images of my pet dog. Amazing isn‚Äôt it!\n\n\n\n\n \n\n\nFig 1: Dreambooth approach (Image Credit)\n\n\n\nDreambooth approach of fine-tuning the text-to image diffusion model is as follows: 1. We pair the input images of our interested subject and text prompt containing a unique identifier and fine tune the model. (Example - for unique identifier you can use a token which is less frequently used such as sks.So the prompt will be ‚ÄúA photo of sks dog‚Äù) 2. We also pass the class of the subject as a prompt while fine-tuning the model and apply class-specific prior preservation loss (Example - ‚ÄúA photo of a dog‚Äù) \nPrior Preservation doesn‚Äôt seem to make huge difference except when trained on faces. So prior preservation is not used while training in this post. Checkout this amazing detailed article in which they have experimented training dreambooth with different settings. You can follow the HuggingFace notebook in which they have implemented prior preservation\nLet‚Äôs install the required libraries and load the CompVis/stable-diffusion-v1-4 models in the next section\n\n\n2 Getting started\nAt first, we will be connecting to Google Drive from Google Colab. This allows us to read images saved in Drive for training model and also save the trained model to the Drive\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nWe will be downloading the required libraries as we did in previous posts. We will be installing and importing an additional library named bitsandbytes. It provides an 8-bit optimizer which saves memory and increases speed while training. For more details checkout this repo\nWe will also be using accelerate library which helps in distributed training, gradient accumulation and many more.\n\n!pip install -Uq accelerate transformers diffusers ftfy\n\n\n!pip install -qq bitsandbytes\n\nWe can authenticate our notebook to Huggingface services using the commands below. Since we will be pushing our model to HuggingFace Hub later, we will need write permissions. So you need to create an Access Token with write permission and use it to authenticate\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful\n\n\n\n\nCode\nfrom PIL import Image\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms as tfms\n\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\n\nfrom transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n\nimport bitsandbytes as bnb\n\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols*w, rows*h))\n    for i, img in enumerate(imgs):grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nNext, we will initialize the core components of the Stable Diffusion model with the weights of CompVis/stable-diffusion-v1-4 repo\n\nmodel_path=\"CompVis/stable-diffusion-v1-4\"\n\n\ntext_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\")\nvae          = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\nunet         = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\ntokenizer    = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 Data\nWe need to provide the images of our subject and the prompt with the unique token to fine-tune model. So let‚Äôs create a dataset which takes images and prompts as input. The dataset needs to convert the images into tensor which is done using PyTorch‚Äôs transforms. It also should return input_ids for the prompt which is provided by the Tokenizer. Both pixel values and prompt ids are returned as dictionary\n\nclass DreamBoothDataset(Dataset):\n    def __init__(self, images_path, prompts , tokenizer, size =512, center_crop=False):\n        self.size, self.center_crop = size, center_crop\n        self.tokenizer = tokenizer\n        self.images_path = Path(images_path)\n        if not self.images_path.exists():\n            raise ValueError(\"Images path doesn't exist\")\n\n        self.images = list(Path(images_path).iterdir())\n        self.num_of_images = len(self.images)\n        self.prompts = prompts \n\n        self.image_transforms = tfms.Compose([\n            tfms.Resize(size, interpolation = tfms.InterpolationMode.BILINEAR),\n            tfms.CenterCrop(size) if center_crop else tfms.RandomCrop(size),\n            tfms.ToTensor(),\n            tfms.Normalize([0.5],[0.5])\n        ])  \n        \n            \n    def __len__(self):\n        return self.num_of_images    \n\n    def __getitem__(self, index):\n        example = {}\n        image = Image.open(self.images[index%self.num_of_images]).convert(\"RGB\")   \n        example[\"images\"] = self.image_transforms(image)  \n        example[\"prompt_ids\"] = self.tokenizer(self.prompts, padding=\"do_not_pad\", truncation=True, \n                                               max_length=self.tokenizer.model_max_length).input_ids \n        \n        return example          \n\nDataloader helps us to sample minibatches from our dataset. Since our dataset returns dictionary, we have to tell the Dataloader how it can combine and return minibatches of pixelvalues and prompt_ids.\n\ndef collate_fn(examples):\n    pixels    = [example[\"images\"] for example in examples]\n    input_ids = [example[\"prompt_ids\"] for example in examples]\n    \n    pixels = torch.stack(pixels).to(memory_format=torch.contiguous_format).float()\n    input_ids = tokenizer.pad({\"input_ids\":input_ids}, padding=True, return_tensors=\"pt\").input_ids\n\n    batch = {\n        \"input_ids\":input_ids,\n        \"pixel_values\":pixels\n    }\n\n    return batch\n\n\ntrain_dataset = DreamBoothDataset(\n        images_path = \"/content/gdrive/My Drive/pintu\",\n        prompts = \"a photo of a sks dog\",\n        tokenizer = tokenizer,\n        size=512,\n        center_crop=True\n    )\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= 1, shuffle=True, collate_fn=collate_fn)\n\nNow let‚Äôs view the our data. Below are the 6 images of my pet Pintu which are used for training\n\nimages =[Image.open(train_dataset.images[i]).resize((256,256)).convert(\"RGB\") for i in range(len(train_dataset))]\nimage_grid(images, rows=1, cols=len(train_dataset))\n\n\n\n\n\n\nFig 2: Images used to train Dreambooth model\n\n\n\n\n4 Training\n\nlearning_rate=5e-06\nmax_train_steps =500\ntrain_batch_size=1\ngradient_accumulation_steps=2\nuse_8bit_adam=True\noutput_dir = \"/content/gdrive/My Drive/pintu_dreambooth\"\n\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nThe function dreambooth_trainer fine-tunes the Stable Diffusion model. We fine-tune the model for 500 steps at a learning rate of 5e-06. The pixel_values of the images from the dataloader are passed to VAE‚Äôs encoder to generate latents. The prompt_ids are passed to the text_encoder to generate embeddings. Random noise is added at each step and the loss is calculated. The loss is then backpropogated and the weights of the model are updated. After the training is completed, all the components of the model are saved to the output_dir path which in our case gets saved to Google Drive\n\ndef dreambooth_trainer(data, text_encoder, vae, unet):\n\n    accelerator=Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n    set_seed(90000)\n    unet.enable_gradient_checkpointing()\n\n    if use_8bit_adam:\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    optimizer = optimizer_class(unet.parameters(), lr=learning_rate)\n\n    noise_scheduler = DDPMScheduler(beta_start= 0.00085, beta_end = 0.012, beta_schedule=\"scaled_linear\",\n                                    num_train_timesteps=1000)\n    \n    \n    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, data)\n\n    text_encoder.to(torch_device)\n    vae.to(torch_device)\n\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader)/gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps/num_update_steps_per_epoch)\n    total_batch_size = train_batch_size *accelerator.num_processes * gradient_accumulation_steps\n\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        unet.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(unet):\n                with torch.no_grad():\n                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n                    latents*=0.18215\n                \n                \n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,(bsz,), device=latents.device).long()\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                with torch.no_grad():\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1,2,3]).mean()\n\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step+=1\n\n\n            logs = {\"loss\":loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step>=max_train_steps:break\n\n        accelerator.wait_for_everyone()\n\n    if accelerator.is_main_process:\n        pipeline=StableDiffusionPipeline(\n            text_encoder=text_encoder, vae=vae, unet=unet, \n            tokenizer=tokenizer,\n            scheduler = PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n            ),\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"))\n        pipeline.save_pretrained(output_dir)\n\n\nimport accelerate\naccelerate.notebook_launcher(dreambooth_trainer, args=(train_dataloader,text_encoder, vae, unet), num_processes = 1)\n\n\n\n5 Results\nNow we can load the model from the saved directory and use it for inference. To get high quality images, you can use prompts from Lexica and replace with your unique identifier.\n\noutput_dir = \"/content/gdrive/My Drive/pintu_dreambooth\"\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npipe = StableDiffusionPipeline.from_pretrained(\n        output_dir,\n        torch_dtype=torch.float16,\n    ).to(torch_device)\n\nBelow are some of the astonishing results generated by our fine-tuned model!\n\n\nCode\nprompt = \"a beautiful portrait of a cute sks dog, beautiful detailed eyes, golden hour, standing on a beach, outdoors, professional award winning portrait photography, Zeiss 150mm f/ 2.8 Hasselblad\" \n\nnum_samples = 3 \nnum_rows = 3 \n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 3: A beautiful portrait of a cute sks dog, beautiful detailed eyes, golden hour, standing on a beach, outdoors, professional award winning portrait photography, Zeiss 150mm f/ 2.8 Hasselblad‚Äù\n\n\n\n\nCode\nprompt = \"portrait of a sks dog wearing a hat, realism, realistic, photorealism, f 3. 5, photography, octane render, trending on artstation, artstationhd, artstationhq, unreal engine, cinema 4 d, 8 k, detailed fur \" \n\nnum_samples = 3 \nnum_rows = 3 \n\n# def dummy_checker(images, **kwargs): return images, False\n# pipe.safety_checker = dummy_checker\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 4: Portrait of a sks dog wearing a hat, realism, realistic, photorealism, f 3. 5, photography, octane render, trending on artstation, artstationhd, artstationhq, unreal engine, cinema 4 d, 8 k, detailed fur ‚Äù\n\n\n\n\nCode\nprompt = \"sks dog as a realistic fantasy knight, closeup portrait art by donato giancola and greg rutkowski, digital art, trending on artstation, symmetry!! \" \n\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 5: sks dog as a realistic fantasy knight, closeup portrait art by donato giancola and greg rutkowski, digital art, trending on artstation, symmetry!!\n\n\n\n\nCode\nprompt = \"white sks dog, with reflection in the puddle, foggy old forest, very detailed, 4 k, professional photography \" \n\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 6 : White sks dog, with reflection in the puddle, foggy old forest, very detailed, 4 k, professional photography\n\n\n\n\nCode\nprompt = \"framed renaissance portrait of a sks dog\" \n\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 7: Framed renaissance portrait of a sks dog\n\n\nIn the below results, the prompt is truncated since CLIP can only handle prompts with maximum of 77 tokens\n\n\nCode\nprompt = \"a sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, partial anatomy, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\"\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha', 'shot, illustration, art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\n\n\n\n\n\nFig 8: A sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, partial anatomy, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\n\n\n\n\nCode\nprompt = \"a sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\"\nnum_samples = 3 \nnum_rows = 3 \n\n\nall_images = [] \nfor _ in range(num_rows):\n    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n    all_images.extend(images)\n\ngrid = image_grid(all_images, num_samples, num_rows)\ngrid \n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha', ', art by greg rutkowski beeple and alphonse mucha']\n\n\n\n\n\n\n\n\n\n\nFig 9: A sks dog as god with a radiant halo, wings, detailed face, gorgeous, flowing hair, very muscular male body, stormy and grand war scene, delicate and intricate borders for decoration, caesar victorious, proud Emperor, crepuscular ray, intricate, highly detailed, 8K, digital painting, fantasy, concept art, sharp focus, over-shoulder shot, illustration, art by greg rutkowski beeple and alphonse mucha\n\n\n\n\n6 Pushing your model to hub\nHuggingFace Hub is a platform with over 60K models, 6K datasets and 6K demo apps(spaces) all open source and publicly available. The best part is that sharing and using any public model on the Hub is completely free of cost. Let‚Äôs next see how we can move our model from Google Drive to HuggingFace Hub. In order to push model to hub, make sure that you have used Access token with write permission while authenticating to HuggingFace\nThe get_full_repo_name returns the repository name for given model in user‚Äôs namespace ({username/model_name}).\n\nfrom huggingface_hub import get_full_repo_name\nmodel_name = \"pintu_dreambooth\"\n\n\nhub_model = get_full_repo_name(model_name)\nhub_model\n\n'prajwal13/pintu_dreambooth'\n\n\n\noutput_dir = \"/content/gdrive/My Drive/pintu_dreambooth\"\nimages_folder = \"/content/gdrive/My Drive/pintu\"\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nWe can then use create_repo to create an empty directory in HuggingFace hub and push our model and the images used for training to the empty repo.\n\nfrom huggingface_hub import HfApi, create_repo\n\ncreate_repo(hub_model)\napi = HfApi()\napi.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=hub_model)\n\n'https://huggingface.co/prajwal13/pintu_dreambooth/tree/main/'\n\n\n\napi.upload_folder(folder_path=images_folder, path_in_repo=\"concept_images\",repo_id=hub_model)\n\n'https://huggingface.co/prajwal13/pintu_dreambooth/tree/main/concept_images'\n\n\nThe last thing to do is create a model card so that our model can easily be found on the Hub. We will also add the images used to train in the model card\n\nfrom huggingface_hub import ModelCard\nimport os\n\nimages_upload = os.listdir(images_folder)\nimage_string = \"\"\n\nfor i, image in enumerate(images_upload):\n    image_string = f'''{image_string}![sks {i}](https://huggingface.co/{hub_model}/resolve/main/concept_images/{image})'''\n\nimage_string\n\n\ncontent = f\"\"\"\n---\nlicense: mit\ntags:\n- pytorch\n- diffusers\n- dreambooth\n---\n\n# Model Card for Dreambooth model trained on My pet Pintu's images\n\nThis model is a diffusion model for unconditional image generation of my cute pet dog Pintu trained using Dreambooth concept. The token to use is sks .\n\n## Usage\n\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from_pretrained({hub_model})\nimage = pipeline('a photo of sks dog').images[0]\nimage\n\nThese are the images on which the dreambooth model is trained on\n\n{image_string}\n\n\"\"\"\n\n\ncard = ModelCard(content)\ncard.push_to_hub(hub_model)\n\n'https://huggingface.co/prajwal13/pintu_dreambooth/blob/main/README.md'\n\n\nNow we can directly load the model from HuggingFace Hub by passing the repo id to the pipeline\n\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(hub_model).to(torch_device)\nimage = pipeline(\"a photo of sks dog wearing glasses, and standing near beach\").images[0]\nimage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 Conclusion\nIn this post, we learnt about Dreambooth and how we can personalize the Stable Diffusion model. We also learnt how to push our model to HuggingFace hub.\nI hope you enjoyed reading it. If there is any feedback, feel free to reach out on LinkedIn or on mail\n\n\n8 References\n\nFastAI course\nDreambooth paper\nHuggingFace Dreambooth notebook"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to prajwal-suresh13.github.io",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n      \nHi there, I‚Äôm Prajwal Suresh. I‚Äôm currently exploring Deep learning, Machine learning and data engineering technologies. This is where I write about my learning journey and explorations\nRead more about me here."
  },
  {
    "objectID": "index.html#latest-blogs",
    "href": "index.html#latest-blogs",
    "title": "Welcome to prajwal-suresh13.github.io",
    "section": "Latest Blogs",
    "text": "Latest Blogs\nClick here to check out more blogs.\n\n\n\n\n  \n\n\n\n\nStable diffusion - Dreambooth\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nPrajwal Suresh\n\n\n\n\n\n\n  \n\n\n\n\nStable diffusion - Negative Prompt, Image to Image & Textual Inversion\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nPrajwal Suresh\n\n\n\n\n\n\n  \n\n\n\n\nStable diffusion - Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nPrajwal Suresh\n\n\n\n\n\n\nNo matching items"
  }
]